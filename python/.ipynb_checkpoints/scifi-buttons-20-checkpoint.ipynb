{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 3e6 # Set maximum number of steps to run environment.\n",
    "run_path = \"scifibuttons20\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 20000 # Frequency at which to save model.\n",
    "env_name = \"scifibuttons20\" # Name of the training environment file.\n",
    "curriculum_file = 'curricula/lessons19.json'\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 3 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 256 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tlessonNr -> 4.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 30\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 5\n",
      "        Memory space size (per agent): 9\n",
      "        Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: -1.6377431788396888. Std of Reward: 3.118293247226211.\n",
      "Step: 10000. Mean Reward: -1.6779324734126584. Std of Reward: 3.1205609475645306.\n",
      "Step: 15000. Mean Reward: -1.2171739046681165. Std of Reward: 2.949470709447256.\n",
      "Step: 20000. Mean Reward: -1.3361887976171327. Std of Reward: 3.1144567649832737.\n",
      "Saved Model\n",
      "Step: 25000. Mean Reward: -1.3668216937988373. Std of Reward: 3.1812067822545522.\n",
      "Step: 30000. Mean Reward: -1.2381818067429091. Std of Reward: 3.484651045803917.\n",
      "Step: 35000. Mean Reward: -1.0290322463458783. Std of Reward: 2.9689915997042005.\n",
      "Step: 40000. Mean Reward: -1.1896296160074074. Std of Reward: 2.939132607665803.\n",
      "Saved Model\n",
      "Step: 45000. Mean Reward: -0.876832288257764. Std of Reward: 2.5073904093822232.\n",
      "Step: 50000. Mean Reward: -1.224557808937755. Std of Reward: 3.4807511751278275.\n",
      "Step: 55000. Mean Reward: -1.1562244773397958. Std of Reward: 3.50603787940207.\n",
      "Step: 60000. Mean Reward: -0.7021367433094017. Std of Reward: 2.2931690187422666.\n",
      "Saved Model\n",
      "Step: 65000. Mean Reward: -0.6676880126167131. Std of Reward: 2.4710606669874897.\n",
      "Step: 70000. Mean Reward: -0.7005219689439564. Std of Reward: 4.25711672353942.\n",
      "Step: 75000. Mean Reward: -0.31496295417604947. Std of Reward: 2.9657168289558142.\n",
      "Step: 80000. Mean Reward: -0.20166306041079923. Std of Reward: 2.3129374100176734.\n",
      "Saved Model\n",
      "Step: 85000. Mean Reward: -0.10453441035951427. Std of Reward: 3.6721375212777008.\n",
      "Step: 90000. Mean Reward: -0.016259836494094536. Std of Reward: 1.8414116882950233.\n",
      "Step: 95000. Mean Reward: 0.09156690555281688. Std of Reward: 1.9535286310595097.\n",
      "Step: 100000. Mean Reward: 0.12411032460035581. Std of Reward: 1.9689138368446917.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: 0.2983386115852848. Std of Reward: 1.5741232961531029.\n",
      "Step: 110000. Mean Reward: 0.32174922906377706. Std of Reward: 1.4387328428840536.\n",
      "Step: 115000. Mean Reward: 0.44988048181447543. Std of Reward: 1.301031198232221.\n",
      "Step: 120000. Mean Reward: 0.3886158229968927. Std of Reward: 1.2966258005316855.\n",
      "Saved Model\n",
      "Step: 125000. Mean Reward: 0.49050724934939616. Std of Reward: 2.2730395751654253.\n",
      "Step: 130000. Mean Reward: 0.47998684519236845. Std of Reward: 1.3185549749374148.\n",
      "Step: 135000. Mean Reward: 0.6289813824685652. Std of Reward: 1.0104156085152975.\n",
      "Step: 140000. Mean Reward: 0.6196697648294117. Std of Reward: 1.1791719727667176.\n",
      "Saved Model\n",
      "Step: 145000. Mean Reward: 0.6356649237093194. Std of Reward: 1.175157891474273.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \tlessonNr -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 150000. Mean Reward: 0.03494382346584266. Std of Reward: 1.605685627742577.\n",
      "Step: 155000. Mean Reward: -0.04119791389218761. Std of Reward: 1.2860566779438936.\n",
      "Step: 160000. Mean Reward: 0.02784415914753235. Std of Reward: 0.9993386238045281.\n",
      "Saved Model\n",
      "Step: 165000. Mean Reward: -0.010920093097820938. Std of Reward: 1.35494077781968.\n",
      "Step: 170000. Mean Reward: -0.008699759470212856. Std of Reward: 1.2442611315956777.\n",
      "Step: 175000. Mean Reward: -0.005419350977849421. Std of Reward: 2.9830038047970677.\n",
      "Step: 180000. Mean Reward: 0.13314904207019215. Std of Reward: 1.1088299121560754.\n",
      "Saved Model\n",
      "Step: 185000. Mean Reward: 0.03516204039745358. Std of Reward: 1.2287094110774417.\n",
      "Step: 190000. Mean Reward: 0.054200915207534305. Std of Reward: 2.8331541179414783.\n",
      "Step: 195000. Mean Reward: 0.17484716418515275. Std of Reward: 0.9456824855311117.\n",
      "Step: 200000. Mean Reward: 0.06722611000909086. Std of Reward: 2.2657194971085968.\n",
      "Saved Model\n",
      "Step: 205000. Mean Reward: 0.07872727526727262. Std of Reward: 1.4488174513238503.\n",
      "Step: 210000. Mean Reward: 0.1265570199817981. Std of Reward: 1.6476879614769668.\n",
      "Step: 215000. Mean Reward: 0.2773913063043477. Std of Reward: 0.9403651779343806.\n",
      "Step: 220000. Mean Reward: 0.18885106602744672. Std of Reward: 1.1106292795832209.\n",
      "Saved Model\n",
      "Step: 225000. Mean Reward: 0.14189814935509254. Std of Reward: 1.0132561679390328.\n",
      "Step: 230000. Mean Reward: 0.2856762763219512. Std of Reward: 0.9156182013280069.\n",
      "Step: 235000. Mean Reward: 0.28174393046843266. Std of Reward: 1.0064425153795313.\n",
      "Step: 240000. Mean Reward: 0.34899830341256366. Std of Reward: 0.8788999024586825.\n",
      "Saved Model\n",
      "Step: 245000. Mean Reward: 0.2932200012775999. Std of Reward: 0.8945891955133465.\n",
      "Step: 250000. Mean Reward: 0.2718940948103869. Std of Reward: 1.0444503096519053.\n",
      "Step: 255000. Mean Reward: 0.28472426584981614. Std of Reward: 0.9316203445753211.\n",
      "Step: 260000. Mean Reward: 0.34529865236782264. Std of Reward: 0.9381767337151039.\n",
      "Saved Model\n",
      "Step: 265000. Mean Reward: 0.3350180514644404. Std of Reward: 0.9824799617628085.\n",
      "Step: 270000. Mean Reward: 0.39081761104696006. Std of Reward: 0.8499949204479006.\n",
      "Step: 275000. Mean Reward: 0.405793358852583. Std of Reward: 0.8113555335115581.\n",
      "Step: 280000. Mean Reward: 0.27321285237831316. Std of Reward: 1.107857893862596.\n",
      "Saved Model\n",
      "Step: 285000. Mean Reward: 0.3387065648451737. Std of Reward: 0.9533242313855529.\n",
      "Step: 290000. Mean Reward: 0.32858024845144035. Std of Reward: 0.951414063694858.\n",
      "Step: 295000. Mean Reward: 0.28969565337565195. Std of Reward: 1.2083651876199168.\n",
      "Step: 300000. Mean Reward: 0.35740963934036135. Std of Reward: 0.9230288429335105.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 0.3862110734091695. Std of Reward: 0.8113423898264955.\n",
      "Step: 310000. Mean Reward: 0.46280210269334493. Std of Reward: 0.86612722986014.\n",
      "Step: 315000. Mean Reward: 0.4127938349204238. Std of Reward: 0.9257517365432142.\n",
      "Step: 320000. Mean Reward: 0.43579044206102935. Std of Reward: 0.9138568263870308.\n",
      "Saved Model\n",
      "Step: 325000. Mean Reward: 0.4293992942167844. Std of Reward: 0.8505809647667741.\n",
      "Step: 330000. Mean Reward: 0.4864511289584962. Std of Reward: 0.8135399526695551.\n",
      "Step: 335000. Mean Reward: 0.5915384625836363. Std of Reward: 0.7165871985729451.\n",
      "Step: 340000. Mean Reward: 0.5793679782757022. Std of Reward: 0.7335656484238098.\n",
      "Saved Model\n",
      "Step: 345000. Mean Reward: 0.5748348756287979. Std of Reward: 1.0075710096425532.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \tlessonNr -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 350000. Mean Reward: 0.6125330142607444. Std of Reward: 0.6501566484574708.\n",
      "Step: 355000. Mean Reward: -0.5541666646657802. Std of Reward: 0.9820873602450847.\n",
      "Step: 360000. Mean Reward: -0.5291522137737958. Std of Reward: 0.9737580185737623.\n",
      "Saved Model\n",
      "Step: 365000. Mean Reward: -0.502583024040406. Std of Reward: 0.9796390109100822.\n",
      "Step: 370000. Mean Reward: -0.4973861367839605. Std of Reward: 0.9598058287313945.\n",
      "Step: 375000. Mean Reward: -0.4644444427886869. Std of Reward: 0.9887224927221214.\n",
      "Step: 380000. Mean Reward: -0.3887985845787987. Std of Reward: 0.9402651532058603.\n",
      "Saved Model\n",
      "Step: 385000. Mean Reward: -0.2915300527646631. Std of Reward: 0.9069801326105975.\n",
      "Step: 390000. Mean Reward: -0.25857677711610494. Std of Reward: 1.005182561545558.\n",
      "Step: 395000. Mean Reward: -0.20437036841944456. Std of Reward: 0.9629273334634924.\n",
      "Step: 400000. Mean Reward: -0.1765270490108203. Std of Reward: 0.9264004441761504.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: -0.08342013700520841. Std of Reward: 0.9476142950398211.\n",
      "Step: 410000. Mean Reward: -0.09505154506185577. Std of Reward: 0.9840317419930309.\n",
      "Step: 415000. Mean Reward: -0.1521228056791229. Std of Reward: 1.469725317794896.\n",
      "Step: 420000. Mean Reward: -0.0658059455364633. Std of Reward: 0.8988390709155247.\n",
      "Saved Model\n",
      "Step: 425000. Mean Reward: 0.05376068492079767. Std of Reward: 0.8528838896445367.\n",
      "Step: 430000. Mean Reward: 0.14997179232552885. Std of Reward: 0.8228501499720715.\n",
      "Step: 435000. Mean Reward: 0.14715517355186775. Std of Reward: 0.8425579960008467.\n",
      "Step: 440000. Mean Reward: 0.20663978600107524. Std of Reward: 0.8026923779941766.\n",
      "Saved Model\n",
      "Step: 445000. Mean Reward: 0.2461153858839743. Std of Reward: 0.8321229642764069.\n",
      "Step: 450000. Mean Reward: 0.3151378457305764. Std of Reward: 0.7463420698426079.\n",
      "Step: 455000. Mean Reward: 0.35066425256739125. Std of Reward: 0.728438651108087.\n",
      "Step: 460000. Mean Reward: 0.3480876991853836. Std of Reward: 0.7528844234155547.\n",
      "Saved Model\n",
      "Step: 465000. Mean Reward: 0.44095294281494113. Std of Reward: 0.725391519596574.\n",
      "Step: 470000. Mean Reward: 0.42263922681222754. Std of Reward: 0.686649272773036.\n",
      "Step: 475000. Mean Reward: 0.5296973245683352. Std of Reward: 0.5578711003106442.\n",
      "Step: 480000. Mean Reward: 0.5373462433205012. Std of Reward: 0.5721930764262032.\n",
      "Saved Model\n",
      "Step: 485000. Mean Reward: 0.5785283917705679. Std of Reward: 0.4958359994468886.\n",
      "Step: 490000. Mean Reward: 0.6025720646501109. Std of Reward: 0.4817710681720034.\n",
      "Step: 495000. Mean Reward: 0.5686856538847754. Std of Reward: 0.5336735640173715.\n",
      "Step: 500000. Mean Reward: 0.5814777802557778. Std of Reward: 0.5330300538265509.\n",
      "Saved Model\n",
      "Step: 505000. Mean Reward: 0.5929431094160832. Std of Reward: 0.5066392713190652.\n",
      "Step: 510000. Mean Reward: 0.5547488606730594. Std of Reward: 0.5785527969979256.\n",
      "Step: 515000. Mean Reward: 0.5738742875954286. Std of Reward: 0.5410332614122768.\n",
      "Step: 520000. Mean Reward: 0.587385847259589. Std of Reward: 0.4830098668705283.\n",
      "Saved Model\n",
      "Step: 525000. Mean Reward: 0.6184768234875274. Std of Reward: 0.4651986898950338.\n",
      "Step: 530000. Mean Reward: 0.592110992631012. Std of Reward: 0.5359407993176964.\n",
      "Step: 535000. Mean Reward: 0.6306472515152103. Std of Reward: 0.47338576162748.\n",
      "Step: 540000. Mean Reward: 0.6203218907976394. Std of Reward: 0.46267915370747664.\n",
      "Saved Model\n",
      "Step: 545000. Mean Reward: 0.6215789497337272. Std of Reward: 0.4679933841507077.\n",
      "Step: 550000. Mean Reward: 0.6484575050607555. Std of Reward: 0.4802209970288748.\n",
      "Step: 555000. Mean Reward: 0.6802938223607903. Std of Reward: 0.40716955434074315.\n",
      "Step: 560000. Mean Reward: 0.6718352477757038. Std of Reward: 0.421485987247837.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \tlessonNr -> 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 565000. Mean Reward: -4.5196446687654825. Std of Reward: 11.65365084644356.\n",
      "Step: 570000. Mean Reward: -9.949238095209523. Std of Reward: 15.88555760344923.\n",
      "Step: 575000. Mean Reward: -6.159935483870964. Std of Reward: 12.863996895400348.\n",
      "Step: 580000. Mean Reward: -6.581677852295297. Std of Reward: 12.174063716955638.\n",
      "Saved Model\n",
      "Step: 585000. Mean Reward: -7.593700787149608. Std of Reward: 13.793057411281076.\n",
      "Step: 590000. Mean Reward: -8.759649122456139. Std of Reward: 14.158212539634883.\n",
      "Step: 595000. Mean Reward: -7.450735293882354. Std of Reward: 14.623771171332596.\n",
      "Step: 600000. Mean Reward: -10.314020618391751. Std of Reward: 15.809664058522046.\n",
      "Saved Model\n",
      "Step: 605000. Mean Reward: -6.13775757575757. Std of Reward: 12.335579059335199.\n",
      "Step: 610000. Mean Reward: -6.941493506233765. Std of Reward: 13.584198841035981.\n",
      "Step: 615000. Mean Reward: -5.554244185488371. Std of Reward: 10.618236889056194.\n",
      "Step: 620000. Mean Reward: -7.285857142228573. Std of Reward: 13.742044346151015.\n",
      "Saved Model\n",
      "Step: 625000. Mean Reward: -5.965408805031441. Std of Reward: 11.0075860486776.\n",
      "Step: 630000. Mean Reward: -5.246141303956519. Std of Reward: 10.898033268098123.\n",
      "Step: 635000. Mean Reward: -8.144462809454547. Std of Reward: 13.857967421679165.\n",
      "Step: 640000. Mean Reward: -6.054244185999999. Std of Reward: 11.829555713512269.\n",
      "Saved Model\n",
      "Step: 645000. Mean Reward: -5.496864864864863. Std of Reward: 11.698430012848828.\n",
      "Step: 650000. Mean Reward: -4.491559632954126. Std of Reward: 10.011803050114308.\n",
      "Step: 655000. Mean Reward: -7.544104477552233. Std of Reward: 13.697415662699909.\n",
      "Step: 660000. Mean Reward: -4.860353535313129. Std of Reward: 10.466591119923198.\n",
      "Saved Model\n",
      "Step: 665000. Mean Reward: -6.233877550680269. Std of Reward: 10.798169121844046.\n",
      "Step: 670000. Mean Reward: -5.674213483146064. Std of Reward: 11.09365886793083.\n",
      "Step: 675000. Mean Reward: -5.959681528509549. Std of Reward: 10.909205626932788.\n",
      "Step: 680000. Mean Reward: -4.587834101382484. Std of Reward: 9.40252634989639.\n",
      "Saved Model\n",
      "Step: 685000. Mean Reward: -4.213273542520176. Std of Reward: 8.363327622674873.\n",
      "Step: 690000. Mean Reward: -4.912268041237111. Std of Reward: 8.861467264086896.\n",
      "Step: 695000. Mean Reward: -3.4038970587352924. Std of Reward: 7.1425637667313655.\n",
      "Step: 700000. Mean Reward: -3.7086192468619212. Std of Reward: 7.148651054643247.\n",
      "Saved Model\n",
      "Step: 705000. Mean Reward: -4.106565217391301. Std of Reward: 8.269252052429902.\n",
      "Step: 710000. Mean Reward: -3.69282700411392. Std of Reward: 7.292374320403632.\n",
      "Step: 715000. Mean Reward: -3.218191881885607. Std of Reward: 6.026590406966834.\n",
      "Step: 720000. Mean Reward: -3.403895130936326. Std of Reward: 7.035470843785695.\n",
      "Saved Model\n",
      "Step: 725000. Mean Reward: -2.540396600543908. Std of Reward: 4.5304316762643815.\n",
      "Step: 730000. Mean Reward: -2.487431192510703. Std of Reward: 4.6764430274470286.\n",
      "Step: 735000. Mean Reward: -2.9259246574568483. Std of Reward: 5.078158727417521.\n",
      "Step: 740000. Mean Reward: -3.366523436910157. Std of Reward: 7.745674851315987.\n",
      "Saved Model\n",
      "Step: 745000. Mean Reward: -3.769285713720167. Std of Reward: 6.848035102472963.\n",
      "Step: 750000. Mean Reward: -3.4576356582015513. Std of Reward: 5.82780193481435.\n",
      "Step: 755000. Mean Reward: -3.4825296435652184. Std of Reward: 6.320662325595744.\n",
      "Step: 760000. Mean Reward: -3.3489219324832713. Std of Reward: 6.107935292612083.\n",
      "Saved Model\n",
      "Step: 765000. Mean Reward: -3.276418439088653. Std of Reward: 5.757162691814943.\n",
      "Step: 770000. Mean Reward: -2.341088234850001. Std of Reward: 4.287992980885779.\n",
      "Step: 775000. Mean Reward: -3.3029368020111525. Std of Reward: 7.2526078298482135.\n",
      "Step: 780000. Mean Reward: -2.213926701024608. Std of Reward: 4.9328183754522605.\n",
      "Saved Model\n",
      "Step: 785000. Mean Reward: -1.9308776591037242. Std of Reward: 4.268847820202702.\n",
      "Step: 790000. Mean Reward: -2.463011695011697. Std of Reward: 5.055433180793949.\n",
      "Step: 795000. Mean Reward: -1.7110843371142168. Std of Reward: 4.243486577741997.\n",
      "Step: 800000. Mean Reward: -1.9093233078724319. Std of Reward: 4.108339984065544.\n",
      "Saved Model\n",
      "Step: 805000. Mean Reward: -2.0601449269299517. Std of Reward: 4.197082891030489.\n",
      "Step: 810000. Mean Reward: -2.064304461498687. Std of Reward: 4.983098872454779.\n",
      "Step: 815000. Mean Reward: -2.1414249359567425. Std of Reward: 4.372594678026705.\n",
      "Step: 820000. Mean Reward: -1.7542567562860367. Std of Reward: 4.2193818258879805.\n",
      "Saved Model\n",
      "Step: 825000. Mean Reward: -1.5754226800020619. Std of Reward: 3.6680748152925555.\n",
      "Step: 830000. Mean Reward: -1.5859999995854168. Std of Reward: 3.5367342267472033.\n",
      "Step: 835000. Mean Reward: -1.5409456736458746. Std of Reward: 3.753924318647829.\n",
      "Step: 840000. Mean Reward: -1.155224137662069. Std of Reward: 2.1911515900878658.\n",
      "Saved Model\n",
      "Step: 845000. Mean Reward: -1.4438047804840637. Std of Reward: 3.2087215029508758.\n",
      "Step: 850000. Mean Reward: -1.3375365342242171. Std of Reward: 2.754627721766086.\n",
      "Step: 855000. Mean Reward: -1.5697807013684204. Std of Reward: 3.4956919525024968.\n",
      "Step: 860000. Mean Reward: -1.4146747963069104. Std of Reward: 2.730979024566712.\n",
      "Saved Model\n",
      "Step: 865000. Mean Reward: -1.2829979876277664. Std of Reward: 2.450409988855171.\n",
      "Step: 870000. Mean Reward: -1.045026737688057. Std of Reward: 1.7659495855615588.\n",
      "Step: 875000. Mean Reward: -1.1133273379136688. Std of Reward: 2.2471749424295044.\n",
      "Step: 880000. Mean Reward: -0.8647297295386824. Std of Reward: 1.2635876077733026.\n",
      "Saved Model\n",
      "Step: 885000. Mean Reward: -0.7799686519278997. Std of Reward: 1.107440919820374.\n",
      "Step: 890000. Mean Reward: -0.7934947048514372. Std of Reward: 0.9996168912866912.\n",
      "Step: 895000. Mean Reward: -0.5973615633377851. Std of Reward: 1.1486238227624108.\n",
      "Step: 900000. Mean Reward: -0.8550160768681672. Std of Reward: 1.4888027672672304.\n",
      "Saved Model\n",
      "Step: 905000. Mean Reward: -0.8654736837277194. Std of Reward: 1.494414231047885.\n",
      "Step: 910000. Mean Reward: -0.7274867253079647. Std of Reward: 1.633428897343445.\n",
      "Step: 915000. Mean Reward: -0.7266115700254546. Std of Reward: 1.330043032214905.\n",
      "Step: 920000. Mean Reward: -0.8431649829592593. Std of Reward: 1.446603271534939.\n",
      "Saved Model\n",
      "Step: 925000. Mean Reward: -0.7409374996610938. Std of Reward: 1.2101606534114184.\n",
      "Step: 930000. Mean Reward: -0.7259540629761485. Std of Reward: 1.5063802059901483.\n",
      "Step: 935000. Mean Reward: -0.57479559738978. Std of Reward: 1.2148661656665642.\n",
      "Step: 940000. Mean Reward: -0.6277482267338653. Std of Reward: 1.199318985274005.\n",
      "Saved Model\n",
      "Step: 945000. Mean Reward: -0.5172455087233534. Std of Reward: 1.2847185658315934.\n",
      "Step: 950000. Mean Reward: -0.3188399998236001. Std of Reward: 0.9701349671048947.\n",
      "Step: 955000. Mean Reward: -0.4205070992640974. Std of Reward: 1.3434417164913168.\n",
      "Step: 960000. Mean Reward: -0.354761029142647. Std of Reward: 1.7736271455973978.\n",
      "Saved Model\n",
      "Step: 965000. Mean Reward: -0.33186528479982735. Std of Reward: 1.0919475917442325.\n",
      "Step: 970000. Mean Reward: -0.2639122135893131. Std of Reward: 1.0115656369298822.\n",
      "Step: 975000. Mean Reward: -0.21057553918651087. Std of Reward: 0.9727214805531404.\n",
      "Step: 980000. Mean Reward: -0.21210332057785988. Std of Reward: 1.14666273392472.\n",
      "Saved Model\n",
      "Step: 985000. Mean Reward: -0.30812379062901357. Std of Reward: 1.4055297525531425.\n",
      "Step: 990000. Mean Reward: -0.33275390557578133. Std of Reward: 1.4496849162132939.\n",
      "Step: 995000. Mean Reward: -0.3619157085578543. Std of Reward: 2.155891190478281.\n",
      "Step: 1000000. Mean Reward: -0.1935604767400341. Std of Reward: 1.2335840343523428.\n",
      "Saved Model\n",
      "Step: 1005000. Mean Reward: -0.16685025786884689. Std of Reward: 1.1091763511180401.\n",
      "Step: 1010000. Mean Reward: -0.25274475497307697. Std of Reward: 1.1250936174034794.\n",
      "Step: 1015000. Mean Reward: -0.17043689303656964. Std of Reward: 1.2554288466590806.\n",
      "Step: 1020000. Mean Reward: -0.1380188677493711. Std of Reward: 0.9508135409410715.\n",
      "Saved Model\n",
      "Step: 1025000. Mean Reward: -0.15917591114389865. Std of Reward: 0.9805939299718383.\n",
      "Step: 1030000. Mean Reward: -0.20174836585980402. Std of Reward: 1.0041640665412257.\n",
      "Step: 1035000. Mean Reward: -0.0787057009593221. Std of Reward: 0.8700093979659395.\n",
      "Step: 1040000. Mean Reward: -0.04341426384855849. Std of Reward: 0.863568853805021.\n",
      "Saved Model\n",
      "Step: 1045000. Mean Reward: -0.14767594102422268. Std of Reward: 0.983194382877255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1050000. Mean Reward: -0.0745705520932516. Std of Reward: 0.9010455581145981.\n",
      "Step: 1055000. Mean Reward: -0.18576323984579446. Std of Reward: 1.0946288265275799.\n",
      "Step: 1060000. Mean Reward: -0.28602941161633993. Std of Reward: 1.2266942741217672.\n",
      "Saved Model\n",
      "Step: 1065000. Mean Reward: -0.13091180857339316. Std of Reward: 1.0164103014648866.\n",
      "Step: 1070000. Mean Reward: -0.06267543856578953. Std of Reward: 0.8661984260600837.\n",
      "Step: 1075000. Mean Reward: -0.08304225350619723. Std of Reward: 0.9523579548658169.\n",
      "Step: 1080000. Mean Reward: 0.013179792008023715. Std of Reward: 0.7960412781210972.\n",
      "Saved Model\n",
      "Step: 1085000. Mean Reward: -0.0017948717940171493. Std of Reward: 0.7669829693695671.\n",
      "Step: 1090000. Mean Reward: 0.03437415883499322. Std of Reward: 0.7387795799934797.\n",
      "Step: 1095000. Mean Reward: -0.02965425524601068. Std of Reward: 0.7942689584449661.\n",
      "Step: 1100000. Mean Reward: 0.054736147816622636. Std of Reward: 0.7180454793043919.\n",
      "Saved Model\n",
      "Step: 1105000. Mean Reward: 0.08840101529999995. Std of Reward: 0.6977112558702896.\n",
      "Step: 1110000. Mean Reward: 0.0492583120199488. Std of Reward: 0.732815899793619.\n",
      "Step: 1115000. Mean Reward: 0.031003764189209478. Std of Reward: 0.7625169615699173.\n",
      "Step: 1120000. Mean Reward: -0.07376756060434232. Std of Reward: 0.9587868728875083.\n",
      "Saved Model\n",
      "Step: 1125000. Mean Reward: -0.052704081586862184. Std of Reward: 1.5876941736493784.\n",
      "Step: 1130000. Mean Reward: 0.008624078638083486. Std of Reward: 0.7254037754792505.\n",
      "Step: 1135000. Mean Reward: 0.06084917621267422. Std of Reward: 0.724062902146155.\n",
      "Step: 1140000. Mean Reward: 0.02396063967158666. Std of Reward: 0.7644833182262761.\n",
      "Saved Model\n",
      "Step: 1145000. Mean Reward: 0.08156403944039405. Std of Reward: 0.7322032290158281.\n",
      "Step: 1150000. Mean Reward: 0.11196319022184044. Std of Reward: 0.676954800795311.\n",
      "Step: 1155000. Mean Reward: 0.10140243903951213. Std of Reward: 0.6917869113579941.\n",
      "Step: 1160000. Mean Reward: 0.10522784816126575. Std of Reward: 0.8687667011231517.\n",
      "Saved Model\n",
      "Step: 1165000. Mean Reward: 0.0829904306315789. Std of Reward: 0.6756210362382895.\n",
      "Step: 1170000. Mean Reward: 0.10099264709264702. Std of Reward: 0.647696801764563.\n",
      "Step: 1175000. Mean Reward: 0.08773333337406053. Std of Reward: 0.8348754661664921.\n",
      "Step: 1180000. Mean Reward: 0.05346475508841095. Std of Reward: 0.6936210047458212.\n",
      "Saved Model\n",
      "Step: 1185000. Mean Reward: 0.1383890214904534. Std of Reward: 0.6760787589349624.\n",
      "Step: 1190000. Mean Reward: 0.10043010753261644. Std of Reward: 0.6943565350770752.\n",
      "Step: 1195000. Mean Reward: 0.11214030917716998. Std of Reward: 0.7166613598393959.\n",
      "Step: 1200000. Mean Reward: 0.07309327041511211. Std of Reward: 0.7112050531572859.\n",
      "Saved Model\n",
      "Step: 1205000. Mean Reward: 0.11629239768023387. Std of Reward: 0.7212551980403014.\n",
      "Step: 1210000. Mean Reward: 0.0988180764878331. Std of Reward: 0.7121322367554449.\n",
      "Step: 1215000. Mean Reward: 0.06510791368944846. Std of Reward: 1.263174999319832.\n",
      "Step: 1220000. Mean Reward: 0.11070114945632177. Std of Reward: 0.6939282419287576.\n",
      "Saved Model\n",
      "Step: 1225000. Mean Reward: 0.10037844041467883. Std of Reward: 0.7004333317284912.\n",
      "Step: 1230000. Mean Reward: 0.12078546309144193. Std of Reward: 0.695085894436838.\n",
      "Step: 1235000. Mean Reward: 0.11212086660091215. Std of Reward: 0.6923837619945055.\n",
      "Step: 1240000. Mean Reward: 0.1896943231539301. Std of Reward: 0.7103977562360574.\n",
      "Saved Model\n",
      "Step: 1245000. Mean Reward: 0.23700873363318772. Std of Reward: 0.6837279042718186.\n",
      "Step: 1250000. Mean Reward: 0.22353319060192714. Std of Reward: 0.7028427596827576.\n",
      "Step: 1255000. Mean Reward: 0.26933402707284076. Std of Reward: 0.7016854792129238.\n",
      "Step: 1260000. Mean Reward: 0.2683213182646756. Std of Reward: 0.7084050082854026.\n",
      "Saved Model\n",
      "Step: 1265000. Mean Reward: 0.25657841140529525. Std of Reward: 0.7342259064712477.\n",
      "Step: 1270000. Mean Reward: 0.264130000045. Std of Reward: 0.7450766692880997.\n",
      "Step: 1275000. Mean Reward: 0.33074036511014193. Std of Reward: 0.6991106778561993.\n",
      "Step: 1280000. Mean Reward: 0.27728571429489796. Std of Reward: 0.7240711009171378.\n",
      "Saved Model\n",
      "Step: 1285000. Mean Reward: 0.228495486481344. Std of Reward: 0.8254618738187804.\n",
      "Step: 1290000. Mean Reward: 0.281942740295501. Std of Reward: 0.7222426649521752.\n",
      "Step: 1295000. Mean Reward: 0.27762386252275023. Std of Reward: 0.7433194997737885.\n",
      "Step: 1300000. Mean Reward: 0.2866175024582104. Std of Reward: 0.7396792763534219.\n",
      "Saved Model\n",
      "Step: 1305000. Mean Reward: 0.2876997112858517. Std of Reward: 0.783030441368798.\n",
      "Step: 1310000. Mean Reward: 0.2960754717150943. Std of Reward: 0.7512062463052875.\n",
      "Step: 1315000. Mean Reward: 0.3371324200995433. Std of Reward: 0.7566571494569929.\n",
      "Step: 1320000. Mean Reward: 0.38724787937379823. Std of Reward: 0.7399562891866663.\n",
      "Saved Model\n",
      "Step: 1325000. Mean Reward: 0.33770186337710734. Std of Reward: 0.7787232445412334.\n",
      "Step: 1330000. Mean Reward: 0.37115942030615934. Std of Reward: 0.7352032623054838.\n",
      "Step: 1335000. Mean Reward: 0.40282531194295895. Std of Reward: 0.740257424850018.\n",
      "Step: 1340000. Mean Reward: 0.4154749103942652. Std of Reward: 0.7302603308438734.\n",
      "Saved Model\n",
      "Step: 1345000. Mean Reward: 0.4071920758036175. Std of Reward: 0.7448432227284233.\n",
      "Step: 1350000. Mean Reward: 0.37160899654671276. Std of Reward: 0.786247827579381.\n",
      "Step: 1355000. Mean Reward: 0.41951515151515145. Std of Reward: 0.7592754170005378.\n",
      "Step: 1360000. Mean Reward: 0.4171158798360515. Std of Reward: 0.7416758134957132.\n",
      "Saved Model\n",
      "Step: 1365000. Mean Reward: 0.38574010328571423. Std of Reward: 0.7915828787761582.\n",
      "Step: 1370000. Mean Reward: 0.40927966102457625. Std of Reward: 0.746414075855283.\n",
      "Step: 1375000. Mean Reward: 0.37467881945225684. Std of Reward: 0.8432859223526348.\n",
      "Step: 1380000. Mean Reward: 0.3850171233013698. Std of Reward: 0.8768713840912049.\n",
      "Saved Model\n",
      "Step: 1385000. Mean Reward: 0.26734388748197. Std of Reward: 1.1753060112083467.\n",
      "Step: 1390000. Mean Reward: 0.3944664372358003. Std of Reward: 0.8822942432959479.\n",
      "Step: 1395000. Mean Reward: 0.3973484848627945. Std of Reward: 0.811978580504436.\n",
      "Step: 1400000. Mean Reward: 0.415451476814346. Std of Reward: 0.749199732522994.\n",
      "Saved Model\n",
      "Step: 1405000. Mean Reward: 0.36030807661698583. Std of Reward: 0.8054221879630807.\n",
      "Step: 1410000. Mean Reward: 0.38417162279184364. Std of Reward: 0.8137115200072397.\n",
      "Step: 1415000. Mean Reward: 0.38950384944396915. Std of Reward: 0.774253973133151.\n",
      "Step: 1420000. Mean Reward: 0.37857021996615897. Std of Reward: 0.827140756170042.\n",
      "Saved Model\n",
      "Step: 1425000. Mean Reward: 0.34912711864406776. Std of Reward: 0.8463749223982119.\n",
      "Step: 1430000. Mean Reward: 0.39823429541595917. Std of Reward: 0.7976523955251325.\n",
      "Step: 1435000. Mean Reward: 0.39173913044160263. Std of Reward: 0.7980444193348384.\n",
      "Step: 1440000. Mean Reward: 0.3776635514018691. Std of Reward: 0.8250853949091576.\n",
      "Saved Model\n",
      "Step: 1445000. Mean Reward: 0.36515638207945894. Std of Reward: 0.8465372488367303.\n",
      "Step: 1450000. Mean Reward: 0.3153703703771043. Std of Reward: 0.8724785396620949.\n",
      "Step: 1455000. Mean Reward: 0.42096121416526133. Std of Reward: 0.7409278765344468.\n",
      "Step: 1460000. Mean Reward: 0.43371935756551133. Std of Reward: 0.732151633354569.\n",
      "Saved Model\n",
      "Step: 1465000. Mean Reward: 0.417393126571668. Std of Reward: 0.7510802461436619.\n",
      "Step: 1470000. Mean Reward: 0.4344662681456874. Std of Reward: 0.7363611813675112.\n",
      "Step: 1475000. Mean Reward: 0.42163606010342236. Std of Reward: 0.7480186367285506.\n",
      "Step: 1480000. Mean Reward: 0.40864592094524804. Std of Reward: 0.7536559170554081.\n",
      "Saved Model\n",
      "Step: 1485000. Mean Reward: 0.42055413469616365. Std of Reward: 0.7522494483718968.\n",
      "Step: 1490000. Mean Reward: 0.4225481978206202. Std of Reward: 0.7423461671743031.\n",
      "Step: 1495000. Mean Reward: 0.45694048617686495. Std of Reward: 0.7151113095611344.\n",
      "Step: 1500000. Mean Reward: 0.44546835443037963. Std of Reward: 0.7306595858764581.\n",
      "Saved Model\n",
      "Step: 1505000. Mean Reward: 0.4254653130287648. Std of Reward: 0.7501046142932027.\n",
      "Step: 1510000. Mean Reward: 0.4066610878661087. Std of Reward: 0.7509321280054431.\n",
      "Step: 1515000. Mean Reward: 0.4491965811965811. Std of Reward: 0.7149546903259542.\n",
      "Step: 1520000. Mean Reward: 0.3866940789473684. Std of Reward: 0.7631549765640101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Step: 1525000. Mean Reward: 0.4153288925895087. Std of Reward: 0.7592098389706659.\n",
      "Step: 1530000. Mean Reward: 0.4004678362573099. Std of Reward: 0.7610936915790879.\n",
      "Step: 1535000. Mean Reward: 0.41377551020408154. Std of Reward: 0.7532994525755279.\n",
      "Step: 1540000. Mean Reward: 0.4413175675751688. Std of Reward: 0.7270777221315075.\n",
      "Saved Model\n",
      "Step: 1545000. Mean Reward: 0.46423469387755095. Std of Reward: 0.7146699628521809.\n",
      "Step: 1550000. Mean Reward: 0.40284289277556107. Std of Reward: 0.7613708145821312.\n",
      "Step: 1555000. Mean Reward: 0.4175972927241962. Std of Reward: 0.7513575847953872.\n",
      "Step: 1560000. Mean Reward: 0.4443240973971451. Std of Reward: 0.7208306564978403.\n",
      "Saved Model\n",
      "Step: 1565000. Mean Reward: 0.37007475083056474. Std of Reward: 0.8078038669352661.\n",
      "Step: 1570000. Mean Reward: 0.35580479452054786. Std of Reward: 0.8293283321114538.\n",
      "Step: 1575000. Mean Reward: 0.3867309304274936. Std of Reward: 0.7851830132623421.\n",
      "Step: 1580000. Mean Reward: 0.3771131447587354. Std of Reward: 0.7875266588796749.\n",
      "Saved Model\n",
      "Step: 1585000. Mean Reward: 0.4122036727879799. Std of Reward: 0.7676626832439332.\n",
      "Step: 1590000. Mean Reward: 0.4040118744698897. Std of Reward: 0.8141765140050315.\n",
      "Step: 1595000. Mean Reward: 0.33639798489336686. Std of Reward: 0.9158342364717287.\n",
      "Step: 1600000. Mean Reward: 0.3965664160401002. Std of Reward: 0.7870057826320876.\n",
      "Saved Model\n",
      "Step: 1605000. Mean Reward: 0.4209312080536912. Std of Reward: 0.744309883933674.\n",
      "Step: 1610000. Mean Reward: 0.41417785234781873. Std of Reward: 0.7529739662638001.\n",
      "Step: 1615000. Mean Reward: 0.4377991631799163. Std of Reward: 0.7531223218250134.\n",
      "Step: 1620000. Mean Reward: 0.37921797004991675. Std of Reward: 0.8023239254032096.\n",
      "Saved Model\n",
      "Step: 1625000. Mean Reward: 0.40427609427609423. Std of Reward: 0.7642884046763974.\n",
      "Step: 1630000. Mean Reward: 0.42856677524429965. Std of Reward: 0.7355576213123676.\n",
      "Step: 1635000. Mean Reward: 0.42676174496644287. Std of Reward: 0.737867472058355.\n",
      "Step: 1640000. Mean Reward: 0.41659999999999997. Std of Reward: 0.7501192838475758.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-583a438fd989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_variance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mrun_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_variance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons20/model-1640000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons20/model-1640000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
