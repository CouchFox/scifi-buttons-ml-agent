{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 3e6 # Set maximum number of steps to run environment.\n",
    "run_path = \"scifibuttons23\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 20000 # Frequency at which to save model.\n",
    "env_name = \"scifibuttons23\" # Name of the training environment file.\n",
    "curriculum_file = 'curricula/lessons21.json'\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 3 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 256 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tlessonNr -> 1.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 30\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 5\n",
      "        Memory space size (per agent): 9\n",
      "        Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: -1.8736708682105487. Std of Reward: 3.891727278369403.\n",
      "Step: 10000. Mean Reward: -1.64887548906506. Std of Reward: 3.948441636934019.\n",
      "Step: 15000. Mean Reward: -1.4975193691821707. Std of Reward: 2.8232580571667016.\n",
      "Step: 20000. Mean Reward: -1.3035888387034842. Std of Reward: 3.136480845124271.\n",
      "Saved Model\n",
      "Step: 25000. Mean Reward: -1.3949829238242322. Std of Reward: 3.760605443608874.\n",
      "Step: 30000. Mean Reward: -1.0003267874196078. Std of Reward: 2.949449018542418.\n",
      "Step: 35000. Mean Reward: -0.8518288999799412. Std of Reward: 2.9649326342665616.\n",
      "Step: 40000. Mean Reward: -1.226443652640845. Std of Reward: 3.7731839690499047.\n",
      "Saved Model\n",
      "Step: 45000. Mean Reward: -0.6242896837058495. Std of Reward: 2.5366985360722425.\n",
      "Step: 50000. Mean Reward: -0.9509972966512128. Std of Reward: 3.800385974673401.\n",
      "Step: 55000. Mean Reward: -0.5986595093270777. Std of Reward: 2.690758639500615.\n",
      "Step: 60000. Mean Reward: -0.6617955711566299. Std of Reward: 3.1285830442807687.\n",
      "Saved Model\n",
      "Step: 65000. Mean Reward: -0.49635677580025134. Std of Reward: 2.8645689546804856.\n",
      "Step: 70000. Mean Reward: -0.6199481771145078. Std of Reward: 2.586825617069559.\n",
      "Step: 75000. Mean Reward: -0.8041158428021342. Std of Reward: 2.8377295245269822.\n",
      "Step: 80000. Mean Reward: -0.3374324240180181. Std of Reward: 2.1861549651474066.\n",
      "Saved Model\n",
      "Step: 85000. Mean Reward: -0.7745697223112761. Std of Reward: 2.7011059023077566.\n",
      "Step: 90000. Mean Reward: -0.3227927849110361. Std of Reward: 2.098618116682992.\n",
      "Step: 95000. Mean Reward: -0.41339900836034477. Std of Reward: 2.8188174443937606.\n",
      "Step: 100000. Mean Reward: -0.797762420116851. Std of Reward: 3.3285238303756954.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: -0.5881095815032878. Std of Reward: 2.763918384514621.\n",
      "Step: 110000. Mean Reward: -0.4257279164422436. Std of Reward: 3.134122402149285.\n",
      "Step: 115000. Mean Reward: -0.07412007458840583. Std of Reward: 2.141192206666065.\n",
      "Step: 120000. Mean Reward: -0.4308172982257217. Std of Reward: 4.650807696531338.\n",
      "Saved Model\n",
      "Step: 125000. Mean Reward: -0.34167815094827586. Std of Reward: 2.6016003229442157.\n",
      "Step: 130000. Mean Reward: -0.05893876500265313. Std of Reward: 2.1301022340410722.\n",
      "Step: 135000. Mean Reward: 0.052537319522387965. Std of Reward: 1.9365539168471446.\n",
      "Step: 140000. Mean Reward: 0.0824000068406315. Std of Reward: 1.9341403827786692.\n",
      "Saved Model\n",
      "Step: 145000. Mean Reward: 0.13781250733786762. Std of Reward: 1.892808018684501.\n",
      "Step: 150000. Mean Reward: 0.0857266884696312. Std of Reward: 1.9044897617458592.\n",
      "Step: 155000. Mean Reward: 0.057154002644639226. Std of Reward: 2.6877877826817542.\n",
      "Step: 160000. Mean Reward: 0.1937109429449218. Std of Reward: 1.7920020531296112.\n",
      "Saved Model\n",
      "Step: 165000. Mean Reward: 0.2502307736588461. Std of Reward: 1.7773933787756313.\n",
      "Step: 170000. Mean Reward: 0.05097363774543614. Std of Reward: 3.288208544617067.\n",
      "Step: 175000. Mean Reward: -0.06326828734243924. Std of Reward: 2.3275153754666125.\n",
      "Step: 180000. Mean Reward: -0.20602596536883125. Std of Reward: 3.290477472210525.\n",
      "Saved Model\n",
      "Step: 185000. Mean Reward: -0.10145453765727276. Std of Reward: 2.428722238546854.\n",
      "Step: 190000. Mean Reward: 0.13478972607196252. Std of Reward: 2.2614954708487924.\n",
      "Step: 195000. Mean Reward: -0.07736180049346734. Std of Reward: 2.595434943729803.\n",
      "Step: 200000. Mean Reward: 0.26813627694849695. Std of Reward: 1.5540721303674034.\n",
      "Saved Model\n",
      "Step: 205000. Mean Reward: 0.07784841831149132. Std of Reward: 2.1288156104835116.\n",
      "Step: 210000. Mean Reward: 0.18048319993340328. Std of Reward: 2.2286455056496512.\n",
      "Step: 215000. Mean Reward: 0.3537568122199636. Std of Reward: 1.867245941366921.\n",
      "Step: 220000. Mean Reward: 0.4234426269291439. Std of Reward: 1.487055730187518.\n",
      "Saved Model\n",
      "Step: 225000. Mean Reward: 0.0628934099200508. Std of Reward: 2.806958095579495.\n",
      "Step: 230000. Mean Reward: 0.21755940197192208. Std of Reward: 2.2430843525984296.\n",
      "Step: 235000. Mean Reward: 0.11923628352673023. Std of Reward: 2.127973770217388.\n",
      "Step: 240000. Mean Reward: 0.3470285767394285. Std of Reward: 1.8614114026885564.\n",
      "Saved Model\n",
      "Step: 245000. Mean Reward: 0.20109054492921788. Std of Reward: 2.1268415062375707.\n",
      "Step: 250000. Mean Reward: 0.047366083300892745. Std of Reward: 2.8657584905762166.\n",
      "Step: 255000. Mean Reward: 0.34500000367723876. Std of Reward: 1.7947037495030407.\n",
      "Step: 260000. Mean Reward: 0.1839867916096915. Std of Reward: 2.2837804968218003.\n",
      "Saved Model\n",
      "Step: 265000. Mean Reward: 0.4150087976270652. Std of Reward: 2.4886991666368283.\n",
      "Step: 270000. Mean Reward: 0.4198327183420073. Std of Reward: 1.578217697455205.\n",
      "Step: 275000. Mean Reward: 0.27025105093974894. Std of Reward: 1.7534508422645676.\n",
      "Step: 280000. Mean Reward: 0.3629044882321637. Std of Reward: 1.9968456393411824.\n",
      "Saved Model\n",
      "Step: 285000. Mean Reward: 0.31666027550844505. Std of Reward: 1.9536443334991889.\n",
      "Step: 290000. Mean Reward: 0.5427515753185534. Std of Reward: 1.459535375172414.\n",
      "Step: 295000. Mean Reward: 0.43619601787807305. Std of Reward: 1.7164081949330552.\n",
      "Step: 300000. Mean Reward: 0.4445744728054965. Std of Reward: 1.8237432029537333.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 0.500298773767311. Std of Reward: 1.3841769853281252.\n",
      "Step: 310000. Mean Reward: 0.6233792077766055. Std of Reward: 1.0801180863429931.\n",
      "Step: 315000. Mean Reward: 0.2966666755977171. Std of Reward: 2.7312552490753514.\n",
      "Step: 320000. Mean Reward: 0.48542074964833654. Std of Reward: 1.4639187244551144.\n",
      "Saved Model\n",
      "Step: 325000. Mean Reward: 0.541304352422117. Std of Reward: 1.2859558220663807.\n",
      "Step: 330000. Mean Reward: 0.5127551053443877. Std of Reward: 1.575687802527073.\n",
      "Step: 335000. Mean Reward: 0.29461723027296643. Std of Reward: 1.836783754468255.\n",
      "Step: 340000. Mean Reward: 0.27591286825062233. Std of Reward: 2.724098778379727.\n",
      "Saved Model\n",
      "Step: 345000. Mean Reward: 0.5277234432451939. Std of Reward: 1.5823630619896032.\n",
      "Step: 350000. Mean Reward: 0.501047960466785. Std of Reward: 1.4825022440815523.\n",
      "Step: 355000. Mean Reward: 0.5930324917543321. Std of Reward: 1.2021048812046118.\n",
      "Step: 360000. Mean Reward: 0.6602439048883467. Std of Reward: 1.5013838807859499.\n",
      "Saved Model\n",
      "Step: 365000. Mean Reward: 0.4810956545655653. Std of Reward: 2.0756548236342813.\n",
      "Step: 370000. Mean Reward: 0.4997047277435039. Std of Reward: 1.4036145965403852.\n",
      "Step: 375000. Mean Reward: 0.43101952750108463. Std of Reward: 1.8240064507235643.\n",
      "Step: 380000. Mean Reward: 0.5719097247565972. Std of Reward: 1.3111340029978316.\n",
      "Saved Model\n",
      "Step: 385000. Mean Reward: 0.6380102058556122. Std of Reward: 1.2628590035236957.\n",
      "Step: 390000. Mean Reward: 0.6554461564375386. Std of Reward: 1.3723747513832347.\n",
      "Step: 395000. Mean Reward: 0.5096925893678118. Std of Reward: 1.8076501000348755.\n",
      "Step: 400000. Mean Reward: 0.6480918738855124. Std of Reward: 1.2422144320352648.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: 0.6970647502549641. Std of Reward: 1.23355460717607.\n",
      "Step: 410000. Mean Reward: 0.29567134471984013. Std of Reward: 5.36191670655079.\n",
      "Step: 415000. Mean Reward: 0.5561963197100205. Std of Reward: 1.3683540836645214.\n",
      "Step: 420000. Mean Reward: 0.5801372235660377. Std of Reward: 1.2762999798360366.\n",
      "Saved Model\n",
      "Step: 425000. Mean Reward: 0.5648825532087247. Std of Reward: 1.5736265243872636.\n",
      "Step: 430000. Mean Reward: 0.5921922448558179. Std of Reward: 1.5399089678554536.\n",
      "Step: 435000. Mean Reward: 0.40921739573782623. Std of Reward: 1.811919285716622.\n",
      "Step: 440000. Mean Reward: 0.37981383354122394. Std of Reward: 2.4285583840648517.\n",
      "Saved Model\n",
      "Step: 445000. Mean Reward: 0.1303421125034213. Std of Reward: 4.559303439401012.\n",
      "Step: 450000. Mean Reward: 0.025833332773413435. Std of Reward: 2.8803122965450214.\n",
      "Step: 455000. Mean Reward: 0.15519573745267018. Std of Reward: 3.4825129734891482.\n",
      "Step: 460000. Mean Reward: 0.09139535532354569. Std of Reward: 5.858397679164964.\n",
      "Saved Model\n",
      "Step: 465000. Mean Reward: 0.11338607939240601. Std of Reward: 3.8212652023319653.\n",
      "Step: 470000. Mean Reward: -0.08264705092731105. Std of Reward: 4.515347830181733.\n",
      "Step: 475000. Mean Reward: -0.1248161748062514. Std of Reward: 5.5023968008604.\n",
      "Step: 480000. Mean Reward: 0.28752976408244085. Std of Reward: 2.6998658299594887.\n",
      "Saved Model\n",
      "Step: 485000. Mean Reward: -0.03920884803132789. Std of Reward: 4.6818467785017885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 490000. Mean Reward: -0.4373029034663921. Std of Reward: 6.7104233419964014.\n",
      "Step: 495000. Mean Reward: -0.6007782034634269. Std of Reward: 6.948109393383422.\n",
      "Step: 500000. Mean Reward: -0.06440170327478544. Std of Reward: 3.196136098736738.\n",
      "Saved Model\n",
      "Step: 505000. Mean Reward: -0.027598245571178574. Std of Reward: 3.1663319782881274.\n",
      "Step: 510000. Mean Reward: -0.1709722217217594. Std of Reward: 3.32166939001777.\n",
      "Step: 515000. Mean Reward: -0.11175626090609123. Std of Reward: 5.509878152002203.\n",
      "Step: 520000. Mean Reward: -0.6879047464442838. Std of Reward: 8.233781340692326.\n",
      "Saved Model\n",
      "Step: 525000. Mean Reward: -0.6862121231037868. Std of Reward: 4.727371402378851.\n",
      "Step: 530000. Mean Reward: -1.5798019564564305. Std of Reward: 7.89650669216875.\n",
      "Step: 535000. Mean Reward: -0.1248936103114908. Std of Reward: 4.77858431945709.\n",
      "Step: 540000. Mean Reward: -0.20482051542615184. Std of Reward: 3.698105203987246.\n",
      "Saved Model\n",
      "Step: 545000. Mean Reward: -0.15198442379844335. Std of Reward: 3.4027149752690438.\n",
      "Step: 550000. Mean Reward: -0.6262048109656594. Std of Reward: 6.024413603974351.\n",
      "Step: 555000. Mean Reward: 0.021406849966160414. Std of Reward: 3.279166361308901.\n",
      "Step: 560000. Mean Reward: 0.03345833997125079. Std of Reward: 2.664993123895782.\n",
      "Saved Model\n",
      "Step: 565000. Mean Reward: 0.17036066332098435. Std of Reward: 4.027632679455367.\n",
      "Step: 570000. Mean Reward: -0.283355260927634. Std of Reward: 4.465865411774702.\n",
      "Step: 575000. Mean Reward: -0.16629481657211076. Std of Reward: 5.557914211027658.\n",
      "Step: 580000. Mean Reward: -0.3267222240405552. Std of Reward: 4.623961137826199.\n",
      "Saved Model\n",
      "Step: 585000. Mean Reward: -0.3172799864263987. Std of Reward: 5.273741013953551.\n",
      "Step: 590000. Mean Reward: -1.9791208714055024. Std of Reward: 8.898833558919481.\n",
      "Step: 595000. Mean Reward: -0.7254901688607954. Std of Reward: 10.908305895670013.\n",
      "Step: 600000. Mean Reward: -0.5665151461772697. Std of Reward: 4.475859102614762.\n",
      "Saved Model\n",
      "Step: 605000. Mean Reward: -1.0766423245766399. Std of Reward: 5.609060835703571.\n",
      "Step: 610000. Mean Reward: -0.09533595638577008. Std of Reward: 5.108639495166455.\n",
      "Step: 615000. Mean Reward: -1.0472856876707208. Std of Reward: 8.766614366759198.\n",
      "Step: 620000. Mean Reward: -2.389577453322543. Std of Reward: 9.855139246425958.\n",
      "Saved Model\n",
      "Step: 625000. Mean Reward: -2.3225531698723363. Std of Reward: 11.276863691778852.\n",
      "Step: 630000. Mean Reward: -2.671162795230231. Std of Reward: 7.84513329914146.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-583a438fd989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, memory, value)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"STEP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"brain_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"RECEIVED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mmessage_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons20/model-1640000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons20/model-1640000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
