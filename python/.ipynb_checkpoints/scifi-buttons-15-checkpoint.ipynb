{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 3e6 # Set maximum number of steps to run environment.\n",
    "run_path = \"scifibuttons15\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 20000 # Frequency at which to save model.\n",
    "env_name = \"scifibuttons15\" # Name of the training environment file.\n",
    "curriculum_file = 'curricula/lessons11.json'\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 128 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tlessonNr -> 1.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 30\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 5\n",
      "        Memory space size (per agent): 3\n",
      "        Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: -0.7482879166264591. Std of Reward: 2.5242344495271256.\n",
      "Step: 10000. Mean Reward: -0.8719665048456068. Std of Reward: 2.8264410085315648.\n",
      "Step: 15000. Mean Reward: -0.7937142634604083. Std of Reward: 2.713473328363231.\n",
      "Step: 20000. Mean Reward: -0.9124291275700405. Std of Reward: 2.694604973157485.\n",
      "Saved Model\n",
      "Step: 25000. Mean Reward: -0.40887752987619036. Std of Reward: 2.170854004339262.\n",
      "Step: 30000. Mean Reward: -0.33583331400902783. Std of Reward: 2.042219293701454.\n",
      "Step: 35000. Mean Reward: -0.3741957865451049. Std of Reward: 2.113633188293018.\n",
      "Step: 40000. Mean Reward: -0.13908569984600008. Std of Reward: 1.9481984992459869.\n",
      "Saved Model\n",
      "Step: 45000. Mean Reward: -0.09937335325770227. Std of Reward: 2.0637964835315867.\n",
      "Step: 50000. Mean Reward: -0.11096857318455502. Std of Reward: 1.9127300999585797.\n",
      "Step: 55000. Mean Reward: 0.1166210167570776. Std of Reward: 1.7067613199185623.\n",
      "Step: 60000. Mean Reward: 0.18502084358812498. Std of Reward: 1.587839847648637.\n",
      "Saved Model\n",
      "Step: 65000. Mean Reward: 0.3853747814700183. Std of Reward: 1.3431207184998584.\n",
      "Step: 70000. Mean Reward: 0.46430669636557914. Std of Reward: 1.1714094194587263.\n",
      "Step: 75000. Mean Reward: 0.5235164910742544. Std of Reward: 1.0381806269819107.\n",
      "Step: 80000. Mean Reward: 0.6574899394063087. Std of Reward: 0.8654765517210239.\n",
      "Saved Model\n",
      "Step: 85000. Mean Reward: 0.6247745417448276. Std of Reward: 0.9322292967762101.\n",
      "Step: 90000. Mean Reward: 0.7231737242772828. Std of Reward: 0.7583925728740061.\n",
      "Step: 95000. Mean Reward: 0.7052045504845453. Std of Reward: 0.7722764131308223.\n",
      "Step: 100000. Mean Reward: 0.7871737037212955. Std of Reward: 0.626496915714144.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: 0.8376030958884021. Std of Reward: 0.5831124332164755.\n",
      "Step: 110000. Mean Reward: 0.8559774765219629. Std of Reward: 0.4971101390530814.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \tlessonNr -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 115000. Mean Reward: 0.41561972295535204. Std of Reward: 0.7954441385172546.\n",
      "Step: 120000. Mean Reward: 0.25373723096671535. Std of Reward: 0.8819982568592127.\n",
      "Saved Model\n",
      "Step: 125000. Mean Reward: 0.23963768599318835. Std of Reward: 0.8859317623076902.\n",
      "Step: 130000. Mean Reward: 0.360143271296275. Std of Reward: 0.8317792461589396.\n",
      "Step: 135000. Mean Reward: 0.3181232922536986. Std of Reward: 0.8658177940817362.\n",
      "Step: 140000. Mean Reward: 0.3760709633888305. Std of Reward: 0.8426249215148953.\n",
      "Saved Model\n",
      "Step: 145000. Mean Reward: 0.3787305741490932. Std of Reward: 0.8482092971048766.\n",
      "Step: 150000. Mean Reward: 0.38524390636268285. Std of Reward: 0.8548354946396146.\n",
      "Step: 155000. Mean Reward: 0.37908434117469875. Std of Reward: 0.8620245892328214.\n",
      "Step: 160000. Mean Reward: 0.39602854133210463. Std of Reward: 0.8628426437321808.\n",
      "Saved Model\n",
      "Step: 165000. Mean Reward: 0.37647059203799993. Std of Reward: 0.8681442094654186.\n",
      "Step: 170000. Mean Reward: 0.43128701945899767. Std of Reward: 0.8387496439099006.\n",
      "Step: 175000. Mean Reward: 0.48238857496937143. Std of Reward: 0.8119167509566313.\n",
      "Step: 180000. Mean Reward: 0.4759843435605145. Std of Reward: 0.8177885862483177.\n",
      "Saved Model\n",
      "Step: 185000. Mean Reward: 0.4398066626273898. Std of Reward: 0.8436952421698358.\n",
      "Step: 190000. Mean Reward: 0.42877593677831943. Std of Reward: 0.8583974728927883.\n",
      "Step: 195000. Mean Reward: 0.41541961867937305. Std of Reward: 0.8727025033645552.\n",
      "Step: 200000. Mean Reward: 0.47113963357433264. Std of Reward: 0.8458616717968712.\n",
      "Saved Model\n",
      "Step: 205000. Mean Reward: 0.5421552637510725. Std of Reward: 0.7998736841565253.\n",
      "Step: 210000. Mean Reward: 0.569046187854518. Std of Reward: 0.7758711876830395.\n",
      "Step: 215000. Mean Reward: 0.596726016922453. Std of Reward: 0.7547929704759944.\n",
      "Step: 220000. Mean Reward: 0.5692337567543161. Std of Reward: 0.7896533244925142.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \tlessonNr -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 225000. Mean Reward: 0.6384556015031853. Std of Reward: 0.7273243465471805.\n",
      "Step: 230000. Mean Reward: -0.0647121009707403. Std of Reward: 0.8308775298171479.\n",
      "Step: 235000. Mean Reward: -0.13120092150277138. Std of Reward: 0.8492191448749941.\n",
      "Step: 240000. Mean Reward: -0.08532088449124854. Std of Reward: 0.8409204400692167.\n",
      "Saved Model\n",
      "Step: 245000. Mean Reward: -0.11328813343152543. Std of Reward: 0.8471146999256174.\n",
      "Step: 250000. Mean Reward: -0.02214866200452962. Std of Reward: 0.8278468825200351.\n",
      "Step: 255000. Mean Reward: 0.044356555091735535. Std of Reward: 0.8112722861935833.\n",
      "Step: 260000. Mean Reward: 0.041120383979261016. Std of Reward: 0.8180921645697599.\n",
      "Saved Model\n",
      "Step: 265000. Mean Reward: 0.010654422706888628. Std of Reward: 0.8315012971092043.\n",
      "Step: 270000. Mean Reward: 0.0536990974280543. Std of Reward: 0.8274975945458417.\n",
      "Step: 275000. Mean Reward: 0.03971330527259174. Std of Reward: 0.8164177109890479.\n",
      "Step: 280000. Mean Reward: 0.034328020627790425. Std of Reward: 0.8192924022094082.\n",
      "Saved Model\n",
      "Step: 285000. Mean Reward: 0.013944136465139659. Std of Reward: 0.8291236400328555.\n",
      "Step: 290000. Mean Reward: 0.0892881379977401. Std of Reward: 0.8176737385240037.\n",
      "Step: 295000. Mean Reward: 0.10334479058487973. Std of Reward: 0.8142183194713573.\n",
      "Step: 300000. Mean Reward: 0.127068586454646. Std of Reward: 0.8085234093727137.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 0.0758305486753623. Std of Reward: 0.8144678438815854.\n",
      "Step: 310000. Mean Reward: 0.13941431889154013. Std of Reward: 0.8007243774061102.\n",
      "Step: 315000. Mean Reward: 0.1107505305769556. Std of Reward: 0.8162183780513191.\n",
      "Step: 320000. Mean Reward: 0.13606217820963729. Std of Reward: 0.813883696547505.\n",
      "Saved Model\n",
      "Step: 325000. Mean Reward: 0.1499562864089617. Std of Reward: 0.7892879823323455.\n",
      "Step: 330000. Mean Reward: 0.13736842311578948. Std of Reward: 0.8100306834375788.\n",
      "Step: 335000. Mean Reward: 0.16510846210444685. Std of Reward: 0.7907223974258638.\n",
      "Step: 340000. Mean Reward: 0.20038420708239058. Std of Reward: 0.7893304399345201.\n",
      "Saved Model\n",
      "Step: 345000. Mean Reward: 0.1253788709683031. Std of Reward: 0.8159870650730671.\n",
      "Step: 350000. Mean Reward: 0.225841272058836. Std of Reward: 0.7828673099934437.\n",
      "Step: 355000. Mean Reward: 0.18694093042964136. Std of Reward: 0.7936037352058853.\n",
      "Step: 360000. Mean Reward: 0.20721812651306637. Std of Reward: 0.7942389092966532.\n",
      "Saved Model\n",
      "Step: 365000. Mean Reward: 0.20569166005755013. Std of Reward: 0.7857078834345592.\n",
      "Step: 370000. Mean Reward: 0.22081309598204857. Std of Reward: 0.7757215364599664.\n",
      "Step: 375000. Mean Reward: 0.2133264269737824. Std of Reward: 0.782220582794612.\n",
      "Step: 380000. Mean Reward: 0.2585885905139139. Std of Reward: 0.7818892901590933.\n",
      "Saved Model\n",
      "Step: 385000. Mean Reward: 0.29866321455378236. Std of Reward: 0.7500325220922154.\n",
      "Step: 390000. Mean Reward: 0.3157994060819265. Std of Reward: 0.7576799982572163.\n",
      "Step: 395000. Mean Reward: 0.32864097565507094. Std of Reward: 0.7404156910794267.\n",
      "Step: 400000. Mean Reward: 0.3335685503597782. Std of Reward: 0.7312511155834799.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: 0.35073964687790926. Std of Reward: 0.7337955865454137.\n",
      "Step: 410000. Mean Reward: 0.3298800019869. Std of Reward: 0.7390422090497963.\n",
      "Step: 415000. Mean Reward: 0.3868348190235411. Std of Reward: 0.7164162875982903.\n",
      "Step: 420000. Mean Reward: 0.3740392176304902. Std of Reward: 0.717145184123774.\n",
      "Saved Model\n",
      "Step: 425000. Mean Reward: 0.3969901979627451. Std of Reward: 0.7022160027737223.\n",
      "Step: 430000. Mean Reward: 0.40521442698703697. Std of Reward: 0.7060058450619096.\n",
      "Step: 435000. Mean Reward: 0.39436715165942027. Std of Reward: 0.7056299898617441.\n",
      "Step: 440000. Mean Reward: 0.37848429011811463. Std of Reward: 0.7343180694500585.\n",
      "Saved Model\n",
      "Step: 445000. Mean Reward: 0.4336328891794455. Std of Reward: 0.6872574113705205.\n",
      "Step: 450000. Mean Reward: 0.40448571622038093. Std of Reward: 0.7096075194746794.\n",
      "Step: 455000. Mean Reward: 0.4128190494654285. Std of Reward: 0.7029815797330525.\n",
      "Step: 460000. Mean Reward: 0.380314887374332. Std of Reward: 0.7197465608150327.\n",
      "Saved Model\n",
      "Step: 465000. Mean Reward: 0.41100094607780924. Std of Reward: 0.7099726283832148.\n",
      "Step: 470000. Mean Reward: 0.44394538803145006. Std of Reward: 0.682542997396405.\n",
      "Step: 475000. Mean Reward: 0.47926276183638933. Std of Reward: 0.6491252116287763.\n",
      "Step: 480000. Mean Reward: 0.5731603793707546. Std of Reward: 0.5579987093261336.\n",
      "Saved Model\n",
      "Step: 485000. Mean Reward: 0.6690616393926402. Std of Reward: 0.44422662908971794.\n",
      "Step: 490000. Mean Reward: 0.693777779787963. Std of Reward: 0.3993758864949247.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \tlessonNr -> 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 495000. Mean Reward: 0.630952382760516. Std of Reward: 1.3968302715137288.\n",
      "Step: 500000. Mean Reward: -7.461028037383178. Std of Reward: 12.967790575155188.\n",
      "Saved Model\n",
      "Step: 505000. Mean Reward: -6.209661016922029. Std of Reward: 10.324024058807614.\n",
      "Step: 510000. Mean Reward: -5.759922480570535. Std of Reward: 9.145047416100432.\n",
      "Step: 515000. Mean Reward: -6.078728813403388. Std of Reward: 10.149463085705479.\n",
      "Step: 520000. Mean Reward: -4.288025477707007. Std of Reward: 9.506441894974994.\n",
      "Saved Model\n",
      "Step: 525000. Mean Reward: -4.612137930824822. Std of Reward: 8.656639244930016.\n",
      "Step: 530000. Mean Reward: -3.7682352938752897. Std of Reward: 6.712738755011329.\n",
      "Step: 535000. Mean Reward: -3.162941176470587. Std of Reward: 6.139258382615801.\n",
      "Step: 540000. Mean Reward: -3.2527173912586926. Std of Reward: 6.34454466167567.\n",
      "Saved Model\n",
      "Step: 545000. Mean Reward: -3.149378238341966. Std of Reward: 5.903861880637143.\n",
      "Step: 550000. Mean Reward: -1.7215589353307978. Std of Reward: 3.912320142918167.\n",
      "Step: 555000. Mean Reward: -1.3891808873610918. Std of Reward: 3.2807117575724867.\n",
      "Step: 560000. Mean Reward: -1.305803275803935. Std of Reward: 4.835767191999434.\n",
      "Saved Model\n",
      "Step: 565000. Mean Reward: -0.5728010470848166. Std of Reward: 2.1968325025590922.\n",
      "Step: 570000. Mean Reward: -0.3901869158635515. Std of Reward: 1.8317156460872865.\n",
      "Step: 575000. Mean Reward: -0.18935684592946073. Std of Reward: 2.5662189759058798.\n",
      "Step: 580000. Mean Reward: 0.006752988191235037. Std of Reward: 1.57104997465745.\n",
      "Saved Model\n",
      "Step: 585000. Mean Reward: -0.16027026887606308. Std of Reward: 5.875237915852028.\n",
      "Step: 590000. Mean Reward: 0.24843902444813004. Std of Reward: 1.1396487131155695.\n",
      "Step: 595000. Mean Reward: 0.16323432408712873. Std of Reward: 2.873053545580581.\n",
      "Step: 600000. Mean Reward: 0.37005934723382783. Std of Reward: 0.8525507178545856.\n",
      "Saved Model\n",
      "Step: 605000. Mean Reward: 0.26953846166399953. Std of Reward: 3.3862505359895945.\n",
      "Step: 610000. Mean Reward: 0.07156549568530238. Std of Reward: 5.89199256526587.\n",
      "Step: 615000. Mean Reward: -0.017714776354984564. Std of Reward: 6.5870286513396.\n",
      "Step: 620000. Mean Reward: -0.04504159674742159. Std of Reward: 5.166303530605795.\n",
      "Saved Model\n",
      "Step: 625000. Mean Reward: 0.3655192036594595. Std of Reward: 1.4246614619762976.\n",
      "Step: 630000. Mean Reward: 0.06640650424975596. Std of Reward: 5.810730829137243.\n",
      "Step: 635000. Mean Reward: -0.03655290064129706. Std of Reward: 4.444122901213537.\n",
      "Step: 640000. Mean Reward: 0.09970312632999953. Std of Reward: 3.9024175978941877.\n",
      "Saved Model\n",
      "Step: 645000. Mean Reward: 0.14047546067239267. Std of Reward: 3.0824121941298706.\n",
      "Step: 650000. Mean Reward: 0.21340361468012037. Std of Reward: 3.0385228708190883.\n",
      "Step: 655000. Mean Reward: 0.295528220036758. Std of Reward: 2.8279585335206137.\n",
      "Step: 660000. Mean Reward: 0.3142026828301043. Std of Reward: 1.3636656923836623.\n",
      "Saved Model\n",
      "Step: 665000. Mean Reward: 0.3555337080367976. Std of Reward: 2.304534729767105.\n",
      "Step: 670000. Mean Reward: 0.16807575794969692. Std of Reward: 3.663919874507087.\n",
      "Step: 675000. Mean Reward: -0.1328973504960274. Std of Reward: 5.451168112003304.\n",
      "Step: 680000. Mean Reward: 0.10926717588396954. Std of Reward: 3.8109326541498127.\n",
      "Saved Model\n",
      "Step: 685000. Mean Reward: 0.4175033738421052. Std of Reward: 1.8679561093497055.\n",
      "Step: 690000. Mean Reward: -0.1692834383433143. Std of Reward: 9.46532104179491.\n",
      "Step: 695000. Mean Reward: 0.1498074075585179. Std of Reward: 4.899498426986464.\n",
      "Step: 700000. Mean Reward: 0.0459665148009125. Std of Reward: 4.8905488313810315.\n",
      "Saved Model\n",
      "Step: 705000. Mean Reward: 0.4707780612244898. Std of Reward: 0.7954056735006719.\n",
      "Step: 710000. Mean Reward: 0.27860335201005576. Std of Reward: 2.586104359325581.\n",
      "Step: 715000. Mean Reward: 0.38051248362312756. Std of Reward: 1.9075531790099851.\n",
      "Step: 720000. Mean Reward: 0.4737984496656331. Std of Reward: 1.020917300708999.\n",
      "Saved Model\n",
      "Step: 725000. Mean Reward: 0.2408993290437583. Std of Reward: 3.442221010984907.\n",
      "Step: 730000. Mean Reward: 0.4762389938042767. Std of Reward: 1.1268935729926584.\n",
      "Step: 735000. Mean Reward: 0.3732092427386393. Std of Reward: 2.3716851263569407.\n",
      "Step: 740000. Mean Reward: 0.4190724270066074. Std of Reward: 1.279661953946511.\n",
      "Saved Model\n",
      "Step: 745000. Mean Reward: 0.5394159714703218. Std of Reward: 0.9064673030199907.\n",
      "Step: 750000. Mean Reward: 0.5153140097374397. Std of Reward: 0.8307377093968938.\n",
      "Step: 755000. Mean Reward: 0.47496962339902793. Std of Reward: 1.6612541820927358.\n",
      "Step: 760000. Mean Reward: 0.5497393365042653. Std of Reward: 0.8305749041487631.\n",
      "Saved Model\n",
      "Step: 765000. Mean Reward: 0.43829268390121956. Std of Reward: 1.8531183815726253.\n",
      "Step: 770000. Mean Reward: 0.5746543779313364. Std of Reward: 0.8180920786591456.\n",
      "Step: 775000. Mean Reward: 0.49802600477446807. Std of Reward: 0.9812708282897863.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 4 : \tlessonNr -> 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 780000. Mean Reward: -5.873873797531515. Std of Reward: 17.64720541409738.\n",
      "Saved Model\n",
      "Step: 785000. Mean Reward: -26.119310154172346. Std of Reward: 30.179787703125033.\n",
      "Step: 790000. Mean Reward: -36.52142839499991. Std of Reward: 28.77854161757129.\n",
      "Step: 795000. Mean Reward: -26.311999757633274. Std of Reward: 28.181906189405783.\n",
      "Step: 800000. Mean Reward: -18.48999978919042. Std of Reward: 22.733035505521514.\n",
      "Saved Model\n",
      "Step: 805000. Mean Reward: -37.43949992019995. Std of Reward: 34.096218704630836.\n",
      "Step: 810000. Mean Reward: -29.8896152877307. Std of Reward: 30.240261125730903.\n",
      "Step: 815000. Mean Reward: -27.78518505262958. Std of Reward: 29.788903738507077.\n",
      "Step: 820000. Mean Reward: -28.23321412017851. Std of Reward: 27.18339958839236.\n",
      "Saved Model\n",
      "Step: 825000. Mean Reward: -18.05804871153654. Std of Reward: 23.929731446269727.\n",
      "Step: 830000. Mean Reward: -16.42456516167386. Std of Reward: 22.44382321027781.\n",
      "Step: 835000. Mean Reward: -20.081578899184162. Std of Reward: 22.75001409036314.\n",
      "Step: 840000. Mean Reward: -18.454390203146286. Std of Reward: 23.54934987535157.\n",
      "Saved Model\n",
      "Step: 845000. Mean Reward: -19.21924996334995. Std of Reward: 22.465888320991194.\n",
      "Step: 850000. Mean Reward: -20.006410224256356. Std of Reward: 24.180418725765687.\n",
      "Step: 855000. Mean Reward: -13.64421047808768. Std of Reward: 18.157158092810157.\n",
      "Step: 860000. Mean Reward: -12.174193508467706. Std of Reward: 17.11118487017847.\n",
      "Saved Model\n",
      "Step: 865000. Mean Reward: -12.417586160448247. Std of Reward: 19.381565236279677.\n",
      "Step: 870000. Mean Reward: -15.17163261297955. Std of Reward: 23.38953687214157.\n",
      "Step: 875000. Mean Reward: -11.657656217671853. Std of Reward: 17.77695198105938.\n",
      "Step: 880000. Mean Reward: -10.426805520722201. Std of Reward: 14.98904310546449.\n",
      "Saved Model\n",
      "Step: 885000. Mean Reward: -13.746785685785673. Std of Reward: 17.48367526682347.\n",
      "Step: 890000. Mean Reward: -12.969999978339247. Std of Reward: 15.492391464065914.\n",
      "Step: 895000. Mean Reward: -9.162795677053747. Std of Reward: 12.293256777073893.\n",
      "Step: 900000. Mean Reward: -10.955714266999983. Std of Reward: 14.861405783161905.\n",
      "Saved Model\n",
      "Step: 905000. Mean Reward: -8.115566988845357. Std of Reward: 11.084559982804358.\n",
      "Step: 910000. Mean Reward: -6.045378135420165. Std of Reward: 8.875718538914455.\n",
      "Step: 915000. Mean Reward: -5.93151077453957. Std of Reward: 8.366156383199215.\n",
      "Step: 920000. Mean Reward: -3.8702840739943194. Std of Reward: 6.318872930196653.\n",
      "Saved Model\n",
      "Step: 925000. Mean Reward: -3.7298918762702717. Std of Reward: 5.430070434104025.\n",
      "Step: 930000. Mean Reward: -3.527268278062442. Std of Reward: 7.245776456890832.\n",
      "Step: 935000. Mean Reward: -3.4074038321586544. Std of Reward: 5.788069878305947.\n",
      "Step: 940000. Mean Reward: -3.1735714145491087. Std of Reward: 4.574717279380968.\n",
      "Saved Model\n",
      "Step: 945000. Mean Reward: -2.5721011539766545. Std of Reward: 4.0263811345861535.\n",
      "Step: 950000. Mean Reward: -2.774387337600791. Std of Reward: 5.453140238727437.\n",
      "Step: 955000. Mean Reward: -2.4042605519859164. Std of Reward: 4.101419953429349.\n",
      "Step: 960000. Mean Reward: -2.088133321313334. Std of Reward: 3.344494763955441.\n",
      "Saved Model\n",
      "Step: 965000. Mean Reward: -2.114426736872612. Std of Reward: 3.6843960659188175.\n",
      "Step: 970000. Mean Reward: -2.092933319863334. Std of Reward: 3.23234745993662.\n",
      "Step: 975000. Mean Reward: -2.189795209054608. Std of Reward: 3.92211076689757.\n",
      "Step: 980000. Mean Reward: -2.0949508067049183. Std of Reward: 3.4962000139165235.\n",
      "Saved Model\n",
      "Step: 985000. Mean Reward: -1.5223116743610392. Std of Reward: 2.4433552900215094.\n",
      "Step: 990000. Mean Reward: -1.5819363279442973. Std of Reward: 2.154856768392504.\n",
      "Step: 995000. Mean Reward: -1.4422409524313256. Std of Reward: 2.4405840838648674.\n",
      "Step: 1000000. Mean Reward: -1.4117782794503466. Std of Reward: 2.765545822436964.\n",
      "Saved Model\n",
      "Step: 1005000. Mean Reward: -0.9458222953913044. Std of Reward: 1.3398548858130193.\n",
      "Step: 1010000. Mean Reward: -0.8555323067129278. Std of Reward: 1.3453745669272046.\n",
      "Step: 1015000. Mean Reward: -0.8977323310130111. Std of Reward: 1.5298226467957003.\n",
      "Step: 1020000. Mean Reward: -0.902481739958029. Std of Reward: 2.3359912548261046.\n",
      "Saved Model\n",
      "Step: 1025000. Mean Reward: -0.7207364865662848. Std of Reward: 1.2128999614433607.\n",
      "Step: 1030000. Mean Reward: -0.7954453666537815. Std of Reward: 1.8874633512890473.\n",
      "Step: 1035000. Mean Reward: -0.7777846052692308. Std of Reward: 1.3857230379474716.\n",
      "Step: 1040000. Mean Reward: -0.601700869095308. Std of Reward: 0.9190448616964545.\n",
      "Saved Model\n",
      "Step: 1045000. Mean Reward: -0.5263095123392858. Std of Reward: 0.9854734002580304.\n",
      "Step: 1050000. Mean Reward: -0.5576680866523605. Std of Reward: 0.8823305316113773.\n",
      "Step: 1055000. Mean Reward: -0.5445120122008487. Std of Reward: 0.8504811166318239.\n",
      "Step: 1060000. Mean Reward: -0.563657847922114. Std of Reward: 0.8621128192658677.\n",
      "Saved Model\n",
      "Step: 1065000. Mean Reward: -0.6316511225299601. Std of Reward: 0.814503323890185.\n",
      "Step: 1070000. Mean Reward: -0.5133923603814714. Std of Reward: 0.8090237533597556.\n",
      "Step: 1075000. Mean Reward: -0.5017614315825243. Std of Reward: 0.9204062516625028.\n",
      "Step: 1080000. Mean Reward: -0.5575549347664835. Std of Reward: 0.9235174171673151.\n",
      "Saved Model\n",
      "Step: 1085000. Mean Reward: -0.5044201806289224. Std of Reward: 0.8092884125511194.\n",
      "Step: 1090000. Mean Reward: -0.550201332424161. Std of Reward: 0.9281460778978424.\n",
      "Step: 1095000. Mean Reward: -0.5992452730795149. Std of Reward: 1.2206334659689262.\n",
      "Step: 1100000. Mean Reward: -0.5318799899266667. Std of Reward: 0.8715415152155992.\n",
      "Saved Model\n",
      "Step: 1105000. Mean Reward: -0.6560222421933241. Std of Reward: 2.0824604888056726.\n",
      "Step: 1110000. Mean Reward: -0.5461366527568989. Std of Reward: 0.7957921971417081.\n",
      "Step: 1115000. Mean Reward: -0.513670202800532. Std of Reward: 0.7894550044769263.\n",
      "Step: 1120000. Mean Reward: -0.48669364672739546. Std of Reward: 0.7939011881382905.\n",
      "Saved Model\n",
      "Step: 1125000. Mean Reward: -0.5406568261648794. Std of Reward: 0.9713579005716181.\n",
      "Step: 1130000. Mean Reward: -0.5660077426136951. Std of Reward: 0.8008535300122976.\n",
      "Step: 1135000. Mean Reward: -0.5152917674403183. Std of Reward: 0.79300974289146.\n",
      "Step: 1140000. Mean Reward: -0.4682496502211669. Std of Reward: 0.811148784972863.\n",
      "Saved Model\n",
      "Step: 1145000. Mean Reward: -0.5204062812385322. Std of Reward: 0.7951304822095755.\n",
      "Step: 1150000. Mean Reward: -0.5224447239505852. Std of Reward: 0.7892921348546108.\n",
      "Step: 1155000. Mean Reward: -0.48694038751523183. Std of Reward: 0.7932203147318933.\n",
      "Step: 1160000. Mean Reward: -0.5292297185621623. Std of Reward: 1.3588741454890674.\n",
      "Saved Model\n",
      "Step: 1165000. Mean Reward: -0.4987989460678851. Std of Reward: 0.7875759368433793.\n",
      "Step: 1170000. Mean Reward: -0.457973323316. Std of Reward: 0.7885440846827475.\n",
      "Step: 1175000. Mean Reward: -0.46692409141145147. Std of Reward: 0.7892020995080915.\n",
      "Step: 1180000. Mean Reward: -0.47014434742388445. Std of Reward: 0.786005795826801.\n",
      "Saved Model\n",
      "Step: 1185000. Mean Reward: -0.535696539808276. Std of Reward: 2.1472727321627816.\n",
      "Step: 1190000. Mean Reward: -0.49648365066143796. Std of Reward: 0.8184882058452645.\n",
      "Step: 1195000. Mean Reward: -0.48006392977877244. Std of Reward: 0.7984775370091676.\n",
      "Step: 1200000. Mean Reward: -0.5212405932456141. Std of Reward: 0.7986864859754437.\n",
      "Saved Model\n",
      "Step: 1205000. Mean Reward: -0.4374552344181586. Std of Reward: 0.8083789508985606.\n",
      "Step: 1210000. Mean Reward: -0.4788074456074535. Std of Reward: 0.8240036061989824.\n",
      "Step: 1215000. Mean Reward: -0.4304619149563046. Std of Reward: 0.8261472463635335.\n",
      "Step: 1220000. Mean Reward: -0.40991535008827085. Std of Reward: 0.8484348640792518.\n",
      "Saved Model\n",
      "Step: 1225000. Mean Reward: -0.41909755416707317. Std of Reward: 0.8447007983282582.\n",
      "Step: 1230000. Mean Reward: -0.45572281343010757. Std of Reward: 0.9091130907267376.\n",
      "Step: 1235000. Mean Reward: -0.3818181761912633. Std of Reward: 0.8740683729938882.\n",
      "Step: 1240000. Mean Reward: -0.37828069659298247. Std of Reward: 0.8821036646104375.\n",
      "Saved Model\n",
      "Step: 1245000. Mean Reward: -0.33582443142704627. Std of Reward: 0.9295831961355471.\n",
      "Step: 1250000. Mean Reward: -0.4273714899976635. Std of Reward: 0.904337672401757.\n",
      "Step: 1255000. Mean Reward: -0.38715122574208677. Std of Reward: 0.8807954833543215.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1260000. Mean Reward: -0.3604561352561403. Std of Reward: 0.8806006240674049.\n",
      "Saved Model\n",
      "Step: 1265000. Mean Reward: -0.35657009931865585. Std of Reward: 0.8846065126152116.\n",
      "Step: 1270000. Mean Reward: -0.3697811012235023. Std of Reward: 0.8915890020116694.\n",
      "Step: 1275000. Mean Reward: -0.2775057692748268. Std of Reward: 0.9030234157029506.\n",
      "Step: 1280000. Mean Reward: -0.34366856680685715. Std of Reward: 0.8894463952392639.\n",
      "Saved Model\n",
      "Step: 1285000. Mean Reward: -0.30399082119724774. Std of Reward: 0.9025452771071737.\n",
      "Step: 1290000. Mean Reward: -0.30641876008466823. Std of Reward: 0.9004313071369279.\n",
      "Step: 1295000. Mean Reward: -0.33103138641367713. Std of Reward: 0.9125929247460871.\n",
      "Step: 1300000. Mean Reward: -0.33520361576131225. Std of Reward: 0.898026914446143.\n",
      "Saved Model\n",
      "Step: 1305000. Mean Reward: -0.29843181409886366. Std of Reward: 0.9173223382898072.\n",
      "Step: 1310000. Mean Reward: -0.37220244335928815. Std of Reward: 0.9006296423078471.\n",
      "Step: 1315000. Mean Reward: -0.28617187143749995. Std of Reward: 0.9162463292616805.\n",
      "Step: 1320000. Mean Reward: -0.3393426697144396. Std of Reward: 0.9274902999033494.\n",
      "Saved Model\n",
      "Step: 1325000. Mean Reward: -0.26130107306236555. Std of Reward: 0.9413228668398741.\n",
      "Step: 1330000. Mean Reward: -0.23456932616911766. Std of Reward: 0.9552955154640218.\n",
      "Step: 1335000. Mean Reward: -0.28014373622176586. Std of Reward: 0.9628368531716704.\n",
      "Step: 1340000. Mean Reward: -0.2852435219409326. Std of Reward: 0.9554181837810383.\n",
      "Saved Model\n",
      "Step: 1345000. Mean Reward: -0.21704356738070538. Std of Reward: 0.9636115262381356.\n",
      "Step: 1350000. Mean Reward: -0.2651276803953013. Std of Reward: 0.9691491693006782.\n",
      "Step: 1355000. Mean Reward: -0.1823626934932642. Std of Reward: 0.9685695952975051.\n",
      "Step: 1360000. Mean Reward: -0.2367036286683468. Std of Reward: 0.9771421470041968.\n",
      "Saved Model\n",
      "Step: 1365000. Mean Reward: -0.2567537684241206. Std of Reward: 0.9747062514051575.\n",
      "Step: 1370000. Mean Reward: -0.24003033327805864. Std of Reward: 0.975468205487882.\n",
      "Step: 1375000. Mean Reward: -0.20853360445417515. Std of Reward: 0.9762419573452122.\n",
      "Step: 1380000. Mean Reward: -0.21570701882604268. Std of Reward: 0.9738827932392963.\n",
      "Saved Model\n",
      "Step: 1385000. Mean Reward: -0.2434756093313008. Std of Reward: 0.9738308154915383.\n",
      "Step: 1390000. Mean Reward: -0.29465931819138275. Std of Reward: 0.971175059744701.\n",
      "Step: 1395000. Mean Reward: -0.21720930201820024. Std of Reward: 0.977891759458741.\n",
      "Step: 1400000. Mean Reward: -0.2705882350099701. Std of Reward: 0.9756109384272581.\n",
      "Saved Model\n",
      "Step: 1405000. Mean Reward: -0.20436619702213277. Std of Reward: 0.9812026306983685.\n",
      "Step: 1410000. Mean Reward: -0.19607467186781027. Std of Reward: 0.9798356658423975.\n",
      "Step: 1415000. Mean Reward: -0.2172745489519038. Std of Reward: 0.9820968461844903.\n",
      "Step: 1420000. Mean Reward: -0.19219413519413547. Std of Reward: 0.9761870164175591.\n",
      "Saved Model\n",
      "Step: 1425000. Mean Reward: -0.19606606595195192. Std of Reward: 0.9796204857423454.\n",
      "Step: 1430000. Mean Reward: -0.24512048174799195. Std of Reward: 0.9826515670177356.\n",
      "Step: 1435000. Mean Reward: -0.19459049529625885. Std of Reward: 0.981005117605645.\n",
      "Step: 1440000. Mean Reward: -0.20356068191273818. Std of Reward: 0.9797746346742892.\n",
      "Saved Model\n",
      "Step: 1445000. Mean Reward: -0.24091633451593625. Std of Reward: 0.9812559828324114.\n",
      "Step: 1450000. Mean Reward: -0.18496468202421795. Std of Reward: 0.9804949294429302.\n",
      "Step: 1455000. Mean Reward: -0.21225125606331657. Std of Reward: 0.9779921783614671.\n",
      "Step: 1460000. Mean Reward: -0.17419191899999997. Std of Reward: 0.9794935321652937.\n",
      "Saved Model\n",
      "Step: 1465000. Mean Reward: -0.1926807227991968. Std of Reward: 0.9828754341199288.\n",
      "Step: 1470000. Mean Reward: -0.21719840465304086. Std of Reward: 0.9795384326652515.\n",
      "Step: 1475000. Mean Reward: -0.19619095459396982. Std of Reward: 0.9795745481532365.\n",
      "Step: 1480000. Mean Reward: -0.17478348427593152. Std of Reward: 0.9803267117747062.\n",
      "Saved Model\n",
      "Step: 1485000. Mean Reward: -0.211969999829. Std of Reward: 0.9794028891762437.\n",
      "Step: 1490000. Mean Reward: -0.23694915237188433. Std of Reward: 0.9798439580255313.\n",
      "Step: 1495000. Mean Reward: -0.20955134592422728. Std of Reward: 0.981048688177038.\n",
      "Step: 1500000. Mean Reward: -0.260965517129064. Std of Reward: 0.978461672867579.\n",
      "Saved Model\n",
      "Step: 1505000. Mean Reward: -0.2247064675323383. Std of Reward: 0.9791890433490308.\n",
      "Step: 1510000. Mean Reward: -0.22162512450448651. Std of Reward: 0.9807717936617103.\n",
      "Step: 1515000. Mean Reward: -0.24641641627527525. Std of Reward: 0.9796936105086589.\n",
      "Step: 1520000. Mean Reward: -0.22101898085714283. Std of Reward: 0.981140522021091.\n",
      "Saved Model\n",
      "Step: 1525000. Mean Reward: -0.21351944149950147. Std of Reward: 0.978666293163794.\n",
      "Step: 1530000. Mean Reward: -0.1967992046799205. Std of Reward: 0.9792796138665438.\n",
      "Step: 1535000. Mean Reward: -0.22224975213875126. Std of Reward: 0.9812244293189107.\n",
      "Step: 1540000. Mean Reward: -0.21242303862959283. Std of Reward: 0.982856570695577.\n",
      "Saved Model\n",
      "Step: 1545000. Mean Reward: -0.22665338630577686. Std of Reward: 0.9796513097851367.\n",
      "Step: 1550000. Mean Reward: -0.210239999878. Std of Reward: 0.9808682595208689.\n",
      "Step: 1555000. Mean Reward: -0.18947052923776223. Std of Reward: 0.9779466447150597.\n",
      "Step: 1560000. Mean Reward: -0.20074626854328356. Std of Reward: 0.97845819443583.\n",
      "Saved Model\n",
      "Step: 1565000. Mean Reward: -0.2116039602831683. Std of Reward: 0.9780494843351847.\n",
      "Step: 1570000. Mean Reward: -0.2557086997302052. Std of Reward: 0.9775206839442665.\n",
      "Step: 1575000. Mean Reward: -0.25372914609421. Std of Reward: 0.9785738102121762.\n",
      "Step: 1580000. Mean Reward: -0.23924361485658155. Std of Reward: 0.9782985699872802.\n",
      "Saved Model\n",
      "Step: 1585000. Mean Reward: -0.3600737571306632. Std of Reward: 3.726636038447105.\n",
      "Step: 1590000. Mean Reward: -0.24990205667678744. Std of Reward: 0.9767643271645364.\n",
      "Step: 1595000. Mean Reward: -0.222027695297725. Std of Reward: 0.9803040280305327.\n",
      "Step: 1600000. Mean Reward: -0.22649999992549016. Std of Reward: 0.9792305928788332.\n",
      "Saved Model\n",
      "Step: 1605000. Mean Reward: -0.27845261111895553. Std of Reward: 0.9769161225763174.\n",
      "Step: 1610000. Mean Reward: -0.16603754927371542. Std of Reward: 0.9764771978800714.\n",
      "Step: 1615000. Mean Reward: -0.2081872508934263. Std of Reward: 0.9766924736843489.\n",
      "Step: 1620000. Mean Reward: -0.3570090616485395. Std of Reward: 3.6011696461509026.\n",
      "Saved Model\n",
      "Step: 1625000. Mean Reward: -0.16373015859623016. Std of Reward: 0.975895993795615.\n",
      "Step: 1630000. Mean Reward: -0.1759122631216351. Std of Reward: 0.9768074436312177.\n",
      "Step: 1635000. Mean Reward: -0.27098153531389696. Std of Reward: 0.9773320562653514.\n",
      "Step: 1640000. Mean Reward: -0.130190954681407. Std of Reward: 0.9753046496209528.\n",
      "Saved Model\n",
      "Step: 1645000. Mean Reward: -0.22704322183889977. Std of Reward: 0.9759728410753538.\n",
      "Step: 1650000. Mean Reward: -0.21929064026600983. Std of Reward: 0.9780958123353559.\n",
      "Step: 1655000. Mean Reward: -0.2342857141213307. Std of Reward: 0.9775792963076613.\n",
      "Step: 1660000. Mean Reward: -0.23386904709821427. Std of Reward: 1.1489348488128972.\n",
      "Saved Model\n",
      "Step: 1665000. Mean Reward: -0.1798212510963257. Std of Reward: 0.9767378614272272.\n",
      "Step: 1670000. Mean Reward: -0.24029614987265546. Std of Reward: 0.9776024378613574.\n",
      "Step: 1675000. Mean Reward: -0.21097222202876983. Std of Reward: 0.9792469811884881.\n",
      "Step: 1680000. Mean Reward: -0.24486803498631476. Std of Reward: 0.9748481885242032.\n",
      "Saved Model\n",
      "Step: 1685000. Mean Reward: -0.21683743827389163. Std of Reward: 0.9772333261588485.\n",
      "Step: 1690000. Mean Reward: -0.16746506967065866. Std of Reward: 0.9754549383872091.\n",
      "Step: 1695000. Mean Reward: -0.24582352931862744. Std of Reward: 0.9771047710982387.\n",
      "Step: 1700000. Mean Reward: -0.2716260952151898. Std of Reward: 0.9766359129911658.\n",
      "Saved Model\n",
      "Step: 1705000. Mean Reward: -0.23919921856445311. Std of Reward: 0.974465871126868.\n",
      "Step: 1710000. Mean Reward: -0.2041641937091988. Std of Reward: 0.977882494484455.\n",
      "Step: 1715000. Mean Reward: -0.24303536331925343. Std of Reward: 0.9777918674404604.\n",
      "Step: 1720000. Mean Reward: -0.24839215665294118. Std of Reward: 0.9782366906064571.\n",
      "Saved Model\n",
      "Step: 1725000. Mean Reward: -0.15107321941123367. Std of Reward: 0.9740836044835401.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1730000. Mean Reward: -0.20466336625346535. Std of Reward: 0.9807112672540009.\n",
      "Step: 1735000. Mean Reward: -0.16429850728457707. Std of Reward: 0.9748099090820516.\n",
      "Step: 1740000. Mean Reward: -0.2929303673027079. Std of Reward: 0.9753634551869282.\n",
      "Saved Model\n",
      "Step: 1745000. Mean Reward: -0.2692443569440628. Std of Reward: 0.9749463730400079.\n",
      "Step: 1750000. Mean Reward: -0.18065868234231536. Std of Reward: 0.9751835722556305.\n",
      "Step: 1755000. Mean Reward: -0.22449950432804755. Std of Reward: 0.9772505333730486.\n",
      "Step: 1760000. Mean Reward: -0.24494613110577865. Std of Reward: 0.9779888627438065.\n",
      "Saved Model\n",
      "Step: 1765000. Mean Reward: -0.17419322694322706. Std of Reward: 0.9783286456370973.\n",
      "Step: 1770000. Mean Reward: -0.20583003931916996. Std of Reward: 0.9734003818383059.\n",
      "Step: 1775000. Mean Reward: -0.1779170778923988. Std of Reward: 0.9771374197949383.\n",
      "Step: 1780000. Mean Reward: -0.17790049731542287. Std of Reward: 0.9754039195201357.\n",
      "Saved Model\n",
      "Step: 1785000. Mean Reward: -0.2182485321810176. Std of Reward: 0.9777261876418344.\n",
      "Step: 1790000. Mean Reward: -0.2539961012641325. Std of Reward: 0.9776829513268073.\n",
      "Step: 1795000. Mean Reward: -0.278856588992248. Std of Reward: 0.9788145668907204.\n",
      "Step: 1800000. Mean Reward: -0.2600970872786407. Std of Reward: 0.9751291626677608.\n",
      "Saved Model\n",
      "Step: 1805000. Mean Reward: -0.2431650484330097. Std of Reward: 0.9745173971629604.\n",
      "Step: 1810000. Mean Reward: -0.21271749738123163. Std of Reward: 0.975060489162259.\n",
      "Step: 1815000. Mean Reward: -0.2296019416737864. Std of Reward: 0.9769666433567242.\n",
      "Step: 1820000. Mean Reward: -0.18459433030009773. Std of Reward: 0.9741800606492979.\n",
      "Saved Model\n",
      "Step: 1825000. Mean Reward: -0.2340308581533269. Std of Reward: 0.9751316461052391.\n",
      "Step: 1830000. Mean Reward: -0.20583736680058082. Std of Reward: 0.9757400452073721.\n",
      "Step: 1835000. Mean Reward: -0.20053346245780793. Std of Reward: 0.9730024959972283.\n",
      "Step: 1840000. Mean Reward: -0.18864917383673468. Std of Reward: 0.9727690943252623.\n",
      "Saved Model\n",
      "Step: 1845000. Mean Reward: -0.1732847422759961. Std of Reward: 0.9712278192880924.\n",
      "Step: 1850000. Mean Reward: -0.1853599220311284. Std of Reward: 0.9721035832003083.\n",
      "Step: 1855000. Mean Reward: -0.2158349513378641. Std of Reward: 0.9749820172989258.\n",
      "Step: 1860000. Mean Reward: -0.1976663451533269. Std of Reward: 0.9741635150987736.\n",
      "Saved Model\n",
      "Step: 1865000. Mean Reward: -0.2634093066524216. Std of Reward: 0.9712170436138966.\n",
      "Step: 1870000. Mean Reward: -0.24385199226565465. Std of Reward: 0.969969952546325.\n",
      "Step: 1875000. Mean Reward: -0.19457170341482194. Std of Reward: 0.9716186075708789.\n",
      "Step: 1880000. Mean Reward: -0.1872463766830918. Std of Reward: 0.9717229343789661.\n",
      "Saved Model\n",
      "Step: 1885000. Mean Reward: -0.254858756972693. Std of Reward: 0.9718740167734025.\n",
      "Step: 1890000. Mean Reward: -0.19604007621183206. Std of Reward: 0.9705855567322522.\n",
      "Step: 1895000. Mean Reward: -0.2342050798297272. Std of Reward: 0.9689640594500603.\n",
      "Step: 1900000. Mean Reward: -0.2112011438379409. Std of Reward: 0.9692584050639994.\n",
      "Saved Model\n",
      "Step: 1905000. Mean Reward: -0.2304995286163996. Std of Reward: 0.9711645774710176.\n",
      "Step: 1910000. Mean Reward: -0.1793834294903661. Std of Reward: 0.9715677721454504.\n",
      "Step: 1915000. Mean Reward: -0.19842654021232226. Std of Reward: 0.9702516350028086.\n",
      "Step: 1920000. Mean Reward: -0.2280754715754717. Std of Reward: 0.9692137808128949.\n",
      "Saved Model\n",
      "Step: 1925000. Mean Reward: -0.18318138639886036. Std of Reward: 0.9712762882918232.\n",
      "Step: 1930000. Mean Reward: -0.23752336438224295. Std of Reward: 0.9695274583707946.\n",
      "Step: 1935000. Mean Reward: -0.20604562731749046. Std of Reward: 0.9712219136335285.\n",
      "Step: 1940000. Mean Reward: -0.17391387547081338. Std of Reward: 0.969190799658797.\n",
      "Saved Model\n",
      "Step: 1945000. Mean Reward: -0.19862200945263156. Std of Reward: 0.9735577805114676.\n",
      "Step: 1950000. Mean Reward: -0.24676056325539905. Std of Reward: 0.9704961821472484.\n",
      "Step: 1955000. Mean Reward: -0.20128571420571426. Std of Reward: 0.9707807188885655.\n",
      "Step: 1960000. Mean Reward: -0.24272384538831288. Std of Reward: 0.973903515040636.\n",
      "Saved Model\n",
      "Step: 1965000. Mean Reward: -0.21153558039981274. Std of Reward: 0.968339093997212.\n",
      "Step: 1970000. Mean Reward: -0.1664918968512869. Std of Reward: 0.9671047522513132.\n",
      "Step: 1975000. Mean Reward: -0.20927631563157895. Std of Reward: 0.9667588586052396.\n",
      "Step: 1980000. Mean Reward: -0.21229186144901777. Std of Reward: 0.9679431893502753.\n",
      "Saved Model\n",
      "Step: 1985000. Mean Reward: -0.31494311674560377. Std of Reward: 4.664562821156293.\n",
      "Step: 1990000. Mean Reward: -0.2502029519095941. Std of Reward: 0.9690153424192227.\n",
      "Step: 1995000. Mean Reward: -0.24655172398881633. Std of Reward: 0.9667468539235482.\n",
      "Step: 2000000. Mean Reward: -0.16746226410754714. Std of Reward: 0.9673355125752293.\n",
      "Saved Model\n",
      "Step: 2005000. Mean Reward: -0.15587121200568177. Std of Reward: 0.9671423848742965.\n",
      "Step: 2010000. Mean Reward: -0.25149953994848206. Std of Reward: 0.965794026210115.\n",
      "Step: 2015000. Mean Reward: -0.22402764961198154. Std of Reward: 0.9674822183244385.\n",
      "Step: 2020000. Mean Reward: -0.1829664178423507. Std of Reward: 0.9675022498102761.\n",
      "Saved Model\n",
      "Step: 2025000. Mean Reward: -0.20605920435245142. Std of Reward: 0.9675187431519506.\n",
      "Step: 2030000. Mean Reward: -0.22782488467281103. Std of Reward: 0.9652127421713135.\n",
      "Step: 2035000. Mean Reward: -0.19202425358208952. Std of Reward: 0.9666203758844576.\n",
      "Step: 2040000. Mean Reward: -0.16425873454013218. Std of Reward: 0.9668081169817013.\n",
      "Saved Model\n",
      "Step: 2045000. Mean Reward: -0.21869926186992616. Std of Reward: 0.9667809341675848.\n",
      "Step: 2050000. Mean Reward: -0.1394454885817669. Std of Reward: 0.9618580140821614.\n",
      "Step: 2055000. Mean Reward: -0.20406133812360597. Std of Reward: 0.963928104418986.\n",
      "Step: 2060000. Mean Reward: -0.1481852551011342. Std of Reward: 0.9656163079814875.\n",
      "Saved Model\n",
      "Step: 2065000. Mean Reward: -0.20534626030009231. Std of Reward: 0.9649459700393155.\n",
      "Step: 2070000. Mean Reward: -0.21410795967429092. Std of Reward: 0.9647211863543795.\n",
      "Step: 2075000. Mean Reward: -0.22554347815760867. Std of Reward: 0.963786254265776.\n",
      "Step: 2080000. Mean Reward: -0.17350877182456137. Std of Reward: 0.963621049992577.\n",
      "Saved Model\n",
      "Step: 2085000. Mean Reward: -0.20704483060750226. Std of Reward: 0.9645683440976438.\n",
      "Step: 2090000. Mean Reward: -0.2067550272934186. Std of Reward: 0.9643256353510967.\n",
      "Step: 2095000. Mean Reward: -0.20152372251186126. Std of Reward: 0.9647316083383393.\n",
      "Step: 2100000. Mean Reward: -0.21733878286830152. Std of Reward: 0.9663915026800882.\n",
      "Saved Model\n",
      "Step: 2105000. Mean Reward: -0.16881591105550414. Std of Reward: 0.9628880800372641.\n",
      "Step: 2110000. Mean Reward: -0.19580527739672426. Std of Reward: 0.9627907015220744.\n",
      "Step: 2115000. Mean Reward: -0.1354368029033457. Std of Reward: 0.9620906867807858.\n",
      "Step: 2120000. Mean Reward: -0.15723502294193545. Std of Reward: 0.9635115810960927.\n",
      "Saved Model\n",
      "Step: 2125000. Mean Reward: -0.15706261504143643. Std of Reward: 0.9619423498987267.\n",
      "Step: 2130000. Mean Reward: -0.2490518782620751. Std of Reward: 0.9619969331317081.\n",
      "Step: 2135000. Mean Reward: -0.20656960860691534. Std of Reward: 0.964163404978413.\n",
      "Step: 2140000. Mean Reward: -0.22313459790243897. Std of Reward: 0.9634509746547931.\n",
      "Saved Model\n",
      "Step: 2145000. Mean Reward: -0.22074410153992738. Std of Reward: 0.9626600274517702.\n",
      "Step: 2150000. Mean Reward: -0.2042557076511415. Std of Reward: 0.9627718585618098.\n",
      "Step: 2155000. Mean Reward: -0.19648401814337896. Std of Reward: 0.9629015149740634.\n",
      "Step: 2160000. Mean Reward: -0.20164246818784026. Std of Reward: 0.9642160743961085.\n",
      "Saved Model\n",
      "Step: 2165000. Mean Reward: -0.21112318832971008. Std of Reward: 0.9642997982205499.\n",
      "Step: 2170000. Mean Reward: -0.3094210525315789. Std of Reward: 0.957052317622208.\n",
      "Step: 2175000. Mean Reward: -0.26361777763555555. Std of Reward: 0.9601937655653385.\n",
      "Step: 2180000. Mean Reward: -0.19920402547849952. Std of Reward: 0.9626029288515404.\n",
      "Saved Model\n",
      "Step: 2185000. Mean Reward: -0.1862351867967183. Std of Reward: 0.9639820516980698.\n",
      "Step: 2190000. Mean Reward: -0.21490925575317602. Std of Reward: 0.9634376504392783.\n",
      "Step: 2195000. Mean Reward: -0.18883636355999994. Std of Reward: 0.9646912415293294.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2200000. Mean Reward: -0.18202739711689492. Std of Reward: 0.9630353930704968.\n",
      "Saved Model\n",
      "Step: 2205000. Mean Reward: -0.16235023031244236. Std of Reward: 0.9621296975144601.\n",
      "Step: 2210000. Mean Reward: -0.20369801073236887. Std of Reward: 0.9624755111876353.\n",
      "Step: 2215000. Mean Reward: -0.18373175174817513. Std of Reward: 0.9644941363584155.\n",
      "Step: 2220000. Mean Reward: -0.2369836955833333. Std of Reward: 0.9634359066447797.\n",
      "Saved Model\n",
      "Step: 2225000. Mean Reward: -0.20449591273751133. Std of Reward: 0.9643753633109925.\n",
      "Step: 2230000. Mean Reward: -0.1587926266506912. Std of Reward: 0.964647218733548.\n",
      "Step: 2235000. Mean Reward: -0.20587114329945552. Std of Reward: 0.9645121924168638.\n",
      "Step: 2240000. Mean Reward: -0.16492165889861746. Std of Reward: 0.9640109749778479.\n",
      "Saved Model\n",
      "Step: 2245000. Mean Reward: -0.23778178526510366. Std of Reward: 0.96231027587833.\n",
      "Step: 2250000. Mean Reward: -0.18458295128139318. Std of Reward: 0.9641346977232034.\n",
      "Step: 2255000. Mean Reward: -0.17021198149677416. Std of Reward: 0.9658322508202586.\n",
      "Step: 2260000. Mean Reward: -0.15599629277571822. Std of Reward: 0.9645330034983963.\n",
      "Saved Model\n",
      "Step: 2265000. Mean Reward: -0.233321233991833. Std of Reward: 0.9636530825850926.\n",
      "Step: 2270000. Mean Reward: -0.15181146017097963. Std of Reward: 0.9637988270302934.\n",
      "Step: 2275000. Mean Reward: -0.22275270751263535. Std of Reward: 0.9634072471095259.\n",
      "Step: 2280000. Mean Reward: -0.23855595657581222. Std of Reward: 0.9630898912965384.\n",
      "Saved Model\n",
      "Step: 2285000. Mean Reward: -0.21922030812964638. Std of Reward: 0.9632847107429277.\n",
      "Step: 2290000. Mean Reward: -0.2004470802226277. Std of Reward: 0.9645477936327241.\n",
      "Step: 2295000. Mean Reward: -0.1602044608884758. Std of Reward: 0.9655017548243144.\n",
      "Step: 2300000. Mean Reward: -0.21246575332054787. Std of Reward: 0.9655533547862548.\n",
      "Saved Model\n",
      "Step: 2305000. Mean Reward: -0.23485972842081446. Std of Reward: 0.9638162286098487.\n",
      "Step: 2310000. Mean Reward: -0.22579999994818178. Std of Reward: 0.9659999981567575.\n",
      "Step: 2315000. Mean Reward: -0.23584615376018095. Std of Reward: 0.9670271812198806.\n",
      "Step: 2320000. Mean Reward: -0.19512797064533816. Std of Reward: 0.9635440429479016.\n",
      "Saved Model\n",
      "Step: 2325000. Mean Reward: -0.23261732841696747. Std of Reward: 0.9635985736363537.\n",
      "Step: 2330000. Mean Reward: -0.20963570118852456. Std of Reward: 0.9647640284624341.\n",
      "Step: 2335000. Mean Reward: -0.21864253383348412. Std of Reward: 0.9637228737265033.\n",
      "Step: 2340000. Mean Reward: -0.17553817842042313. Std of Reward: 0.9663699563766937.\n",
      "Saved Model\n",
      "Step: 2345000. Mean Reward: -0.21595475102081443. Std of Reward: 0.9630963536008661.\n",
      "Step: 2350000. Mean Reward: -0.2392682925971093. Std of Reward: 0.9638710661744211.\n",
      "Step: 2355000. Mean Reward: -0.19419708020529194. Std of Reward: 0.9647131364982326.\n",
      "Step: 2360000. Mean Reward: -0.22582197993188005. Std of Reward: 0.963830997565125.\n",
      "Saved Model\n",
      "Step: 2365000. Mean Reward: -0.2077737225237226. Std of Reward: 0.9649432531462685.\n",
      "Step: 2370000. Mean Reward: -0.22510869556612315. Std of Reward: 0.964734225572622.\n",
      "Step: 2375000. Mean Reward: -0.2363758991951439. Std of Reward: 0.9646858506686137.\n",
      "Step: 2380000. Mean Reward: -0.23364695336379926. Std of Reward: 0.964584825790293.\n",
      "Saved Model\n",
      "Step: 2385000. Mean Reward: -0.19950045402906444. Std of Reward: 0.9643803455438428.\n",
      "Step: 2390000. Mean Reward: -0.21495920207524927. Std of Reward: 0.9650450941710388.\n",
      "Step: 2395000. Mean Reward: -0.18069597056684977. Std of Reward: 0.9639435900048009.\n",
      "Step: 2400000. Mean Reward: -0.15165898605253456. Std of Reward: 0.9623409505302626.\n",
      "Saved Model\n",
      "Step: 2405000. Mean Reward: -0.21563848914028774. Std of Reward: 0.9627794122946233.\n",
      "Step: 2410000. Mean Reward: -0.23294434461131056. Std of Reward: 0.9638674820796079.\n",
      "Step: 2415000. Mean Reward: -0.1856660582655109. Std of Reward: 0.9634310426967951.\n",
      "Step: 2420000. Mean Reward: -0.17792538660418558. Std of Reward: 0.9627720394860309.\n",
      "Saved Model\n",
      "Step: 2425000. Mean Reward: -0.1529714810533578. Std of Reward: 0.9612181315647992.\n",
      "Step: 2430000. Mean Reward: -0.20130434773278982. Std of Reward: 0.9633689949962433.\n",
      "Step: 2435000. Mean Reward: -0.20598737594589717. Std of Reward: 0.9636164991201283.\n",
      "Step: 2440000. Mean Reward: -0.18790145976003644. Std of Reward: 0.9647685009707077.\n",
      "Saved Model\n",
      "Step: 2445000. Mean Reward: -0.19363306072570385. Std of Reward: 0.962611432582304.\n",
      "Step: 2450000. Mean Reward: -0.2131826400443038. Std of Reward: 0.9633015659999676.\n",
      "Step: 2455000. Mean Reward: -0.211816546669964. Std of Reward: 0.9621232004173038.\n",
      "Step: 2460000. Mean Reward: -0.21335135125855856. Std of Reward: 0.9633785954127975.\n",
      "Saved Model\n",
      "Step: 2465000. Mean Reward: -0.2147234813037171. Std of Reward: 0.9637166884849278.\n",
      "Step: 2470000. Mean Reward: -0.20177536223278983. Std of Reward: 0.9643815697002557.\n",
      "Step: 2475000. Mean Reward: -0.19111111097267752. Std of Reward: 0.9636844164479986.\n",
      "Step: 2480000. Mean Reward: -0.1538602940128676. Std of Reward: 0.9619441609857522.\n",
      "Saved Model\n",
      "Step: 2485000. Mean Reward: -0.18514571940346078. Std of Reward: 0.9631844321244708.\n",
      "Step: 2490000. Mean Reward: -0.19386156640801455. Std of Reward: 0.9623564293656375.\n",
      "Step: 2495000. Mean Reward: -0.19986437600994572. Std of Reward: 0.9623799236259853.\n",
      "Step: 2500000. Mean Reward: -0.18622950809289615. Std of Reward: 0.9621251856111266.\n",
      "Saved Model\n",
      "Step: 2505000. Mean Reward: -0.1896815285586897. Std of Reward: 0.9627659523774463.\n",
      "Step: 2510000. Mean Reward: -0.20697297286306304. Std of Reward: 0.9623744234599938.\n",
      "Step: 2515000. Mean Reward: -0.2220071683396057. Std of Reward: 0.9618883124192399.\n",
      "Step: 2520000. Mean Reward: -0.16488138678467149. Std of Reward: 0.9635217037499565.\n",
      "Saved Model\n",
      "Step: 2525000. Mean Reward: -0.15044158223183068. Std of Reward: 0.9621178858646297.\n",
      "Step: 2530000. Mean Reward: -0.2326094726175156. Std of Reward: 0.9614241282333156.\n",
      "Step: 2535000. Mean Reward: -0.19179836503633058. Std of Reward: 0.96430669958814.\n",
      "Step: 2540000. Mean Reward: -0.1397884083937442. Std of Reward: 0.9608960913984198.\n",
      "Saved Model\n",
      "Step: 2545000. Mean Reward: -0.2076465283011722. Std of Reward: 0.9634333131895847.\n",
      "Step: 2550000. Mean Reward: -0.2111081080054054. Std of Reward: 0.9618107627873881.\n",
      "Step: 2555000. Mean Reward: -0.1779033727484047. Std of Reward: 0.9618658646378111.\n",
      "Step: 2560000. Mean Reward: -0.18280399265426495. Std of Reward: 0.9623381782520027.\n",
      "Saved Model\n",
      "Step: 2565000. Mean Reward: -0.2217009847296329. Std of Reward: 0.9639392322752605.\n",
      "Step: 2570000. Mean Reward: -0.19194016308794196. Std of Reward: 0.9624597568248154.\n",
      "Step: 2575000. Mean Reward: -0.19821719446696826. Std of Reward: 0.9623320976111921.\n",
      "Step: 2580000. Mean Reward: -0.2580070857989371. Std of Reward: 0.9603632764719349.\n",
      "Saved Model\n",
      "Step: 2585000. Mean Reward: -0.23210338670766487. Std of Reward: 0.9626554721640492.\n",
      "Step: 2590000. Mean Reward: -0.20262443428597282. Std of Reward: 0.9624028137103139.\n",
      "Step: 2595000. Mean Reward: -0.21069369360090087. Std of Reward: 0.9635985323292289.\n",
      "Step: 2600000. Mean Reward: -0.2612113173271441. Std of Reward: 0.9591291735472962.\n",
      "Saved Model\n",
      "Step: 2605000. Mean Reward: -0.21172227219026146. Std of Reward: 0.9614515113538102.\n",
      "Step: 2610000. Mean Reward: -0.16024657527305933. Std of Reward: 0.9629096097044628.\n",
      "Step: 2615000. Mean Reward: -0.21626798547482012. Std of Reward: 0.9620616409780596.\n",
      "Step: 2620000. Mean Reward: -0.2019026148999098. Std of Reward: 0.9629254932187783.\n",
      "Saved Model\n",
      "Step: 2625000. Mean Reward: -0.2262522360679785. Std of Reward: 0.9627260773355689.\n",
      "Step: 2630000. Mean Reward: -0.21286486479639635. Std of Reward: 0.9628149840690186.\n",
      "Step: 2635000. Mean Reward: -0.25161347510992904. Std of Reward: 0.9612470031445887.\n",
      "Step: 2640000. Mean Reward: -0.1396675898873499. Std of Reward: 0.960306814874897.\n",
      "Saved Model\n",
      "Step: 2645000. Mean Reward: -0.20679891783408472. Std of Reward: 0.9623255460284593.\n",
      "Step: 2650000. Mean Reward: -0.20217902337070523. Std of Reward: 0.9618153837988654.\n",
      "Step: 2655000. Mean Reward: -0.1909057969637681. Std of Reward: 0.9615682457883532.\n",
      "Step: 2660000. Mean Reward: -0.25993810781874443. Std of Reward: 0.9613654891210827.\n",
      "Saved Model\n",
      "Step: 2665000. Mean Reward: -0.23899821102325575. Std of Reward: 0.9621913710090731.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-583a438fd989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, memory, value)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"STEP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"brain_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"RECEIVED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mmessage_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons15/model-2660000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons15/model-2660000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
