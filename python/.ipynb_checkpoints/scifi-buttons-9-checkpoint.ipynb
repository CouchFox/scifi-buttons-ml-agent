{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e6 # Set maximum number of steps to run environment.\n",
    "run_path = \"scifibuttons10\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 20000 # Frequency at which to save model.\n",
    "env_name = \"scifibuttons10\" # Name of the training environment file.\n",
    "curriculum_file = 'curricula/lessons.json'\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tlessonNr -> 1.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 30\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 5\n",
      "        Memory space size (per agent): 3\n",
      "        Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: -0.011548837209302333. Std of Reward: 1.0518325099607235.\n",
      "Step: 10000. Mean Reward: -0.20024147727272726. Std of Reward: 1.0511179113513986.\n",
      "Step: 15000. Mean Reward: 0.09874752475247521. Std of Reward: 1.0499794543575989.\n",
      "Step: 20000. Mean Reward: 0.13332978723404254. Std of Reward: 1.0336569026056035.\n",
      "Saved Model\n",
      "Step: 25000. Mean Reward: 0.12806399999999996. Std of Reward: 1.0263145745355076.\n",
      "Step: 30000. Mean Reward: 0.13332539682539682. Std of Reward: 1.0446438109403386.\n",
      "Step: 35000. Mean Reward: 0.16192168674698795. Std of Reward: 1.04118980388902.\n",
      "Step: 40000. Mean Reward: 0.2869686192468619. Std of Reward: 0.9941466116307603.\n",
      "Saved Model\n",
      "Step: 45000. Mean Reward: 0.2032623574144487. Std of Reward: 1.0213689992796433.\n",
      "Step: 50000. Mean Reward: 0.2384692028985507. Std of Reward: 1.0150930491871475.\n",
      "Step: 55000. Mean Reward: 0.30869310344827594. Std of Reward: 0.986033985890825.\n",
      "Step: 60000. Mean Reward: 0.3874418918918919. Std of Reward: 0.9445866491961783.\n",
      "Saved Model\n",
      "Step: 65000. Mean Reward: 0.33823162939297124. Std of Reward: 0.9739197742623681.\n",
      "Step: 70000. Mean Reward: 0.26768705035971224. Std of Reward: 1.0100059561997274.\n",
      "Step: 75000. Mean Reward: 0.4234634551495017. Std of Reward: 0.9385710831186242.\n",
      "Step: 80000. Mean Reward: 0.4501047197640118. Std of Reward: 0.9219543577931774.\n",
      "Saved Model\n",
      "Step: 85000. Mean Reward: 0.45426025236593054. Std of Reward: 0.9272597281260265.\n",
      "Step: 90000. Mean Reward: 0.3740358490566038. Std of Reward: 0.9723642067670105.\n",
      "Step: 95000. Mean Reward: 0.49464221556886234. Std of Reward: 0.9000175804915667.\n",
      "Step: 100000. Mean Reward: 0.48896476510067116. Std of Reward: 0.908387468661688.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: 0.5646976744186046. Std of Reward: 0.8505498598686021.\n",
      "Step: 110000. Mean Reward: 0.5838118686868687. Std of Reward: 0.8350108361878027.\n",
      "Step: 115000. Mean Reward: 0.5386306179775281. Std of Reward: 0.8757476527732292.\n",
      "Step: 120000. Mean Reward: 0.6114337500000001. Std of Reward: 0.8151265231459087.\n",
      "Saved Model\n",
      "Step: 125000. Mean Reward: 0.5572402402402402. Std of Reward: 0.8616117200331254.\n",
      "Step: 130000. Mean Reward: 0.6081249999999999. Std of Reward: 0.81931625886457.\n",
      "Step: 135000. Mean Reward: 0.6282234332425068. Std of Reward: 0.8085884506364497.\n",
      "Step: 140000. Mean Reward: 0.656122093023256. Std of Reward: 0.7704114278960487.\n",
      "Saved Model\n",
      "Step: 145000. Mean Reward: 0.7191995192307693. Std of Reward: 0.7107839860713929.\n",
      "Step: 150000. Mean Reward: 0.7534588235294116. Std of Reward: 0.6679837354707957.\n",
      "Step: 155000. Mean Reward: 0.8303495726495727. Std of Reward: 0.5601128141286356.\n",
      "Step: 160000. Mean Reward: 0.8153204081632652. Std of Reward: 0.5984015462913804.\n",
      "Saved Model\n",
      "Step: 165000. Mean Reward: 0.8521747311827957. Std of Reward: 0.5303143024127916.\n",
      "Step: 170000. Mean Reward: 0.8650300171526588. Std of Reward: 0.5057122539026921.\n",
      "Step: 175000. Mean Reward: 0.8585385826771654. Std of Reward: 0.5246641727626047.\n",
      "Step: 180000. Mean Reward: 0.8995390070921987. Std of Reward: 0.4468848847874544.\n",
      "Saved Model\n",
      "Step: 185000. Mean Reward: 0.9027387862796833. Std of Reward: 0.43194887984771196.\n",
      "Step: 190000. Mean Reward: 0.8916940607734808. Std of Reward: 0.4588832852193201.\n",
      "Step: 195000. Mean Reward: 0.9213362183754995. Std of Reward: 0.39340546597490866.\n",
      "Step: 200000. Mean Reward: 0.9049166666666667. Std of Reward: 0.4281596664660611.\n",
      "Saved Model\n",
      "Step: 205000. Mean Reward: 0.930333734939759. Std of Reward: 0.3632238291560358.\n",
      "Step: 210000. Mean Reward: 0.9447604055496265. Std of Reward: 0.3269434266465227.\n",
      "Step: 215000. Mean Reward: 0.9290618996798292. Std of Reward: 0.37403359275027176.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \tlessonNr -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 220000. Mean Reward: 0.9523692748091603. Std of Reward: 0.30260458114385036.\n",
      "Saved Model\n",
      "Step: 225000. Mean Reward: 0.44438178294573644. Std of Reward: 0.8727339641981616.\n",
      "Step: 230000. Mean Reward: 0.4531243654822335. Std of Reward: 0.855758517704964.\n",
      "Step: 235000. Mean Reward: 0.4767234513274336. Std of Reward: 0.8407668340968808.\n",
      "Step: 240000. Mean Reward: 0.4771428571428571. Std of Reward: 0.8377179836126398.\n",
      "Saved Model\n",
      "Step: 245000. Mean Reward: 0.47585191637630664. Std of Reward: 0.8530002613371671.\n",
      "Step: 250000. Mean Reward: 0.3892585616438356. Std of Reward: 0.9019382719071233.\n",
      "Step: 255000. Mean Reward: 0.43883935018050546. Std of Reward: 0.8755007140858999.\n",
      "Step: 260000. Mean Reward: 0.5431556603773585. Std of Reward: 0.8133268188691798.\n",
      "Saved Model\n",
      "Step: 265000. Mean Reward: 0.4530385802469135. Std of Reward: 0.870582926549343.\n",
      "Step: 270000. Mean Reward: 0.542686968838527. Std of Reward: 0.8178747231553036.\n",
      "Step: 275000. Mean Reward: 0.5097868098159509. Std of Reward: 0.8395177441484662.\n",
      "Step: 280000. Mean Reward: 0.5020496688741722. Std of Reward: 0.8453430017332376.\n",
      "Saved Model\n",
      "Step: 285000. Mean Reward: 0.5281744868035191. Std of Reward: 0.8313442059475524.\n",
      "Step: 290000. Mean Reward: 0.4799899135446686. Std of Reward: 0.8617703867423078.\n",
      "Step: 295000. Mean Reward: 0.5443536931818183. Std of Reward: 0.8202992338151512.\n",
      "Step: 300000. Mean Reward: 0.5647491961414791. Std of Reward: 0.812489968154342.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 0.5657665662650602. Std of Reward: 0.8059974544394717.\n",
      "Step: 310000. Mean Reward: 0.6215943708609272. Std of Reward: 0.7582128616540241.\n",
      "Step: 315000. Mean Reward: 0.602673780487805. Std of Reward: 0.7742257375379584.\n",
      "Step: 320000. Mean Reward: 0.6703870967741936. Std of Reward: 0.714706210591555.\n",
      "Saved Model\n",
      "Step: 325000. Mean Reward: 0.567738726790451. Std of Reward: 0.8084272169004669.\n",
      "Step: 330000. Mean Reward: 0.6718312182741117. Std of Reward: 0.7206006184670286.\n",
      "Step: 335000. Mean Reward: 0.5074488950276244. Std of Reward: 0.8645151338103786.\n",
      "Step: 340000. Mean Reward: 0.628353494623656. Std of Reward: 0.7738079103790595.\n",
      "Saved Model\n",
      "Step: 345000. Mean Reward: 0.6424680365296804. Std of Reward: 0.755514295415057.\n",
      "Step: 350000. Mean Reward: 0.6933188235294118. Std of Reward: 0.709002148921346.\n",
      "Step: 355000. Mean Reward: 0.6715071090047393. Std of Reward: 0.7303365751368593.\n",
      "Step: 360000. Mean Reward: 0.6440820045558087. Std of Reward: 0.7574985998736634.\n",
      "Saved Model\n",
      "Step: 365000. Mean Reward: 0.7264357298474945. Std of Reward: 0.6712341068967699.\n",
      "Step: 370000. Mean Reward: 0.6542139534883721. Std of Reward: 0.7466319651831196.\n",
      "Step: 375000. Mean Reward: 0.692121076233184. Std of Reward: 0.7165789121642381.\n",
      "Step: 380000. Mean Reward: 0.7184251626898049. Std of Reward: 0.6838066773205597.\n",
      "Saved Model\n",
      "Step: 385000. Mean Reward: 0.716578313253012. Std of Reward: 0.6890444644487875.\n",
      "Step: 390000. Mean Reward: 0.7188266932270916. Std of Reward: 0.682945733820227.\n",
      "Step: 395000. Mean Reward: 0.6984407894736843. Std of Reward: 0.7061343981972709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \tlessonNr -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400000. Mean Reward: 0.7272355679702048. Std of Reward: 0.6802598206259214.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: 0.06128363228699556. Std of Reward: 0.9940684491262368.\n",
      "Step: 410000. Mean Reward: 0.14770072115384617. Std of Reward: 0.9834637955893735.\n",
      "Step: 415000. Mean Reward: 0.21694419134396356. Std of Reward: 0.9682044490115966.\n",
      "Step: 420000. Mean Reward: 0.19928917050691244. Std of Reward: 0.9717136692333229.\n",
      "Saved Model\n",
      "Step: 425000. Mean Reward: 0.17870340909090912. Std of Reward: 0.9785985863896559.\n",
      "Step: 430000. Mean Reward: 0.3005425790754258. Std of Reward: 0.9461287038996017.\n",
      "Step: 435000. Mean Reward: 0.24179646017699122. Std of Reward: 0.9708599398115222.\n",
      "Step: 440000. Mean Reward: 0.2558529411764706. Std of Reward: 0.958440708191592.\n",
      "Saved Model\n",
      "Step: 445000. Mean Reward: 0.28705380577427825. Std of Reward: 0.9514692363575713.\n",
      "Step: 450000. Mean Reward: 0.38468690095846647. Std of Reward: 0.9115837994253413.\n",
      "Step: 455000. Mean Reward: 0.21633250620347375. Std of Reward: 1.056672027080455.\n",
      "Step: 460000. Mean Reward: 0.32883078880407124. Std of Reward: 0.9313011302252993.\n",
      "Saved Model\n",
      "Step: 465000. Mean Reward: 0.34709930715935333. Std of Reward: 0.9314362909190786.\n",
      "Step: 470000. Mean Reward: 0.3431880733944954. Std of Reward: 0.9258088616685383.\n",
      "Step: 475000. Mean Reward: 0.36183618581907095. Std of Reward: 0.9204113616377769.\n",
      "Step: 480000. Mean Reward: 0.3984304245283019. Std of Reward: 0.9127033305349723.\n",
      "Saved Model\n",
      "Step: 485000. Mean Reward: 0.35535193133047216. Std of Reward: 0.9237164802042838.\n",
      "Step: 490000. Mean Reward: 0.5158403263403264. Std of Reward: 0.8454767035264508.\n",
      "Step: 495000. Mean Reward: 0.4736641975308642. Std of Reward: 0.8792592980245517.\n",
      "Step: 500000. Mean Reward: 0.386159229208925. Std of Reward: 0.9106650003715071.\n",
      "Saved Model\n",
      "Step: 505000. Mean Reward: 0.4759940191387561. Std of Reward: 0.8836745172721632.\n",
      "Step: 510000. Mean Reward: 0.45201717557251914. Std of Reward: 0.8833918285769925.\n",
      "Step: 515000. Mean Reward: 0.4845897435897436. Std of Reward: 0.8647865063194285.\n",
      "Step: 520000. Mean Reward: 0.4366577946768061. Std of Reward: 0.8892744301806295.\n",
      "Saved Model\n",
      "Step: 525000. Mean Reward: 0.5304410646387834. Std of Reward: 0.8385459754780901.\n",
      "Step: 530000. Mean Reward: 0.42683535528596184. Std of Reward: 0.8987667892857231.\n",
      "Step: 535000. Mean Reward: 0.44653703703703695. Std of Reward: 0.8910546114003038.\n",
      "Step: 540000. Mean Reward: 0.48427645985401463. Std of Reward: 0.8727251281996622.\n",
      "Saved Model\n",
      "Step: 545000. Mean Reward: 0.5008241758241759. Std of Reward: 0.8599243190365502.\n",
      "Step: 550000. Mean Reward: 0.4717212837837838. Std of Reward: 0.8786365214605513.\n",
      "Step: 555000. Mean Reward: 0.5187621951219512. Std of Reward: 0.8473859006474485.\n",
      "Step: 560000. Mean Reward: 0.5066347687400319. Std of Reward: 0.8564922830244585.\n",
      "Saved Model\n",
      "Step: 565000. Mean Reward: 0.5676635802469135. Std of Reward: 0.8174394951186139.\n",
      "Step: 570000. Mean Reward: 0.5932245508982037. Std of Reward: 0.7991133380016278.\n",
      "Step: 575000. Mean Reward: 0.5907358208955223. Std of Reward: 0.8004919748824417.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \tlessonNr -> 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 580000. Mean Reward: 0.6565741017964072. Std of Reward: 0.7473951487374193.\n",
      "Saved Model\n",
      "Step: 585000. Mean Reward: -0.8572037037037057. Std of Reward: 2.2123172063399483.\n",
      "Step: 590000. Mean Reward: -1.0032068965517291. Std of Reward: 2.3224455619515294.\n",
      "Step: 595000. Mean Reward: -0.6023648648648667. Std of Reward: 2.070858250557165.\n",
      "Step: 600000. Mean Reward: -0.8585142857142866. Std of Reward: 1.9676619101494732.\n",
      "Saved Model\n",
      "Step: 605000. Mean Reward: -1.261980000000003. Std of Reward: 2.2917884849174097.\n",
      "Step: 610000. Mean Reward: -0.7116818181818194. Std of Reward: 1.830947680437812.\n",
      "Step: 615000. Mean Reward: -0.6680845070422536. Std of Reward: 2.156017847246931.\n",
      "Step: 620000. Mean Reward: -0.8644850746268672. Std of Reward: 2.1675581194675706.\n",
      "Saved Model\n",
      "Step: 625000. Mean Reward: -0.6910153846153827. Std of Reward: 2.6435770012053226.\n",
      "Step: 630000. Mean Reward: -0.5657368421052654. Std of Reward: 2.5683535891341234.\n",
      "Step: 635000. Mean Reward: -0.40632500000000055. Std of Reward: 1.9830451474626107.\n",
      "Step: 640000. Mean Reward: -0.4278915662650603. Std of Reward: 2.0041206662445825.\n",
      "Saved Model\n",
      "Step: 645000. Mean Reward: -0.21983333333333407. Std of Reward: 1.9045509429900942.\n",
      "Step: 650000. Mean Reward: -0.45628235294117697. Std of Reward: 1.8590744288601067.\n",
      "Step: 655000. Mean Reward: -0.33895454545454623. Std of Reward: 2.0043576480426917.\n",
      "Step: 660000. Mean Reward: -29.598166666661736. Std of Reward: 43.217552499591555.\n",
      "Saved Model\n",
      "Step: 665000. Mean Reward: 0.12544278606965176. Std of Reward: 1.2825157764627382.\n",
      "Step: 670000. Mean Reward: 0.02222105263157909. Std of Reward: 1.3030141670591253.\n",
      "Step: 675000. Mean Reward: 0.39799795081967226. Std of Reward: 1.1087421229856318.\n",
      "Step: 680000. Mean Reward: 0.39386504424778757. Std of Reward: 1.1594171123522647.\n",
      "Saved Model\n",
      "Step: 685000. Mean Reward: 0.580953125. Std of Reward: 1.0705924605342292.\n",
      "Step: 690000. Mean Reward: 0.6896783783783784. Std of Reward: 0.8121271422678721.\n",
      "Step: 695000. Mean Reward: 0.7783314285714286. Std of Reward: 0.6848119607481118.\n",
      "Step: 700000. Mean Reward: 0.7233304878048789. Std of Reward: 1.028340216039483.\n",
      "Saved Model\n",
      "Step: 705000. Mean Reward: 0.7758084112149534. Std of Reward: 0.718368519714058.\n",
      "Step: 710000. Mean Reward: 0.8349449999999999. Std of Reward: 0.6037415079941083.\n",
      "Step: 715000. Mean Reward: 0.8566269982238011. Std of Reward: 0.5403520506736217.\n",
      "Step: 720000. Mean Reward: 0.8794982110912344. Std of Reward: 0.5054112390610745.\n",
      "Saved Model\n",
      "Step: 725000. Mean Reward: 0.8789581239530987. Std of Reward: 0.5052037848661138.\n",
      "Step: 730000. Mean Reward: 0.9147067545304779. Std of Reward: 0.44015112127426087.\n",
      "Step: 735000. Mean Reward: 0.8981238447319783. Std of Reward: 0.7135974651955052.\n",
      "Step: 740000. Mean Reward: 0.9092308845577209. Std of Reward: 0.4243307146412727.\n",
      "Saved Model\n",
      "Step: 745000. Mean Reward: 0.9170476529160739. Std of Reward: 0.40913652118077654.\n",
      "Step: 750000. Mean Reward: 0.9306834030683403. Std of Reward: 0.37140415950369726.\n",
      "Step: 755000. Mean Reward: 0.9326502808988765. Std of Reward: 0.36729572768132607.\n",
      "Step: 760000. Mean Reward: 0.9469257555847569. Std of Reward: 0.31445085427956654.\n",
      "Saved Model\n",
      "Step: 765000. Mean Reward: 0.9514089759797725. Std of Reward: 0.30168941285435885.\n",
      "Step: 770000. Mean Reward: 0.9420931758530183. Std of Reward: 0.3354504295154389.\n",
      "Step: 775000. Mean Reward: 0.9598495203836931. Std of Reward: 0.27016193932344096.\n",
      "Step: 780000. Mean Reward: 0.9433719059405941. Std of Reward: 0.32897442080636335.\n",
      "Saved Model\n",
      "Step: 785000. Mean Reward: 0.9396293103448277. Std of Reward: 0.34491718489583867.\n",
      "Step: 790000. Mean Reward: 0.9342578692493947. Std of Reward: 0.3579510557630082.\n",
      "Step: 795000. Mean Reward: 0.9314470802919708. Std of Reward: 0.36535802100593484.\n",
      "Step: 800000. Mean Reward: 0.9549323493234934. Std of Reward: 0.30134594194579595.\n",
      "Saved Model\n",
      "Step: 805000. Mean Reward: 0.9520873108265426. Std of Reward: 0.30253728342129915.\n",
      "Step: 810000. Mean Reward: 0.9333798133022171. Std of Reward: 0.35736932825910206.\n",
      "Step: 815000. Mean Reward: 0.9552654249126893. Std of Reward: 0.29009837956977697.\n",
      "Step: 820000. Mean Reward: 0.9181826137689614. Std of Reward: 0.3933340274401556.\n",
      "Saved Model\n",
      "Step: 825000. Mean Reward: 0.9251458823529412. Std of Reward: 0.3813015950207892.\n",
      "Step: 830000. Mean Reward: 0.9197783687943263. Std of Reward: 0.3939378644020301.\n",
      "Step: 835000. Mean Reward: 0.916128896882494. Std of Reward: 0.40299685643843763.\n",
      "Step: 840000. Mean Reward: 0.8815054545454545. Std of Reward: 0.4752151368947377.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-583a438fd989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_variance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mrun_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_variance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons10/model-840000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons10/model-840000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
