{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 1e7 # Set maximum number of steps to run environment.\n",
    "run_path = \"scifibuttons7\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 20000 # Frequency at which to save model.\n",
    "env_name = \"scifibuttons7\" # Name of the training environment file.\n",
    "curriculum_file = 'curricula/lessons.json'\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tlessonNr -> 1.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 30\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 5\n",
      "        Memory space size (per agent): 3\n",
      "        Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: 0.3764537037037037. Std of Reward: 0.9859712910351609.\n",
      "Step: 10000. Mean Reward: 0.3954035874439461. Std of Reward: 0.9742229459050621.\n",
      "Step: 15000. Mean Reward: 0.4923584905660377. Std of Reward: 0.9357575045563731.\n",
      "Step: 20000. Mean Reward: 0.6121428571428572. Std of Reward: 0.8401505328307393.\n",
      "Saved Model\n",
      "Step: 25000. Mean Reward: 0.6871897810218978. Std of Reward: 0.7674162455344369.\n",
      "Step: 30000. Mean Reward: 0.6904349315068492. Std of Reward: 0.7832125793143444.\n",
      "Step: 35000. Mean Reward: 0.7850066006600659. Std of Reward: 0.6570297098003953.\n",
      "Step: 40000. Mean Reward: 0.8060663265306123. Std of Reward: 0.6364408894821834.\n",
      "Saved Model\n",
      "Step: 45000. Mean Reward: 0.8587592954990214. Std of Reward: 0.5580458625622013.\n",
      "Step: 50000. Mean Reward: 0.8167729357798165. Std of Reward: 0.6212960346421208.\n",
      "Step: 55000. Mean Reward: 0.8846372180451127. Std of Reward: 0.5101811800080106.\n",
      "Step: 60000. Mean Reward: 0.9234664031620554. Std of Reward: 0.41568018277535.\n",
      "Saved Model\n",
      "Step: 65000. Mean Reward: 0.9015670816044261. Std of Reward: 0.47581864738130075.\n",
      "Step: 70000. Mean Reward: 0.9625468468468468. Std of Reward: 0.2863549159617537.\n",
      "Step: 75000. Mean Reward: 0.9654372177055104. Std of Reward: 0.2892083334735089.\n",
      "Step: 80000. Mean Reward: 0.9746151288445553. Std of Reward: 0.2598417716703452.\n",
      "Saved Model\n",
      "Step: 85000. Mean Reward: 0.9731804961505561. Std of Reward: 0.25216776992234086.\n",
      "Step: 90000. Mean Reward: 0.983968. Std of Reward: 0.19139960025036615.\n",
      "Step: 95000. Mean Reward: 0.9884467939972715. Std of Reward: 0.18167678214854444.\n",
      "Step: 100000. Mean Reward: 0.9819612005856516. Std of Reward: 0.2274637244317868.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: 0.9876306666666668. Std of Reward: 0.1859652358001951.\n",
      "Step: 110000. Mean Reward: 0.9944351585014409. Std of Reward: 0.11737298476866447.\n",
      "Step: 115000. Mean Reward: 0.9867649226234341. Std of Reward: 0.2336418285766932.\n",
      "Step: 120000. Mean Reward: 0.9822333019755409. Std of Reward: 0.2583408312307722.\n",
      "Saved Model\n",
      "Step: 125000. Mean Reward: 0.9934698375870069. Std of Reward: 0.12553128262921187.\n",
      "Step: 130000. Mean Reward: 0.9831063651591287. Std of Reward: 0.3489005001977116.\n",
      "Step: 135000. Mean Reward: 0.9952379162191192. Std of Reward: 0.10433396659760821.\n",
      "Step: 140000. Mean Reward: 0.9949332229580575. Std of Reward: 0.14158692779917254.\n",
      "Saved Model\n",
      "Step: 145000. Mean Reward: 0.9991008858267718. Std of Reward: 0.0198322234212275.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \tlessonNr -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 150000. Mean Reward: 0.9996013257575759. Std of Reward: 0.0015399226859587759.\n",
      "Step: 155000. Mean Reward: 0.48697701149425815. Std of Reward: 1.9227971063310456.\n",
      "Step: 160000. Mean Reward: 0.597651851851852. Std of Reward: 1.0673253890919456.\n",
      "Saved Model\n",
      "Step: 165000. Mean Reward: 0.6878020304568523. Std of Reward: 1.154393248850153.\n",
      "Step: 170000. Mean Reward: 0.08516666666668553. Std of Reward: 4.0896479796213745.\n",
      "Step: 175000. Mean Reward: 0.7407318435754195. Std of Reward: 1.0868221553528736.\n",
      "Step: 180000. Mean Reward: 0.6177027027027029. Std of Reward: 1.1889892716887287.\n",
      "Saved Model\n",
      "Step: 185000. Mean Reward: 0.7158786407766989. Std of Reward: 0.9381687956628273.\n",
      "Step: 190000. Mean Reward: 0.793551111111111. Std of Reward: 0.7299429617358166.\n",
      "Step: 195000. Mean Reward: 0.4851927710843438. Std of Reward: 2.3248140200668317.\n",
      "Step: 200000. Mean Reward: 0.7388397435897436. Std of Reward: 0.9587948264303892.\n",
      "Saved Model\n",
      "Step: 205000. Mean Reward: 0.7559845360824742. Std of Reward: 0.6848614388929459.\n",
      "Step: 210000. Mean Reward: 0.6772366412213746. Std of Reward: 1.46142356004805.\n",
      "Step: 215000. Mean Reward: 0.8699783281733747. Std of Reward: 0.484441297834278.\n",
      "Step: 220000. Mean Reward: 0.900328990228013. Std of Reward: 0.4276440716600848.\n",
      "Saved Model\n",
      "Step: 225000. Mean Reward: 0.8731744966442954. Std of Reward: 0.48764046651654896.\n",
      "Step: 230000. Mean Reward: 0.5123571428571432. Std of Reward: 1.8529842316990082.\n",
      "Step: 235000. Mean Reward: 0.7023629629629682. Std of Reward: 2.200548145879045.\n",
      "Step: 240000. Mean Reward: -0.1360769230769028. Std of Reward: 5.53686139170964.\n",
      "Saved Model\n",
      "Step: 245000. Mean Reward: 0.4503880597015023. Std of Reward: 3.70325540916752.\n",
      "Step: 250000. Mean Reward: 0.9084226804123712. Std of Reward: 0.3965281567218493.\n",
      "Step: 255000. Mean Reward: 0.8923488372093026. Std of Reward: 0.42223086976483437.\n",
      "Step: 260000. Mean Reward: 0.9159411764705885. Std of Reward: 0.48265533562820834.\n",
      "Saved Model\n",
      "Step: 265000. Mean Reward: 0.5847443609022516. Std of Reward: 3.8661903061697807.\n",
      "Step: 270000. Mean Reward: 0.9467789855072465. Std of Reward: 0.32549155423333337.\n",
      "Step: 275000. Mean Reward: 0.8850661764705898. Std of Reward: 1.001554844187149.\n",
      "Step: 280000. Mean Reward: 0.9347381818181818. Std of Reward: 0.5426044236293591.\n",
      "Saved Model\n",
      "Step: 285000. Mean Reward: 0.9323878048780487. Std of Reward: 0.39563232296316486.\n",
      "Step: 290000. Mean Reward: 0.8844000000000001. Std of Reward: 0.4607580977209024.\n",
      "Step: 295000. Mean Reward: 0.5887398373983608. Std of Reward: 4.398248470370159.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \tlessonNr -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300000. Mean Reward: 0.9226887417218547. Std of Reward: 0.48616162773695415.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 0.9215052631578947. Std of Reward: 0.3611713946149233.\n",
      "Step: 310000. Mean Reward: 0.9093262711864406. Std of Reward: 0.40271645867717626.\n",
      "Step: 315000. Mean Reward: 0.9076913827655311. Std of Reward: 0.4108510510125635.\n",
      "Step: 320000. Mean Reward: 0.9167417840375588. Std of Reward: 0.4030974776862354.\n",
      "Saved Model\n",
      "Step: 325000. Mean Reward: 0.9601418312387792. Std of Reward: 0.25156688791244675.\n",
      "Step: 330000. Mean Reward: 0.974672131147541. Std of Reward: 0.1754633884852809.\n",
      "Step: 335000. Mean Reward: 0.9760265625000001. Std of Reward: 0.15645984183468165.\n",
      "Step: 340000. Mean Reward: 0.9810947109471094. Std of Reward: 0.14774249442314416.\n",
      "Saved Model\n",
      "Step: 345000. Mean Reward: 0.9867040704070406. Std of Reward: 0.118552042551416.\n",
      "Step: 350000. Mean Reward: 0.9766201743462017. Std of Reward: 0.2028875829100881.\n",
      "Step: 355000. Mean Reward: 0.9822031047865458. Std of Reward: 0.1955652209460028.\n",
      "Step: 360000. Mean Reward: 0.9918439181916038. Std of Reward: 0.06980886074004668.\n",
      "Saved Model\n",
      "Step: 365000. Mean Reward: 0.9697298136645964. Std of Reward: 0.2910682482442726.\n",
      "Step: 370000. Mean Reward: 0.9869804161566706. Std of Reward: 0.21527059538412577.\n",
      "Step: 375000. Mean Reward: 0.9935698005698006. Std of Reward: 0.06631930960049384.\n",
      "Step: 380000. Mean Reward: 0.9931685823754789. Std of Reward: 0.0897495361704891.\n",
      "Saved Model\n",
      "Step: 385000. Mean Reward: 0.9921011131725419. Std of Reward: 0.1030852088180161.\n",
      "Step: 390000. Mean Reward: 0.9854234326824254. Std of Reward: 0.22970341381961504.\n",
      "Step: 395000. Mean Reward: 0.9929806167400882. Std of Reward: 0.0995822975509338.\n",
      "Step: 400000. Mean Reward: 0.9907150684931507. Std of Reward: 0.1337058623502415.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: 0.9787838214783825. Std of Reward: 0.4885997804507486.\n",
      "Step: 410000. Mean Reward: 0.8764655870445375. Std of Reward: 1.6288940206769038.\n",
      "Step: 415000. Mean Reward: 0.9969828178694157. Std of Reward: 0.0032257096980626678.\n",
      "Step: 420000. Mean Reward: 0.9893514526710402. Std of Reward: 0.19610476037910038.\n",
      "Saved Model\n",
      "Step: 425000. Mean Reward: 0.9918936781609193. Std of Reward: 0.16711022929186783.\n",
      "Step: 430000. Mean Reward: 0.9971703020134228. Std of Reward: 0.002946734726162469.\n",
      "Step: 435000. Mean Reward: 0.9953455759599332. Std of Reward: 0.07533473953802851.\n",
      "Step: 440000. Mean Reward: 0.9976169686985172. Std of Reward: 0.0018383813982447482.\n",
      "Saved Model\n",
      "Step: 445000. Mean Reward: 0.9913024344569289. Std of Reward: 0.20742761318873346.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \tlessonNr -> 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 450000. Mean Reward: 0.9917327354260091. Std of Reward: 0.165625560610357.\n",
      "Step: 460000. Mean Reward: -63.280000000005444. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 465000. Mean Reward: -7.187900000000023. Std of Reward: 10.522686106218366.\n",
      "Step: 470000. Mean Reward: -5.710428571428445. Std of Reward: 6.656103511753775.\n",
      "Step: 475000. Mean Reward: -7.549714285714109. Std of Reward: 7.270606728745378.\n",
      "Step: 480000. Mean Reward: -1.6919999999999975. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 485000. Mean Reward: -11.064250000000538. Std of Reward: 21.681683720079512.\n",
      "Step: 490000. Mean Reward: -5.780571428571252. Std of Reward: 10.338760700215365.\n",
      "Step: 495000. Mean Reward: -16.876999999999363. Std of Reward: 12.340500881244134.\n",
      "Step: 500000. Mean Reward: -3.8931666666665876. Std of Reward: 5.462852945627733.\n",
      "Saved Model\n",
      "Step: 505000. Mean Reward: -4.330899999999989. Std of Reward: 3.9289061696609435.\n",
      "Step: 510000. Mean Reward: -5.133555555555414. Std of Reward: 6.85950920030962.\n",
      "Step: 515000. Mean Reward: -11.02119999999972. Std of Reward: 11.861977851943177.\n",
      "Step: 520000. Mean Reward: -2.352875000000003. Std of Reward: 2.4694703094742887.\n",
      "Saved Model\n",
      "Step: 525000. Mean Reward: -9.2551250000001. Std of Reward: 13.103492048281797.\n",
      "Step: 530000. Mean Reward: -6.444499999999867. Std of Reward: 6.0098306340526335.\n",
      "Step: 535000. Mean Reward: -5.3022499999998525. Std of Reward: 7.335380166528251.\n",
      "Step: 540000. Mean Reward: -5.115888888888816. Std of Reward: 6.397539170032167.\n",
      "Saved Model\n",
      "Step: 545000. Mean Reward: -3.060249999999939. Std of Reward: 5.874170482218273.\n",
      "Step: 550000. Mean Reward: -3.4414666666666154. Std of Reward: 5.276778270455717.\n",
      "Step: 555000. Mean Reward: -4.78749999999989. Std of Reward: 8.765359747323261.\n",
      "Step: 560000. Mean Reward: -1.5118965517241418. Std of Reward: 2.3793263657631636.\n",
      "Saved Model\n",
      "Step: 565000. Mean Reward: -1.8344761904761817. Std of Reward: 3.1940541430101828.\n",
      "Step: 570000. Mean Reward: -2.0417647058823576. Std of Reward: 1.7557864847215436.\n",
      "Step: 575000. Mean Reward: -2.261391304347824. Std of Reward: 2.61645845502673.\n",
      "Step: 580000. Mean Reward: -1.0578461538461472. Std of Reward: 2.397274810027584.\n",
      "Saved Model\n",
      "Step: 585000. Mean Reward: -1.9131600000000022. Std of Reward: 2.0866666754419674.\n",
      "Step: 590000. Mean Reward: -1.5565000000000033. Std of Reward: 2.001273884077577.\n",
      "Step: 595000. Mean Reward: -1.424928571428573. Std of Reward: 2.06912018791583.\n",
      "Step: 600000. Mean Reward: -4.001000000000001. Std of Reward: 3.168380687641266.\n",
      "Saved Model\n",
      "Step: 605000. Mean Reward: -1.1077804878048767. Std of Reward: 1.29835313156838.\n",
      "Step: 610000. Mean Reward: -1.5153600000000043. Std of Reward: 2.4284324471559957.\n",
      "Step: 615000. Mean Reward: -1.1819069767441859. Std of Reward: 1.8126851701910076.\n",
      "Step: 620000. Mean Reward: -0.9882380952380965. Std of Reward: 1.9332788556715956.\n",
      "Saved Model\n",
      "Step: 625000. Mean Reward: -0.5558749999999993. Std of Reward: 1.163297551234563.\n",
      "Step: 630000. Mean Reward: -0.46323636363636395. Std of Reward: 1.506390713094008.\n",
      "Step: 635000. Mean Reward: -0.5792419354838705. Std of Reward: 1.1594590741777075.\n",
      "Step: 640000. Mean Reward: -0.4343382352941174. Std of Reward: 1.0946552050530918.\n",
      "Saved Model\n",
      "Step: 645000. Mean Reward: -0.3158461538461538. Std of Reward: 1.1153935729083284.\n",
      "Step: 650000. Mean Reward: -0.27139215686274487. Std of Reward: 1.1543716919067872.\n",
      "Step: 655000. Mean Reward: -0.5427936507936503. Std of Reward: 1.314399629485428.\n",
      "Step: 660000. Mean Reward: -0.20824324324324336. Std of Reward: 1.0378046841551383.\n",
      "Saved Model\n",
      "Step: 665000. Mean Reward: 0.06643307086614168. Std of Reward: 1.0318465795023635.\n",
      "Step: 670000. Mean Reward: -0.11685833333333336. Std of Reward: 1.0541520865592509.\n",
      "Step: 675000. Mean Reward: -0.20009259259259252. Std of Reward: 1.0816598145394762.\n",
      "Step: 680000. Mean Reward: -0.3824634146341464. Std of Reward: 1.0192690015113768.\n",
      "Saved Model\n",
      "Step: 685000. Mean Reward: -0.11695238095238097. Std of Reward: 1.0338452069909405.\n",
      "Step: 690000. Mean Reward: -0.13532978723404251. Std of Reward: 1.1006330614338418.\n",
      "Step: 695000. Mean Reward: -0.13073529411764714. Std of Reward: 1.0731510038152225.\n",
      "Step: 700000. Mean Reward: 0.007352459016393359. Std of Reward: 1.0201346819000248.\n",
      "Saved Model\n",
      "Step: 705000. Mean Reward: -0.031634615384615435. Std of Reward: 1.051790991390878.\n",
      "Step: 710000. Mean Reward: 0.0008249999999999314. Std of Reward: 1.0736921436682865.\n",
      "Step: 715000. Mean Reward: -0.24279545454545437. Std of Reward: 1.1290963719949085.\n",
      "Step: 720000. Mean Reward: -0.13910112359550555. Std of Reward: 1.1747788225486742.\n",
      "Saved Model\n",
      "Step: 725000. Mean Reward: 0.04190654205607498. Std of Reward: 1.13186309509232.\n",
      "Step: 730000. Mean Reward: -0.22679518072289137. Std of Reward: 1.0648841529734032.\n",
      "Step: 735000. Mean Reward: 0.10742647058823586. Std of Reward: 1.0915452527087777.\n",
      "Step: 740000. Mean Reward: -0.1816521739130429. Std of Reward: 1.1712255234766265.\n",
      "Saved Model\n",
      "Step: 745000. Mean Reward: -0.013885245901638447. Std of Reward: 1.1482562417281073.\n",
      "Step: 750000. Mean Reward: -0.7699999999999992. Std of Reward: 1.576735398260925.\n",
      "Step: 755000. Mean Reward: -0.30300000000000005. Std of Reward: 1.0982502967382777.\n",
      "Step: 760000. Mean Reward: -2.327586206896651. Std of Reward: 9.4044158832246.\n",
      "Saved Model\n",
      "Step: 765000. Mean Reward: -0.15649999999999975. Std of Reward: 1.6617217870687762.\n",
      "Step: 770000. Mean Reward: -0.5059999999999937. Std of Reward: 1.3477412956498611.\n",
      "Step: 775000. Mean Reward: -9.950166666667213. Std of Reward: 20.26554493400738.\n",
      "Step: 780000. Mean Reward: -0.45135714285713774. Std of Reward: 1.2710447675122938.\n",
      "Saved Model\n",
      "Step: 785000. Mean Reward: -0.038379310344823364. Std of Reward: 1.2473262276536166.\n",
      "Step: 790000. Mean Reward: 0.05239393939394175. Std of Reward: 1.2014923028894733.\n",
      "Step: 795000. Mean Reward: -0.17592307692307105. Std of Reward: 1.3367962540537117.\n",
      "Step: 800000. Mean Reward: -3.766874999999908. Std of Reward: 8.436187459947247.\n",
      "Saved Model\n",
      "Step: 805000. Mean Reward: -0.20679310344827384. Std of Reward: 1.2290041210702265.\n",
      "Step: 810000. Mean Reward: -0.39749999999999636. Std of Reward: 1.6756022880677086.\n",
      "Step: 815000. Mean Reward: -1.2619999999999678. Std of Reward: 4.29028577136758.\n",
      "Step: 820000. Mean Reward: -1.8194999999999844. Std of Reward: 0.7944999999999844.\n",
      "Saved Model\n",
      "Step: 825000. Mean Reward: -8.629000000000628. Std of Reward: 0.0.\n",
      "Step: 830000. Mean Reward: -3.278999999999905. Std of Reward: 0.0.\n",
      "Step: 835000. Mean Reward: -3.197000000000194. Std of Reward: 4.179000000000194.\n",
      "Step: 840000. Mean Reward: -1.2897499999999462. Std of Reward: 5.4217891285225.\n",
      "Saved Model\n",
      "Step: 845000. Mean Reward: -7.167000000000338. Std of Reward: 0.0.\n",
      "Step: 850000. Mean Reward: 0.19078571428571464. Std of Reward: 0.973441808266746.\n",
      "Step: 855000. Mean Reward: -0.5742499999999988. Std of Reward: 3.4069010387007097.\n",
      "Saved Model\n",
      "Step: 870000. Mean Reward: -1.2608181818181978. Std of Reward: 3.0953777803857925.\n",
      "Saved Model\n",
      "Step: 900000. Mean Reward: -36.41900000000227. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 905000. Mean Reward: -0.6844999999999684. Std of Reward: 0.8839203866864745.\n",
      "Step: 910000. Mean Reward: -0.16960000000000114. Std of Reward: 1.9672062016982388.\n",
      "Step: 915000. Mean Reward: -0.28837499999997196. Std of Reward: 1.8075986790144496.\n",
      "Step: 920000. Mean Reward: -1.8877999999999737. Std of Reward: 0.6420091588131431.\n",
      "Saved Model\n",
      "Step: 925000. Mean Reward: -1.4424999999999226. Std of Reward: 2.0344999999999223.\n",
      "Step: 930000. Mean Reward: 0.7459999999999998. Std of Reward: 0.0.\n",
      "Step: 940000. Mean Reward: -5.012000000000485. Std of Reward: 4.009000000000485.\n",
      "Saved Model\n",
      "Step: 945000. Mean Reward: 0.6274999999999997. Std of Reward: 0.2765000000000002.\n",
      "Step: 950000. Mean Reward: -1.847666666666606. Std of Reward: 2.658434919688988.\n",
      "Step: 955000. Mean Reward: 0.25899999999999945. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1000000. Mean Reward: -25.91200000000864. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1050000. Mean Reward: -13.210500000004023. Std of Reward: 12.174500000004022.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1100000. Mean Reward: -25.253000000007862. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1150000. Mean Reward: -25.13100000000773. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1200000. Mean Reward: -25.08000000000765. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1250000. Mean Reward: -25.07400000000766. Std of Reward: 0.0.\n",
      "Step: 1255000. Mean Reward: -0.15550000000000008. Std of Reward: 1.0205000000000002.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1300000. Mean Reward: -24.832000000007366. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1305000. Mean Reward: -1.2240000000000002. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1350000. Mean Reward: -24.887000000007433. Std of Reward: 0.0.\n",
      "Step: 1355000. Mean Reward: -0.09099999999999997. Std of Reward: 0.988.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1400000. Mean Reward: -25.020000000007585. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1405000. Mean Reward: -1.3705000000000003. Std of Reward: 0.3085000000000002.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1450000. Mean Reward: -24.348000000006774. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1500000. Mean Reward: -25.004000000007576. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Step: 1505000. Mean Reward: 0.7899999999999999. Std of Reward: 0.0.\n",
      "Step: 1510000. Mean Reward: -0.0663333333332703. Std of Reward: 1.1845258779593395.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1550000. Mean Reward: -22.20800000000416. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1600000. Mean Reward: -13.002000000003786. Std of Reward: 11.999000000003786.\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Step: 1650000. Mean Reward: -24.99900000000757. Std of Reward: 0.0.\n",
      "Saved Model\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-583a438fd989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, memory, value)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"STEP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_brains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"brain_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mn_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"RECEIVED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/unityagents/environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mmessage_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons6/model-3000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons6/model-3000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
