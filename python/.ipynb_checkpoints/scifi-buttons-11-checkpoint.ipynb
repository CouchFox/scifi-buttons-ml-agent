{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e6 # Set maximum number of steps to run environment.\n",
    "run_path = \"scifibuttons11\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 20000 # Frequency at which to save model.\n",
    "env_name = \"scifibuttons11\" # Name of the training environment file.\n",
    "curriculum_file = 'curricula/lessons11.json'\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tlessonNr -> 1.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 30\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 5\n",
      "        Memory space size (per agent): 3\n",
      "        Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: -0.25552272076136373. Std of Reward: 1.2007563181310434.\n",
      "Step: 10000. Mean Reward: -0.2662719239692983. Std of Reward: 1.2000845693637356.\n",
      "Step: 15000. Mean Reward: -0.09347736084156384. Std of Reward: 1.1738052924232174.\n",
      "Step: 20000. Mean Reward: -0.17564515585887103. Std of Reward: 1.1818880874019024.\n",
      "Saved Model\n",
      "Step: 25000. Mean Reward: -0.1440186851822431. Std of Reward: 1.2265875474252848.\n",
      "Step: 30000. Mean Reward: 0.12423077423269231. Std of Reward: 1.15823928459418.\n",
      "Step: 35000. Mean Reward: 0.1473180127547892. Std of Reward: 1.1438931126791776.\n",
      "Step: 40000. Mean Reward: 0.1853873286390845. Std of Reward: 1.1458425364265812.\n",
      "Saved Model\n",
      "Step: 45000. Mean Reward: 0.23684859617077467. Std of Reward: 1.1370476980034805.\n",
      "Step: 50000. Mean Reward: 0.2899671091759868. Std of Reward: 1.11276437749431.\n",
      "Step: 55000. Mean Reward: 0.36657670815056814. Std of Reward: 1.0422913687436863.\n",
      "Step: 60000. Mean Reward: 0.45065029293208086. Std of Reward: 1.0093556930530971.\n",
      "Saved Model\n",
      "Step: 65000. Mean Reward: 0.48316794222519077. Std of Reward: 0.9751599036264632.\n",
      "Step: 70000. Mean Reward: 0.39791892206486484. Std of Reward: 1.0409610867275976.\n",
      "Step: 75000. Mean Reward: 0.5169603006811414. Std of Reward: 0.9456664884128001.\n",
      "Step: 80000. Mean Reward: 0.5522707450993449. Std of Reward: 0.9302025710115968.\n",
      "Saved Model\n",
      "Step: 85000. Mean Reward: 0.553829116193038. Std of Reward: 0.9293930602480011.\n",
      "Step: 90000. Mean Reward: 0.5966495926721311. Std of Reward: 0.8786800756323302.\n",
      "Step: 95000. Mean Reward: 0.6917037690188356. Std of Reward: 0.7776996473908373.\n",
      "Step: 100000. Mean Reward: 0.7192823729235568. Std of Reward: 0.7361365259313409.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: 0.7424082584701834. Std of Reward: 0.7145151147544753.\n",
      "Step: 110000. Mean Reward: 0.7825032777529488. Std of Reward: 0.6546526864013344.\n",
      "Step: 115000. Mean Reward: 0.7610604039107384. Std of Reward: 0.6831610895285298.\n",
      "Step: 120000. Mean Reward: 0.773465975507497. Std of Reward: 0.6579392334929794.\n",
      "Saved Model\n",
      "Step: 125000. Mean Reward: 0.8503477830504031. Std of Reward: 0.534677843811743.\n",
      "Step: 130000. Mean Reward: 0.8872802442397738. Std of Reward: 0.46206900724278344.\n",
      "Step: 135000. Mean Reward: 0.8982239063021887. Std of Reward: 0.4432770349698109.\n",
      "Step: 140000. Mean Reward: 0.9108326666385108. Std of Reward: 0.4152067243073264.\n",
      "Saved Model\n",
      "Step: 145000. Mean Reward: 0.9325076574031395. Std of Reward: 0.35093651970831424.\n",
      "Step: 150000. Mean Reward: 0.9363832120499999. Std of Reward: 0.34694874680140736.\n",
      "Step: 155000. Mean Reward: 0.9451354099218009. Std of Reward: 0.3206799461107326.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \tlessonNr -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 160000. Mean Reward: 0.9524708551904145. Std of Reward: 0.2956335936768277.\n",
      "Saved Model\n",
      "Step: 165000. Mean Reward: 0.4221805807342419. Std of Reward: 0.8670093492584117.\n",
      "Step: 170000. Mean Reward: 0.40379614879935793. Std of Reward: 0.8825601143775941.\n",
      "Step: 175000. Mean Reward: 0.4215551852132107. Std of Reward: 0.871964926254879.\n",
      "Step: 180000. Mean Reward: 0.4554991958542673. Std of Reward: 0.8551886680449765.\n",
      "Saved Model\n",
      "Step: 185000. Mean Reward: 0.5024216311708464. Std of Reward: 0.8223836007109665.\n",
      "Step: 190000. Mean Reward: 0.44083836954078554. Std of Reward: 0.8681233100033026.\n",
      "Step: 195000. Mean Reward: 0.44013494401491476. Std of Reward: 0.8728523663020634.\n",
      "Step: 200000. Mean Reward: 0.48846364959122085. Std of Reward: 0.8386277326965265.\n",
      "Saved Model\n",
      "Step: 205000. Mean Reward: 0.5308221932854278. Std of Reward: 0.8153238829687934.\n",
      "Step: 210000. Mean Reward: 0.4667351000688742. Std of Reward: 0.8573438708100418.\n",
      "Step: 215000. Mean Reward: 0.5312719898788905. Std of Reward: 0.8171948241989296.\n",
      "Step: 220000. Mean Reward: 0.4725559953425559. Std of Reward: 0.8530701677023669.\n",
      "Saved Model\n",
      "Step: 225000. Mean Reward: 0.48089628742637647. Std of Reward: 0.8517113696247278.\n",
      "Step: 230000. Mean Reward: 0.5111019114375795. Std of Reward: 0.8312699378624948.\n",
      "Step: 235000. Mean Reward: 0.5328817738165025. Std of Reward: 0.821669989985359.\n",
      "Step: 240000. Mean Reward: 0.5334532377715827. Std of Reward: 0.8226358865741564.\n",
      "Saved Model\n",
      "Step: 245000. Mean Reward: 0.5686144581656626. Std of Reward: 0.7984804142273502.\n",
      "Step: 250000. Mean Reward: 0.5558660511235566. Std of Reward: 0.8107545387064242.\n",
      "Step: 255000. Mean Reward: 0.595279361736602. Std of Reward: 0.781054134212385.\n",
      "Step: 260000. Mean Reward: 0.6175480228446327. Std of Reward: 0.7628939321466938.\n",
      "Saved Model\n",
      "Step: 265000. Mean Reward: 0.6733560092981858. Std of Reward: 0.7135447078653003.\n",
      "Step: 270000. Mean Reward: 0.6771875002556818. Std of Reward: 0.7099324664189622.\n",
      "Step: 275000. Mean Reward: 0.6752855545363942. Std of Reward: 0.7132627758558883.\n",
      "Step: 280000. Mean Reward: 0.6747379680732619. Std of Reward: 0.7157586086584058.\n",
      "Saved Model\n",
      "Step: 285000. Mean Reward: 0.6991976621955366. Std of Reward: 0.6939991143288348.\n",
      "Step: 290000. Mean Reward: 0.7638824154179025. Std of Reward: 0.6196305760486817.\n",
      "Step: 295000. Mean Reward: 0.7735433885242768. Std of Reward: 0.6094386271401261.\n",
      "Step: 300000. Mean Reward: 0.7828950105337839. Std of Reward: 0.5956885220872644.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 0.7666076845980789. Std of Reward: 0.6192395151514358.\n",
      "Step: 310000. Mean Reward: 0.7726292247519881. Std of Reward: 0.6127772798669999.\n",
      "Step: 315000. Mean Reward: 0.8026969093439682. Std of Reward: 0.5722551023432572.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \tlessonNr -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 320000. Mean Reward: 0.8291464647282829. Std of Reward: 0.5302728512111721.\n",
      "Saved Model\n",
      "Step: 325000. Mean Reward: 0.04381871352222221. Std of Reward: 0.9872340160003865.\n",
      "Step: 330000. Mean Reward: 0.059305396161882885. Std of Reward: 0.9877589502584642.\n",
      "Step: 335000. Mean Reward: 0.14905454555090908. Std of Reward: 0.9737686942496118.\n",
      "Step: 340000. Mean Reward: 0.13075059109101655. Std of Reward: 0.9773825520460282.\n",
      "Saved Model\n",
      "Step: 345000. Mean Reward: 0.20071691183639703. Std of Reward: 0.9657164271971983.\n",
      "Step: 350000. Mean Reward: 0.24541561724370278. Std of Reward: 0.9536267527645189.\n",
      "Step: 355000. Mean Reward: 0.2852468355303797. Std of Reward: 0.9448395329266399.\n",
      "Step: 360000. Mean Reward: 0.36479274625582897. Std of Reward: 0.9109133710957098.\n",
      "Saved Model\n",
      "Step: 365000. Mean Reward: 0.35288113707687335. Std of Reward: 0.9153695722018446.\n",
      "Step: 370000. Mean Reward: 0.37220214203748325. Std of Reward: 0.9101928040522765.\n",
      "Step: 375000. Mean Reward: 0.39512467204790025. Std of Reward: 0.8976251206933336.\n",
      "Step: 380000. Mean Reward: 0.5157718122214765. Std of Reward: 0.8283337615527834.\n",
      "Saved Model\n",
      "Step: 385000. Mean Reward: 0.578119266206422. Std of Reward: 0.7865500896604739.\n",
      "Step: 390000. Mean Reward: 0.6063899372389937. Std of Reward: 0.7643400073105082.\n",
      "Step: 395000. Mean Reward: 0.619038224591862. Std of Reward: 0.7563295997806995.\n",
      "Step: 400000. Mean Reward: 0.6447956732716347. Std of Reward: 0.7322694778319837.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: 0.6723036953371825. Std of Reward: 0.7102200397492329.\n",
      "Step: 410000. Mean Reward: 0.7293588237270587. Std of Reward: 0.6476958487581952.\n",
      "Step: 415000. Mean Reward: 0.7389835489970622. Std of Reward: 0.6377433369761231.\n",
      "Step: 420000. Mean Reward: 0.725831422247133. Std of Reward: 0.6525308635289024.\n",
      "Saved Model\n",
      "Step: 425000. Mean Reward: 0.740629370805944. Std of Reward: 0.6381863599775934.\n",
      "Step: 430000. Mean Reward: 0.792245714476. Std of Reward: 0.5708420969701524.\n",
      "Step: 435000. Mean Reward: 0.8321607755895096. Std of Reward: 0.5081886252759341.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \tlessonNr -> 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 440000. Mean Reward: 0.8378500001650001. Std of Reward: 0.4995266256656213.\n",
      "Saved Model\n",
      "Step: 445000. Mean Reward: 0.40566666894358966. Std of Reward: 0.7845989097329832.\n",
      "Step: 450000. Mean Reward: 0.5172198299073274. Std of Reward: 0.6832037577229316.\n",
      "Step: 455000. Mean Reward: 0.5208874484264068. Std of Reward: 0.7046292296651896.\n",
      "Step: 460000. Mean Reward: 0.5485593250084745. Std of Reward: 0.7789908406764169.\n",
      "Saved Model\n",
      "Step: 465000. Mean Reward: 0.6823920284468439. Std of Reward: 0.5092541540615023.\n",
      "Step: 470000. Mean Reward: 0.7101002884154727. Std of Reward: 0.5545676993368216.\n",
      "Step: 475000. Mean Reward: 0.7716027414068493. Std of Reward: 0.3821311451321884.\n",
      "Step: 480000. Mean Reward: 0.805535309307517. Std of Reward: 0.3643904607437168.\n",
      "Saved Model\n",
      "Step: 485000. Mean Reward: 0.7965124170530474. Std of Reward: 0.4567664990081585.\n",
      "Step: 490000. Mean Reward: 0.7884615401486487. Std of Reward: 0.5123910780241286.\n",
      "Step: 495000. Mean Reward: 0.7981947083043478. Std of Reward: 0.4766182284261431.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 4 : \tlessonNr -> 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500000. Mean Reward: 0.8012222233527778. Std of Reward: 0.5134938301778914.\n",
      "Saved Model\n",
      "Step: 505000. Mean Reward: -4.069285665128561. Std of Reward: 5.059283530928398.\n",
      "Step: 510000. Mean Reward: -2.6879032070322535. Std of Reward: 3.8483803074219733.\n",
      "Step: 515000. Mean Reward: -2.2041666301439378. Std of Reward: 2.9918114490353744.\n",
      "Step: 520000. Mean Reward: -2.1021428315178565. Std of Reward: 2.1611232438308714.\n",
      "Saved Model\n",
      "Step: 525000. Mean Reward: -1.526919624107142. Std of Reward: 1.7856217806376369.\n",
      "Step: 530000. Mean Reward: -1.5047391070086948. Std of Reward: 1.6507113976014467.\n",
      "Step: 535000. Mean Reward: -1.1824652599027776. Std of Reward: 1.4380489135260568.\n",
      "Step: 540000. Mean Reward: -1.3784642685821429. Std of Reward: 1.311221593773318.\n",
      "Saved Model\n",
      "Step: 545000. Mean Reward: -0.9809142717600002. Std of Reward: 1.2692989054135608.\n",
      "Step: 550000. Mean Reward: -1.075846141569231. Std of Reward: 1.075592340752895.\n",
      "Step: 555000. Mean Reward: -0.9669414747526597. Std of Reward: 1.192902906309605.\n",
      "Step: 560000. Mean Reward: -0.8616504736844661. Std of Reward: 1.1799508057244477.\n",
      "Saved Model\n",
      "Step: 565000. Mean Reward: -0.9392129498171299. Std of Reward: 1.0628372272114177.\n",
      "Step: 570000. Mean Reward: -0.8545652038768117. Std of Reward: 1.0788628091720815.\n",
      "Step: 575000. Mean Reward: -0.9771363504227274. Std of Reward: 0.9851106112631275.\n",
      "Step: 580000. Mean Reward: -0.8195278855429186. Std of Reward: 1.059557349592252.\n",
      "Saved Model\n",
      "Step: 585000. Mean Reward: -0.8404440056853283. Std of Reward: 0.9702718656980068.\n",
      "Step: 590000. Mean Reward: -0.7662867561966913. Std of Reward: 0.9949982384688616.\n",
      "Step: 595000. Mean Reward: -0.78005493689011. Std of Reward: 0.978931112012225.\n",
      "Step: 600000. Mean Reward: -0.7575563817687971. Std of Reward: 1.0170008020337313.\n",
      "Saved Model\n",
      "Step: 605000. Mean Reward: -0.8028977177272728. Std of Reward: 0.9583006681581872.\n",
      "Step: 610000. Mean Reward: -0.7041556224983444. Std of Reward: 0.9395622258905967.\n",
      "Step: 615000. Mean Reward: -0.7270957027788779. Std of Reward: 0.9306272801739277.\n",
      "Step: 620000. Mean Reward: -0.584446196147152. Std of Reward: 1.0165697238400595.\n",
      "Saved Model\n",
      "Step: 625000. Mean Reward: -0.6847524693069307. Std of Reward: 0.9312274175523645.\n",
      "Step: 630000. Mean Reward: -0.6184883668106312. Std of Reward: 0.9621470061174032.\n",
      "Step: 635000. Mean Reward: -0.6541166610216668. Std of Reward: 0.949669484328293.\n",
      "Step: 640000. Mean Reward: -0.6477413735896552. Std of Reward: 0.9547481950892444.\n",
      "Saved Model\n",
      "Step: 645000. Mean Reward: -0.6263175620135136. Std of Reward: 0.9588956455207652.\n",
      "Step: 650000. Mean Reward: -0.6095759660441696. Std of Reward: 0.986495827261413.\n",
      "Step: 655000. Mean Reward: -0.609295527347079. Std of Reward: 0.9677052942302141.\n",
      "Step: 660000. Mean Reward: -0.5604982760137457. Std of Reward: 0.9987578210730766.\n",
      "Saved Model\n",
      "Step: 665000. Mean Reward: -0.540697668807309. Std of Reward: 0.9931597474301793.\n",
      "Step: 670000. Mean Reward: -0.5259666613266667. Std of Reward: 0.9814339312425873.\n",
      "Step: 675000. Mean Reward: -0.542840131192177. Std of Reward: 0.9815383459416789.\n",
      "Step: 680000. Mean Reward: -0.47505189792906577. Std of Reward: 1.0186375192205104.\n",
      "Saved Model\n",
      "Step: 685000. Mean Reward: -0.4829965699486302. Std of Reward: 1.0243746859712417.\n",
      "Step: 690000. Mean Reward: -0.35042207320779223. Std of Reward: 1.041009427264732.\n",
      "Step: 695000. Mean Reward: -0.5077592542759258. Std of Reward: 0.9985857930643384.\n",
      "Step: 700000. Mean Reward: -0.4835906001157718. Std of Reward: 0.9941184540199962.\n",
      "Saved Model\n",
      "Step: 705000. Mean Reward: -0.3383157843263158. Std of Reward: 1.0361505855819972.\n",
      "Step: 710000. Mean Reward: -0.3817213068721312. Std of Reward: 1.0201008793989468.\n",
      "Step: 715000. Mean Reward: -0.3934628924752651. Std of Reward: 1.0281335023075757.\n",
      "Step: 720000. Mean Reward: -0.3552777739346405. Std of Reward: 1.022015742024716.\n",
      "Saved Model\n",
      "Step: 725000. Mean Reward: -0.2854952042076678. Std of Reward: 1.0389309251830492.\n",
      "Step: 730000. Mean Reward: -0.28006514334039095. Std of Reward: 1.028821703221056.\n",
      "Step: 735000. Mean Reward: -0.4014743549166667. Std of Reward: 1.013044915766111.\n",
      "Step: 740000. Mean Reward: -0.337359371521875. Std of Reward: 1.0122991975046243.\n",
      "Saved Model\n",
      "Step: 745000. Mean Reward: -0.32291530522964174. Std of Reward: 1.0340609909279546.\n",
      "Step: 750000. Mean Reward: -0.37043102976034487. Std of Reward: 1.0435886692468186.\n",
      "Step: 755000. Mean Reward: -0.3615476138486395. Std of Reward: 1.0273268251229706.\n",
      "Step: 760000. Mean Reward: -0.3145954648576052. Std of Reward: 1.026425559431025.\n",
      "Saved Model\n",
      "Step: 765000. Mean Reward: -0.28172467947151897. Std of Reward: 1.0279664299480327.\n",
      "Step: 770000. Mean Reward: -0.3161059151479751. Std of Reward: 1.0302306421253553.\n",
      "Step: 775000. Mean Reward: -0.37616128561451617. Std of Reward: 1.0147981380005895.\n",
      "Step: 780000. Mean Reward: -0.3525552012902209. Std of Reward: 1.0211885041271727.\n",
      "Saved Model\n",
      "Step: 785000. Mean Reward: -0.25048741792610063. Std of Reward: 1.0376239014144595.\n",
      "Step: 790000. Mean Reward: -0.36133956048909655. Std of Reward: 1.0208305110002138.\n",
      "Step: 795000. Mean Reward: -0.24741227740350877. Std of Reward: 1.024103493246002.\n",
      "Step: 800000. Mean Reward: -0.2785267827008929. Std of Reward: 1.0272995327712047.\n",
      "Saved Model\n",
      "Step: 805000. Mean Reward: -0.26613128232262573. Std of Reward: 1.026123945676869.\n",
      "Step: 810000. Mean Reward: -0.3276067040106707. Std of Reward: 1.0131849469291174.\n",
      "Step: 815000. Mean Reward: -0.1449845646651235. Std of Reward: 1.031137188491877.\n",
      "Step: 820000. Mean Reward: -0.14916666377380955. Std of Reward: 1.0268843975305384.\n",
      "Saved Model\n",
      "Step: 825000. Mean Reward: -0.2486039860398861. Std of Reward: 1.0209814774747896.\n",
      "Step: 830000. Mean Reward: -0.14373937426770542. Std of Reward: 1.0119110232469823.\n",
      "Step: 835000. Mean Reward: -0.15281064822928997. Std of Reward: 1.0182271245689787.\n",
      "Step: 840000. Mean Reward: -0.21020527600586514. Std of Reward: 1.01659368135024.\n",
      "Saved Model\n",
      "Step: 845000. Mean Reward: -0.1996676706933535. Std of Reward: 1.0090382530569029.\n",
      "Step: 850000. Mean Reward: -0.21451951656456458. Std of Reward: 1.0047973854024124.\n",
      "Step: 855000. Mean Reward: -0.22081080791891894. Std of Reward: 1.0101840738692514.\n",
      "Step: 860000. Mean Reward: -0.2264538024116848. Std of Reward: 1.0060742107295482.\n",
      "Saved Model\n",
      "Step: 865000. Mean Reward: -0.23313043243478257. Std of Reward: 1.0076101281285224.\n",
      "Step: 870000. Mean Reward: -0.15549723573342544. Std of Reward: 1.012742788806323.\n",
      "Step: 875000. Mean Reward: -0.10227393445478723. Std of Reward: 1.0143077900061181.\n",
      "Step: 880000. Mean Reward: -0.18574175651923078. Std of Reward: 1.0100272672489934.\n",
      "Saved Model\n",
      "Step: 885000. Mean Reward: -0.14028074682219255. Std of Reward: 1.0104255934347135.\n",
      "Step: 890000. Mean Reward: -0.06999999805241937. Std of Reward: 1.0037397806082229.\n",
      "Step: 895000. Mean Reward: -0.16012711666949153. Std of Reward: 1.0090700519183209.\n",
      "Step: 900000. Mean Reward: -0.07417808038767122. Std of Reward: 1.0063179908528874.\n",
      "Saved Model\n",
      "Step: 905000. Mean Reward: -0.07918055332222224. Std of Reward: 1.0042839942874673.\n",
      "Step: 910000. Mean Reward: -0.05503989171808511. Std of Reward: 1.005314434077231.\n",
      "Step: 915000. Mean Reward: -0.13326975270844685. Std of Reward: 1.0036962015964117.\n",
      "Step: 920000. Mean Reward: -0.12458903875753427. Std of Reward: 1.0131125746973928.\n",
      "Saved Model\n",
      "Step: 925000. Mean Reward: -0.0667654969541779. Std of Reward: 1.0084393459213918.\n",
      "Step: 930000. Mean Reward: -0.1426719558267196. Std of Reward: 1.0137938212972333.\n",
      "Step: 935000. Mean Reward: 0.025000001607142858. Std of Reward: 1.0044733931479104.\n",
      "Step: 940000. Mean Reward: -0.15634114407161462. Std of Reward: 1.0077525450528368.\n",
      "Saved Model\n",
      "Step: 945000. Mean Reward: -0.10185333180933334. Std of Reward: 1.0050737771473186.\n",
      "Step: 950000. Mean Reward: -0.0392578110390625. Std of Reward: 1.013671214300035.\n",
      "Step: 955000. Mean Reward: -0.0323505419089674. Std of Reward: 1.0060524601213594.\n",
      "Step: 960000. Mean Reward: -0.11314713740326975. Std of Reward: 1.0083643914331368.\n",
      "Saved Model\n",
      "Step: 965000. Mean Reward: 0.05841032768478261. Std of Reward: 0.9982796948432705.\n",
      "Step: 970000. Mean Reward: -0.03295865499741604. Std of Reward: 0.9939679012322274.\n",
      "Step: 975000. Mean Reward: 0.08345744816489359. Std of Reward: 0.9954213063464319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 980000. Mean Reward: -0.051801073744623664. Std of Reward: 1.0051483639886996.\n",
      "Saved Model\n",
      "Step: 985000. Mean Reward: -0.006989388683023878. Std of Reward: 1.0023375961269492.\n",
      "Step: 990000. Mean Reward: -0.05351562350390627. Std of Reward: 1.0083796089467214.\n",
      "Step: 995000. Mean Reward: -0.18160664669529086. Std of Reward: 1.011367478275578.\n",
      "Step: 1000000. Mean Reward: 0.025813334497333314. Std of Reward: 0.9966620645610872.\n",
      "Saved Model\n",
      "Step: 1005000. Mean Reward: -0.030241934548387094. Std of Reward: 0.9990391594872301.\n",
      "Step: 1010000. Mean Reward: -0.1363450273932749. Std of Reward: 1.0220251800284086.\n",
      "Step: 1015000. Mean Reward: -0.026093748660156253. Std of Reward: 1.0011631129684218.\n",
      "Step: 1020000. Mean Reward: -0.005540895977572562. Std of Reward: 0.9978203340322298.\n",
      "Saved Model\n",
      "Step: 1025000. Mean Reward: -0.11024999889583334. Std of Reward: 1.0096872139658988.\n",
      "Step: 1030000. Mean Reward: -0.05871657612032086. Std of Reward: 1.0033290623421967.\n",
      "Step: 1035000. Mean Reward: -0.03934959238211382. Std of Reward: 1.0023170224830467.\n",
      "Step: 1040000. Mean Reward: -0.02637680979420293. Std of Reward: 1.0059749329418195.\n",
      "Saved Model\n",
      "Step: 1045000. Mean Reward: -0.01437673016343491. Std of Reward: 1.0040211181562366.\n",
      "Step: 1050000. Mean Reward: -0.06259999889857143. Std of Reward: 1.0020110974726772.\n",
      "Step: 1055000. Mean Reward: 0.04426513084149855. Std of Reward: 1.0034648005474018.\n",
      "Step: 1060000. Mean Reward: 0.017592068898017003. Std of Reward: 0.9979708919371415.\n",
      "Saved Model\n",
      "Step: 1065000. Mean Reward: -0.00039589315689149917. Std of Reward: 1.0109657860790155.\n",
      "Step: 1070000. Mean Reward: 0.07128440470183484. Std of Reward: 0.9952639536197914.\n",
      "Step: 1075000. Mean Reward: -0.007683615120056506. Std of Reward: 1.0032750050319459.\n",
      "Step: 1080000. Mean Reward: -0.016622805846491236. Std of Reward: 1.0051666586887633.\n",
      "Saved Model\n",
      "Step: 1085000. Mean Reward: 0.05877005430614972. Std of Reward: 0.9926787935593402.\n",
      "Step: 1090000. Mean Reward: 0.03976945339625358. Std of Reward: 0.9981704895491449.\n",
      "Step: 1095000. Mean Reward: 0.012500000812130159. Std of Reward: 1.0049013204052557.\n",
      "Step: 1100000. Mean Reward: -0.07050151868541035. Std of Reward: 1.0057565191909155.\n",
      "Saved Model\n",
      "Step: 1105000. Mean Reward: 0.023074713604885038. Std of Reward: 0.9977270287674459.\n",
      "Step: 1110000. Mean Reward: -0.02348802324251499. Std of Reward: 1.0053385363423222.\n",
      "Step: 1115000. Mean Reward: 0.0455959310479651. Std of Reward: 0.9994348731708814.\n",
      "Step: 1120000. Mean Reward: -0.02361193934029852. Std of Reward: 1.0084146405387906.\n",
      "Saved Model\n",
      "Step: 1125000. Mean Reward: 0.13264534956976742. Std of Reward: 0.9761614171279929.\n",
      "Step: 1130000. Mean Reward: 0.19835243620916906. Std of Reward: 0.968851160596212.\n",
      "Step: 1135000. Mean Reward: -0.019100945430599373. Std of Reward: 1.008542876359309.\n",
      "Step: 1140000. Mean Reward: 0.048603604459459465. Std of Reward: 0.9905934088652782.\n",
      "Saved Model\n",
      "Step: 1145000. Mean Reward: 0.04256172913580246. Std of Reward: 1.0022736861503787.\n",
      "Step: 1150000. Mean Reward: 0.13539358664431486. Std of Reward: 0.9877117763855237.\n",
      "Step: 1155000. Mean Reward: -0.0024902716498054365. Std of Reward: 1.0289983343790283.\n",
      "Step: 1160000. Mean Reward: 0.034492064063492034. Std of Reward: 1.0048167623272566.\n",
      "Saved Model\n",
      "Step: 1165000. Mean Reward: -0.04192690968106315. Std of Reward: 1.0125786665194922.\n",
      "Step: 1170000. Mean Reward: 0.11898936225531911. Std of Reward: 0.9896739683376703.\n",
      "Step: 1175000. Mean Reward: 0.08177777828253965. Std of Reward: 0.9896359832270766.\n",
      "Step: 1180000. Mean Reward: 0.060000000544483956. Std of Reward: 0.9831163682099817.\n",
      "Saved Model\n",
      "Step: 1185000. Mean Reward: 0.06659932712457912. Std of Reward: 0.9892179090378503.\n",
      "Step: 1190000. Mean Reward: 0.00928093698160541. Std of Reward: 1.00775922866121.\n",
      "Step: 1195000. Mean Reward: 0.10692187579687498. Std of Reward: 1.005039866041369.\n",
      "Step: 1200000. Mean Reward: 0.0595364243923841. Std of Reward: 1.000026630399305.\n",
      "Saved Model\n",
      "Step: 1205000. Mean Reward: -0.022799999645000034. Std of Reward: 1.0237057650400438.\n",
      "Step: 1210000. Mean Reward: 0.10279720325874123. Std of Reward: 1.0006362700298115.\n",
      "Step: 1215000. Mean Reward: 0.011666667111666638. Std of Reward: 1.0016604823588375.\n",
      "Step: 1220000. Mean Reward: 0.11293220398983046. Std of Reward: 0.9813252689828107.\n",
      "Saved Model\n",
      "Step: 1225000. Mean Reward: -0.024237012378246783. Std of Reward: 1.0166197578987597.\n",
      "Step: 1230000. Mean Reward: 0.1497903230112903. Std of Reward: 0.9669613385081117.\n",
      "Step: 1235000. Mean Reward: 0.06377272803181817. Std of Reward: 1.0100612451623765.\n",
      "Step: 1240000. Mean Reward: 0.17392156906442577. Std of Reward: 0.9728255357845971.\n",
      "Saved Model\n",
      "Step: 1245000. Mean Reward: -0.09817484612423315. Std of Reward: 1.01629528246451.\n",
      "Step: 1250000. Mean Reward: 0.1781350487909968. Std of Reward: 0.9712054832229096.\n",
      "Step: 1255000. Mean Reward: 0.026569579797734597. Std of Reward: 1.0066137500590844.\n",
      "Step: 1260000. Mean Reward: 0.1327232147053571. Std of Reward: 0.9752009609585968.\n",
      "Saved Model\n",
      "Step: 1265000. Mean Reward: 0.13621993169415805. Std of Reward: 0.971158290841978.\n",
      "Step: 1270000. Mean Reward: 0.1813373866291793. Std of Reward: 0.9531883968464887.\n",
      "Step: 1275000. Mean Reward: 0.12235015816403785. Std of Reward: 0.9710144198721177.\n",
      "Step: 1280000. Mean Reward: 0.1390723273977987. Std of Reward: 0.9744653449677191.\n",
      "Saved Model\n",
      "Step: 1285000. Mean Reward: 0.08319571902140672. Std of Reward: 0.9851097603757337.\n",
      "Step: 1290000. Mean Reward: 0.17583606639016391. Std of Reward: 0.969352431206148.\n",
      "Step: 1295000. Mean Reward: 0.12315384659230766. Std of Reward: 0.9846898647645023.\n",
      "Step: 1300000. Mean Reward: 0.17810763920659717. Std of Reward: 0.9547796315143382.\n",
      "Saved Model\n",
      "Step: 1305000. Mean Reward: 0.018661017401694898. Std of Reward: 0.9927023485655629.\n",
      "Step: 1310000. Mean Reward: 0.19763066255923342. Std of Reward: 0.9651884847439587.\n",
      "Step: 1315000. Mean Reward: 0.09802250846302248. Std of Reward: 0.9799429224988799.\n",
      "Step: 1320000. Mean Reward: 0.09612099711387899. Std of Reward: 0.9807331155248376.\n",
      "Saved Model\n",
      "Step: 1325000. Mean Reward: 0.22347896466828474. Std of Reward: 0.9475907708368931.\n",
      "Step: 1330000. Mean Reward: 0.03456896592068964. Std of Reward: 0.9933195323852596.\n",
      "Step: 1335000. Mean Reward: 0.17570957159570957. Std of Reward: 0.9702166609074958.\n",
      "Step: 1340000. Mean Reward: 0.12184210610526312. Std of Reward: 0.9746607065710077.\n",
      "Saved Model\n",
      "Step: 1345000. Mean Reward: 0.1403380787402135. Std of Reward: 0.983405809146592.\n",
      "Step: 1350000. Mean Reward: 0.0661149829547038. Std of Reward: 0.9803742142145029.\n",
      "Step: 1355000. Mean Reward: 0.09976265851740504. Std of Reward: 0.9679516530235945.\n",
      "Step: 1360000. Mean Reward: 0.1910327871163934. Std of Reward: 0.9418499405461425.\n",
      "Saved Model\n",
      "Step: 1365000. Mean Reward: 0.09778301924056602. Std of Reward: 0.9836556801311864.\n",
      "Step: 1370000. Mean Reward: 0.12820652222826082. Std of Reward: 0.9750852568067586.\n",
      "Step: 1375000. Mean Reward: 0.12682432500844593. Std of Reward: 0.990034319337799.\n",
      "Step: 1380000. Mean Reward: 0.09055374629967425. Std of Reward: 0.9691004691862984.\n",
      "Saved Model\n",
      "Step: 1385000. Mean Reward: 0.2821725242300319. Std of Reward: 0.9229179532256901.\n",
      "Step: 1390000. Mean Reward: 0.1566833337883333. Std of Reward: 0.9750378195399604.\n",
      "Step: 1395000. Mean Reward: 0.17148692850163397. Std of Reward: 0.9653450201574469.\n",
      "Step: 1400000. Mean Reward: 0.17014285760476192. Std of Reward: 0.9634547106849096.\n",
      "Saved Model\n",
      "Step: 1405000. Mean Reward: 0.16967213160000003. Std of Reward: 0.9674994254613878.\n",
      "Step: 1410000. Mean Reward: 0.2744699650459364. Std of Reward: 0.9307992128731047.\n",
      "Step: 1415000. Mean Reward: 0.24663461579326926. Std of Reward: 0.9324799206182622.\n",
      "Step: 1420000. Mean Reward: 0.09908783811655403. Std of Reward: 0.9856834192180867.\n",
      "Saved Model\n",
      "Step: 1425000. Mean Reward: 0.2101023894522184. Std of Reward: 0.9471602913094381.\n",
      "Step: 1430000. Mean Reward: 0.07934375030937499. Std of Reward: 0.9843319627521341.\n",
      "Step: 1435000. Mean Reward: 0.18694055973951046. Std of Reward: 0.9650561561527932.\n",
      "Step: 1440000. Mean Reward: 0.1477822583064519. Std of Reward: 1.048779975131004.\n",
      "Saved Model\n",
      "Step: 1445000. Mean Reward: 0.04358361804948805. Std of Reward: 0.9928546548495157.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1450000. Mean Reward: 0.14777358519433964. Std of Reward: 0.9934091033609256.\n",
      "Step: 1455000. Mean Reward: 0.05376404538202245. Std of Reward: 0.9960737381021454.\n",
      "Step: 1460000. Mean Reward: 0.20650519075259513. Std of Reward: 0.9603704053006428.\n",
      "Saved Model\n",
      "Step: 1465000. Mean Reward: 0.14913533876879695. Std of Reward: 0.9860440653388655.\n",
      "Step: 1470000. Mean Reward: 0.11079787267021277. Std of Reward: 0.9856831440178092.\n",
      "Step: 1475000. Mean Reward: 0.1397718634543726. Std of Reward: 0.9829887424627739.\n",
      "Step: 1480000. Mean Reward: 0.10099236675572523. Std of Reward: 0.9936046296349377.\n",
      "Saved Model\n",
      "Step: 1485000. Mean Reward: 0.26094771267810457. Std of Reward: 0.9464101853747259.\n",
      "Step: 1490000. Mean Reward: 0.010567765941391923. Std of Reward: 1.0037908676741991.\n",
      "Step: 1495000. Mean Reward: 0.09692592634259257. Std of Reward: 0.9983322176270527.\n",
      "Step: 1500000. Mean Reward: 0.13790836690438266. Std of Reward: 1.0152802231424969.\n",
      "Saved Model\n",
      "Step: 1505000. Mean Reward: 0.13160231706563708. Std of Reward: 1.0022510570841303.\n",
      "Step: 1510000. Mean Reward: 0.09938297915106396. Std of Reward: 1.0284607078412857.\n",
      "Step: 1515000. Mean Reward: 0.28464426907707513. Std of Reward: 0.9480790187107411.\n",
      "Step: 1520000. Mean Reward: 0.11617647098269893. Std of Reward: 0.9778164890460277.\n",
      "Saved Model\n",
      "Step: 1525000. Mean Reward: 0.09818565441983132. Std of Reward: 1.0207642122563374.\n",
      "Step: 1530000. Mean Reward: 0.17805357177142858. Std of Reward: 0.9793766495297117.\n",
      "Step: 1535000. Mean Reward: 0.22107142882857142. Std of Reward: 0.9614067604003507.\n",
      "Step: 1540000. Mean Reward: 0.18757510772746783. Std of Reward: 0.9903833844357415.\n",
      "Saved Model\n",
      "Step: 1545000. Mean Reward: 0.10875555592222219. Std of Reward: 1.0034171649582373.\n",
      "Step: 1550000. Mean Reward: 0.09058685488967133. Std of Reward: 1.0055362048475325.\n",
      "Step: 1555000. Mean Reward: 0.15343750035336537. Std of Reward: 1.0165758823493924.\n",
      "Step: 1560000. Mean Reward: 0.07595121976829264. Std of Reward: 1.0351342305555262.\n",
      "Saved Model\n",
      "Step: 1565000. Mean Reward: 0.1465196082328431. Std of Reward: 1.0139715858391094.\n",
      "Step: 1570000. Mean Reward: 0.19207265005982904. Std of Reward: 0.9773102581814143.\n",
      "Step: 1575000. Mean Reward: -0.01824074023148153. Std of Reward: 1.0778994020902353.\n",
      "Step: 1580000. Mean Reward: 0.22262376266584158. Std of Reward: 0.974602887934283.\n",
      "Saved Model\n",
      "Step: 1585000. Mean Reward: 0.10502487603980099. Std of Reward: 1.0397740015651045.\n",
      "Step: 1590000. Mean Reward: 0.13591397883333334. Std of Reward: 1.0399066711522689.\n",
      "Step: 1595000. Mean Reward: 0.28376373666758237. Std of Reward: 0.9981372673000268.\n",
      "Step: 1600000. Mean Reward: 0.09116279115116309. Std of Reward: 1.0904977190265397.\n",
      "Saved Model\n",
      "Step: 1605000. Mean Reward: -0.14376190436190428. Std of Reward: 1.2046824872122537.\n",
      "Step: 1610000. Mean Reward: 0.10003906305078139. Std of Reward: 1.1148017788974964.\n",
      "Step: 1615000. Mean Reward: 0.024098360926229984. Std of Reward: 1.159243325704567.\n",
      "Step: 1620000. Mean Reward: 0.053333333858975665. Std of Reward: 1.231579314094926.\n",
      "Saved Model\n",
      "Step: 1625000. Mean Reward: 0.15187500054545658. Std of Reward: 1.3234618667804638.\n",
      "Step: 1630000. Mean Reward: -0.17218309810563118. Std of Reward: 1.451932110079543.\n",
      "Step: 1635000. Mean Reward: -0.026857142407140266. Std of Reward: 1.434349322679123.\n",
      "Step: 1640000. Mean Reward: -0.49833333277082664. Std of Reward: 1.9339067028601895.\n",
      "Saved Model\n",
      "Step: 1645000. Mean Reward: -0.13012499956874773. Std of Reward: 1.4244799433686406.\n",
      "Step: 1650000. Mean Reward: -0.3356249992187432. Std of Reward: 1.929061359714461.\n",
      "Step: 1655000. Mean Reward: -0.21294117600000295. Std of Reward: 2.171938045614754.\n",
      "Step: 1660000. Mean Reward: -0.2535454540272676. Std of Reward: 1.8076058173429161.\n",
      "Saved Model\n",
      "Step: 1665000. Mean Reward: -0.4762499994000284. Std of Reward: 3.324202487516412.\n",
      "Step: 1670000. Mean Reward: -1.5108823510882805. Std of Reward: 4.77252098432714.\n",
      "Step: 1675000. Mean Reward: -1.250526315157899. Std of Reward: 3.73830453134976.\n",
      "Step: 1680000. Mean Reward: -4.160833330083427. Std of Reward: 7.606983856431887.\n",
      "Saved Model\n",
      "Step: 1685000. Mean Reward: -4.234999997000118. Std of Reward: 6.2410856403941315.\n",
      "Step: 1690000. Mean Reward: -1.6826923061923262. Std of Reward: 4.281417635705793.\n",
      "Step: 1695000. Mean Reward: -3.0628571417855963. Std of Reward: 9.66634287841763.\n",
      "Step: 1700000. Mean Reward: -1.150999999174999. Std of Reward: 3.6254815530239184.\n",
      "Saved Model\n",
      "Step: 1705000. Mean Reward: -2.113888885722298. Std of Reward: 6.250461216524306.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-583a438fd989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_variance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mrun_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_variance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons11/model-1700000.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons11/model-1700000.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 7 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 7 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
