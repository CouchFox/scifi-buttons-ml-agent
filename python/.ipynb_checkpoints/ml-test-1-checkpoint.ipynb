{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ml-test-1\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 2000 # Frequency at which to save training statistics.\n",
    "save_freq = 8000 # Frequency at which to save model.\n",
    "env_name = \"ml-test-1\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 2 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 5000 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 64 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 16\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 4\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000. Mean Reward: 0.10834742268041238. Std of Reward: 1.0028308246687119.\n",
      "Step: 4000. Mean Reward: -0.14692631578947357. Std of Reward: 1.0431907950292674.\n",
      "Step: 6000. Mean Reward: -0.05445346534653462. Std of Reward: 1.019684481831508.\n",
      "Step: 8000. Mean Reward: -0.007383695652173895. Std of Reward: 1.019088908661019.\n",
      "Saved Model\n",
      "Step: 10000. Mean Reward: 0.15242093023255815. Std of Reward: 1.0206079822906609.\n",
      "Step: 12000. Mean Reward: 0.009364285714285752. Std of Reward: 1.0258590193267365.\n",
      "Step: 14000. Mean Reward: -0.0961747126436781. Std of Reward: 1.038528507981021.\n",
      "Step: 16000. Mean Reward: 0.21440645161290328. Std of Reward: 0.9893735629453371.\n",
      "Saved Model\n",
      "Step: 18000. Mean Reward: -0.0008038095238094818. Std of Reward: 1.0332824796940236.\n",
      "Step: 20000. Mean Reward: -0.04397938144329893. Std of Reward: 1.0256931648601615.\n",
      "Step: 22000. Mean Reward: 0.0793967741935484. Std of Reward: 1.0123678732801675.\n",
      "Step: 24000. Mean Reward: -0.0271670329670329. Std of Reward: 1.0325661533824246.\n",
      "Saved Model\n",
      "Step: 26000. Mean Reward: -0.12927678571428564. Std of Reward: 1.0631792082830545.\n",
      "Step: 28000. Mean Reward: -0.08968709677419348. Std of Reward: 1.0491032439148429.\n",
      "Step: 30000. Mean Reward: 0.046570370370370415. Std of Reward: 1.0209157875627766.\n",
      "Step: 32000. Mean Reward: -0.01589249999999995. Std of Reward: 1.0458223788931609.\n",
      "Saved Model\n",
      "Step: 34000. Mean Reward: -0.059996202531645504. Std of Reward: 1.0344350129715876.\n",
      "Step: 36000. Mean Reward: 0.04409780219780223. Std of Reward: 1.0267270815534035.\n",
      "Step: 38000. Mean Reward: -0.05401176470588232. Std of Reward: 1.0304509496459777.\n",
      "Step: 40000. Mean Reward: 0.183729347826087. Std of Reward: 1.0022670107157772.\n",
      "Saved Model\n",
      "Step: 42000. Mean Reward: 0.13032244897959186. Std of Reward: 1.009805353696041.\n",
      "Step: 44000. Mean Reward: 0.024169512195121978. Std of Reward: 1.047101140511621.\n",
      "Step: 46000. Mean Reward: 0.24352222222222228. Std of Reward: 0.9827592532642687.\n",
      "Step: 48000. Mean Reward: 0.18570114942528737. Std of Reward: 1.0002963680938102.\n",
      "Saved Model\n",
      "Step: 50000. Mean Reward: -0.0858216216216216. Std of Reward: 1.051365819698264.\n",
      "Step: 52000. Mean Reward: 0.004425675675675712. Std of Reward: 1.0537727078199384.\n",
      "Step: 54000. Mean Reward: 0.11335643564356437. Std of Reward: 1.0334536260094342.\n",
      "Step: 56000. Mean Reward: 0.23606444444444455. Std of Reward: 1.0188171922284424.\n",
      "Saved Model\n",
      "Step: 58000. Mean Reward: 0.059570588235294174. Std of Reward: 1.0251803653445033.\n",
      "Step: 60000. Mean Reward: 0.23179622641509437. Std of Reward: 0.992498794726185.\n",
      "Step: 62000. Mean Reward: 0.19096666666666667. Std of Reward: 1.0076167314191928.\n",
      "Step: 64000. Mean Reward: 0.0960580645161291. Std of Reward: 1.0242981825308677.\n",
      "Saved Model\n",
      "Step: 66000. Mean Reward: 0.12451666666666672. Std of Reward: 1.0333229120442242.\n",
      "Step: 68000. Mean Reward: 0.12462659574468092. Std of Reward: 1.0288148948716127.\n",
      "Step: 70000. Mean Reward: 0.24474406779661023. Std of Reward: 0.9596911071323296.\n",
      "Step: 72000. Mean Reward: 0.17971100917431196. Std of Reward: 1.016606397211465.\n",
      "Saved Model\n",
      "Step: 74000. Mean Reward: 0.08036697247706427. Std of Reward: 1.0348744381626178.\n",
      "Step: 76000. Mean Reward: 0.22990960000000002. Std of Reward: 0.9953226959272253.\n",
      "Step: 78000. Mean Reward: 0.24581637931034486. Std of Reward: 0.9804589821507038.\n",
      "Step: 80000. Mean Reward: 0.0977721739130435. Std of Reward: 1.0306090168732716.\n",
      "Saved Model\n",
      "Step: 82000. Mean Reward: 0.24184623655913984. Std of Reward: 0.9870934198317218.\n",
      "Step: 84000. Mean Reward: 0.2933428571428572. Std of Reward: 0.9832011507468509.\n",
      "Step: 86000. Mean Reward: 0.2243390909090909. Std of Reward: 0.995290281192144.\n",
      "Step: 88000. Mean Reward: 0.11535420560747667. Std of Reward: 1.02983833467512.\n",
      "Saved Model\n",
      "Step: 90000. Mean Reward: 0.10933636363636373. Std of Reward: 1.0456343717843761.\n",
      "Step: 92000. Mean Reward: 0.14501226415094345. Std of Reward: 1.0189526816742276.\n",
      "Step: 94000. Mean Reward: 0.297047572815534. Std of Reward: 0.973969935426894.\n",
      "Step: 96000. Mean Reward: 0.3382861538461538. Std of Reward: 0.9506897021680019.\n",
      "Saved Model\n",
      "Step: 98000. Mean Reward: 0.29664375. Std of Reward: 1.005070731766573.\n",
      "Step: 100000. Mean Reward: 0.36895611510791365. Std of Reward: 0.9334789703437975.\n",
      "Step: 102000. Mean Reward: 0.20629464285714288. Std of Reward: 1.0018743847551168.\n",
      "Step: 104000. Mean Reward: 0.07229583333333339. Std of Reward: 1.059037776603982.\n",
      "Saved Model\n",
      "Step: 106000. Mean Reward: 0.1585395161290323. Std of Reward: 1.0164712837143484.\n",
      "Step: 108000. Mean Reward: 0.21433148148148154. Std of Reward: 1.0275706378934581.\n",
      "Step: 110000. Mean Reward: 0.18394727272727285. Std of Reward: 1.037924345089239.\n",
      "Step: 112000. Mean Reward: 0.22889333333333337. Std of Reward: 1.0042945573662916.\n",
      "Saved Model\n",
      "Step: 114000. Mean Reward: 0.2957191304347826. Std of Reward: 0.9951823228102609.\n",
      "Step: 116000. Mean Reward: 0.3508284615384616. Std of Reward: 0.9503205213331257.\n",
      "Step: 118000. Mean Reward: 0.319408064516129. Std of Reward: 0.9623373517033155.\n",
      "Step: 120000. Mean Reward: 0.31579826086956525. Std of Reward: 0.9725408572155193.\n",
      "Saved Model\n",
      "Step: 122000. Mean Reward: 0.28202666666666676. Std of Reward: 0.9855638695707015.\n",
      "Step: 124000. Mean Reward: 0.1981691588785047. Std of Reward: 1.0377680700328307.\n",
      "Step: 126000. Mean Reward: 0.27785419847328247. Std of Reward: 0.9735191484537101.\n",
      "Step: 128000. Mean Reward: 0.21648137254901964. Std of Reward: 1.0124081036709003.\n",
      "Saved Model\n",
      "Step: 130000. Mean Reward: 0.3740725806451613. Std of Reward: 0.9420157828910941.\n",
      "Step: 132000. Mean Reward: 0.3395533980582524. Std of Reward: 0.9685239567101389.\n",
      "Step: 134000. Mean Reward: 0.31879292929292935. Std of Reward: 0.9808678645606568.\n",
      "Step: 136000. Mean Reward: 0.4604613445378152. Std of Reward: 0.9190822076333043.\n",
      "Saved Model\n",
      "Step: 138000. Mean Reward: 0.3599713235294118. Std of Reward: 0.9463465444499525.\n",
      "Step: 140000. Mean Reward: 0.4543917910447761. Std of Reward: 0.881059717091791.\n",
      "Step: 142000. Mean Reward: 0.4454848484848485. Std of Reward: 0.9144381402512634.\n",
      "Step: 144000. Mean Reward: 0.23931792452830192. Std of Reward: 1.031600037567322.\n",
      "Saved Model\n",
      "Step: 146000. Mean Reward: 0.31954919354838707. Std of Reward: 1.006412207803283.\n",
      "Step: 148000. Mean Reward: 0.44575739130434794. Std of Reward: 0.8875820398148503.\n",
      "Step: 150000. Mean Reward: 0.26466555555555565. Std of Reward: 1.020529326287873.\n",
      "Step: 152000. Mean Reward: 0.33005272727272733. Std of Reward: 0.976836727653379.\n",
      "Saved Model\n",
      "Step: 154000. Mean Reward: 0.35900190476190474. Std of Reward: 0.9593412029176166.\n",
      "Step: 156000. Mean Reward: 0.10637765957446815. Std of Reward: 1.0796027766507315.\n",
      "Step: 158000. Mean Reward: 0.3188256410256411. Std of Reward: 0.977020855803944.\n",
      "Step: 160000. Mean Reward: 0.5031638157894737. Std of Reward: 0.8955990112143221.\n",
      "Saved Model\n",
      "Step: 162000. Mean Reward: 0.38268217054263565. Std of Reward: 0.9432109254229136.\n",
      "Step: 164000. Mean Reward: 0.5827066176470588. Std of Reward: 0.7867367551738643.\n",
      "Step: 166000. Mean Reward: 0.5285874125874126. Std of Reward: 0.839869268870225.\n",
      "Step: 168000. Mean Reward: 0.47314263565891473. Std of Reward: 0.9142681949888922.\n",
      "Saved Model\n",
      "Step: 170000. Mean Reward: 0.3526085714285714. Std of Reward: 0.9782260375767949.\n",
      "Step: 172000. Mean Reward: 0.40141119402985076. Std of Reward: 0.9184217184574825.\n",
      "Step: 174000. Mean Reward: 0.3480685185185185. Std of Reward: 0.9672307051432348.\n",
      "Step: 176000. Mean Reward: 0.42623200000000006. Std of Reward: 0.904618244514963.\n",
      "Saved Model\n",
      "Step: 178000. Mean Reward: 0.39050215827338125. Std of Reward: 0.9759779865771111.\n",
      "Step: 180000. Mean Reward: 0.4217748031496063. Std of Reward: 0.9146816689035103.\n",
      "Step: 182000. Mean Reward: 0.3619495798319329. Std of Reward: 0.9604470202995813.\n",
      "Step: 184000. Mean Reward: 0.3778646551724139. Std of Reward: 0.9734803868925846.\n",
      "Saved Model\n",
      "Step: 186000. Mean Reward: 0.4579558441558442. Std of Reward: 0.8805044208084638.\n",
      "Step: 188000. Mean Reward: 0.4177592105263158. Std of Reward: 0.8977449810318061.\n",
      "Step: 190000. Mean Reward: 0.41425401459854017. Std of Reward: 0.9209144229811399.\n",
      "Step: 192000. Mean Reward: 0.39261221374045807. Std of Reward: 0.951068172962284.\n",
      "Saved Model\n",
      "Step: 194000. Mean Reward: 0.3854273504273505. Std of Reward: 0.9701256232534938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 196000. Mean Reward: 0.5930954198473283. Std of Reward: 0.793165184307369.\n",
      "Step: 198000. Mean Reward: 0.49647597402597404. Std of Reward: 0.857371979391751.\n",
      "Step: 200000. Mean Reward: 0.2713635593220339. Std of Reward: 0.9711358699529975.\n",
      "Saved Model\n",
      "Step: 202000. Mean Reward: 0.3726402877697842. Std of Reward: 0.9431976179117766.\n",
      "Step: 204000. Mean Reward: 0.45155248226950356. Std of Reward: 0.8949006497876131.\n",
      "Step: 206000. Mean Reward: 0.35471171875. Std of Reward: 0.9471932815413368.\n",
      "Step: 208000. Mean Reward: 0.4364146153846154. Std of Reward: 0.9105571458526441.\n",
      "Saved Model\n",
      "Step: 210000. Mean Reward: 0.4620373983739838. Std of Reward: 0.893809511489447.\n",
      "Step: 212000. Mean Reward: 0.3864727272727273. Std of Reward: 0.9246013229814218.\n",
      "Step: 214000. Mean Reward: 0.3437488372093023. Std of Reward: 0.9583617009983799.\n",
      "Step: 216000. Mean Reward: 0.621483950617284. Std of Reward: 0.744932874257691.\n",
      "Saved Model\n",
      "Step: 218000. Mean Reward: 0.42218275862068966. Std of Reward: 0.9132253212421932.\n",
      "Step: 220000. Mean Reward: 0.44776513157894743. Std of Reward: 0.8877522013237962.\n",
      "Step: 222000. Mean Reward: 0.370464. Std of Reward: 0.9214839249297841.\n",
      "Step: 224000. Mean Reward: 0.2693944827586207. Std of Reward: 0.9709961854237591.\n",
      "Saved Model\n",
      "Step: 226000. Mean Reward: 0.5370265432098765. Std of Reward: 0.8220455783820221.\n",
      "Step: 228000. Mean Reward: 0.4416020408163266. Std of Reward: 0.8844818835005797.\n",
      "Step: 230000. Mean Reward: 0.3625724719101124. Std of Reward: 0.9286108990759551.\n",
      "Step: 232000. Mean Reward: 0.48950789473684214. Std of Reward: 0.8540749295772648.\n",
      "Saved Model\n",
      "Step: 234000. Mean Reward: 0.36508773006134976. Std of Reward: 0.9332202390504478.\n",
      "Step: 236000. Mean Reward: 0.5485628571428571. Std of Reward: 0.8127779262630157.\n",
      "Step: 238000. Mean Reward: 0.4277514705882353. Std of Reward: 0.8953372266412261.\n",
      "Step: 240000. Mean Reward: 0.42645378787878785. Std of Reward: 0.9006502107825332.\n",
      "Saved Model\n",
      "Step: 242000. Mean Reward: 0.5456423529411765. Std of Reward: 0.8295562391141948.\n",
      "Step: 244000. Mean Reward: 0.584574846625767. Std of Reward: 0.7830163517824636.\n",
      "Step: 246000. Mean Reward: 0.5231606741573034. Std of Reward: 0.8437044489068215.\n",
      "Step: 248000. Mean Reward: 0.497374358974359. Std of Reward: 0.8586734716299318.\n",
      "Saved Model\n",
      "Step: 250000. Mean Reward: 0.44551043478260877. Std of Reward: 0.9083602791627783.\n",
      "Step: 252000. Mean Reward: 0.40257375886524827. Std of Reward: 0.9277530094128088.\n",
      "Step: 254000. Mean Reward: 0.4383953947368421. Std of Reward: 0.8778315615420295.\n",
      "Step: 256000. Mean Reward: 0.5219637583892617. Std of Reward: 0.8416665687282189.\n",
      "Saved Model\n",
      "Step: 258000. Mean Reward: 0.5023508196721312. Std of Reward: 0.8776182984334532.\n",
      "Step: 260000. Mean Reward: 0.5571340277777779. Std of Reward: 0.8220512309294755.\n",
      "Step: 262000. Mean Reward: 0.410431007751938. Std of Reward: 0.9189189609062041.\n",
      "Step: 264000. Mean Reward: 0.482710625. Std of Reward: 0.8680933225031796.\n",
      "Saved Model\n",
      "Step: 266000. Mean Reward: 0.5871028368794325. Std of Reward: 0.7714440141334344.\n",
      "Step: 268000. Mean Reward: 0.5418308176100629. Std of Reward: 0.8359258204841873.\n",
      "Step: 270000. Mean Reward: 0.4859490566037737. Std of Reward: 0.8763235946982691.\n",
      "Step: 272000. Mean Reward: 0.47952631578947374. Std of Reward: 0.8483579384174467.\n",
      "Saved Model\n",
      "Step: 274000. Mean Reward: 0.5471948051948052. Std of Reward: 0.825717677750475.\n",
      "Step: 276000. Mean Reward: 0.41502818791946305. Std of Reward: 0.9030414534904916.\n",
      "Step: 278000. Mean Reward: 0.5504390728476822. Std of Reward: 0.8150804856023339.\n",
      "Step: 280000. Mean Reward: 0.49092857142857144. Std of Reward: 0.8618829274037936.\n",
      "Saved Model\n",
      "Step: 282000. Mean Reward: 0.40174625850340134. Std of Reward: 0.9081952172049952.\n",
      "Step: 284000. Mean Reward: 0.4704877551020409. Std of Reward: 0.8820257385062396.\n",
      "Step: 286000. Mean Reward: 0.5318187878787879. Std of Reward: 0.8401362889860537.\n",
      "Step: 288000. Mean Reward: 0.4726217391304348. Std of Reward: 0.8869037404869421.\n",
      "Saved Model\n",
      "Step: 290000. Mean Reward: 0.5702736486486486. Std of Reward: 0.8007468131048029.\n",
      "Step: 292000. Mean Reward: 0.43394259259259255. Std of Reward: 0.9088560542700928.\n",
      "Step: 294000. Mean Reward: 0.5045786206896553. Std of Reward: 0.8565552483891071.\n",
      "Step: 296000. Mean Reward: 0.5325135135135135. Std of Reward: 0.8296016157761905.\n",
      "Saved Model\n",
      "Step: 298000. Mean Reward: 0.4253357664233577. Std of Reward: 0.9472245570969317.\n",
      "Step: 300000. Mean Reward: 0.41365769230769234. Std of Reward: 0.9051160738736321.\n",
      "Step: 302000. Mean Reward: 0.4418798780487805. Std of Reward: 0.8972237359427188.\n",
      "Step: 304000. Mean Reward: 0.5522490566037737. Std of Reward: 0.8164852324605474.\n",
      "Saved Model\n",
      "Step: 306000. Mean Reward: 0.5332834586466166. Std of Reward: 0.8425562595776327.\n",
      "Step: 308000. Mean Reward: 0.3307329192546584. Std of Reward: 0.9570030582566578.\n",
      "Step: 310000. Mean Reward: 0.45435781250000007. Std of Reward: 0.9163986372965451.\n",
      "Step: 312000. Mean Reward: 0.5285509316770186. Std of Reward: 0.8465553076785707.\n",
      "Saved Model\n",
      "Step: 314000. Mean Reward: 0.4652420560747664. Std of Reward: 0.9485519770108012.\n",
      "Step: 316000. Mean Reward: 0.5018946564885497. Std of Reward: 0.8563844449807777.\n",
      "Step: 318000. Mean Reward: 0.41722713178294574. Std of Reward: 0.9265637028641996.\n",
      "Step: 320000. Mean Reward: 0.48235038167938926. Std of Reward: 0.8583961794069594.\n",
      "Saved Model\n",
      "Step: 322000. Mean Reward: 0.485276551724138. Std of Reward: 0.8662573089915102.\n",
      "Step: 324000. Mean Reward: 0.5659284848484849. Std of Reward: 0.8163405338019222.\n",
      "Step: 326000. Mean Reward: 0.5400536231884058. Std of Reward: 0.8316039534450418.\n",
      "Step: 328000. Mean Reward: 0.490920134228188. Std of Reward: 0.8523446898601925.\n",
      "Saved Model\n",
      "Step: 330000. Mean Reward: 0.4493854838709677. Std of Reward: 0.9099819732965955.\n",
      "Step: 332000. Mean Reward: 0.40347175572519095. Std of Reward: 0.9004866447059967.\n",
      "Step: 334000. Mean Reward: 0.5955874172185431. Std of Reward: 0.7883616976144355.\n",
      "Step: 336000. Mean Reward: 0.5880349315068494. Std of Reward: 0.7980620585263041.\n",
      "Saved Model\n",
      "Step: 338000. Mean Reward: 0.3979608391608392. Std of Reward: 0.935815168532355.\n",
      "Step: 340000. Mean Reward: 0.6308072727272728. Std of Reward: 0.7620787876681611.\n",
      "Step: 342000. Mean Reward: 0.5366278571428572. Std of Reward: 0.8409698655266903.\n",
      "Step: 344000. Mean Reward: 0.539711724137931. Std of Reward: 0.8567173370007852.\n",
      "Saved Model\n",
      "Step: 346000. Mean Reward: 0.5206626506024096. Std of Reward: 0.8497775773072499.\n",
      "Step: 348000. Mean Reward: 0.517817261904762. Std of Reward: 0.8452106862589424.\n",
      "Step: 350000. Mean Reward: 0.7252539325842698. Std of Reward: 0.6585021919129789.\n",
      "Step: 352000. Mean Reward: 0.5639272222222222. Std of Reward: 0.8009673667808442.\n",
      "Saved Model\n",
      "Step: 354000. Mean Reward: 0.5333422818791945. Std of Reward: 0.8358683991487321.\n",
      "Step: 356000. Mean Reward: 0.5247361445783133. Std of Reward: 0.8407366463840431.\n",
      "Step: 358000. Mean Reward: 0.5722486666666666. Std of Reward: 0.8218600935063232.\n",
      "Step: 360000. Mean Reward: 0.4592741935483871. Std of Reward: 0.8778958616938505.\n",
      "Saved Model\n",
      "Step: 362000. Mean Reward: 0.4396496503496503. Std of Reward: 0.9242920202639715.\n",
      "Step: 364000. Mean Reward: 0.5269118881118882. Std of Reward: 0.8498489298486916.\n",
      "Step: 366000. Mean Reward: 0.5604525974025973. Std of Reward: 0.8172218205919951.\n",
      "Step: 368000. Mean Reward: 0.5329167701863354. Std of Reward: 0.827076336254431.\n",
      "Saved Model\n",
      "Step: 370000. Mean Reward: 0.5667454022988506. Std of Reward: 0.8074563046834881.\n",
      "Step: 372000. Mean Reward: 0.5491656441717792. Std of Reward: 0.8367912212864873.\n",
      "Step: 374000. Mean Reward: 0.5810737704918033. Std of Reward: 0.8015500320983223.\n",
      "Step: 376000. Mean Reward: 0.6253671875000001. Std of Reward: 0.7570690220049886.\n",
      "Saved Model\n",
      "Step: 378000. Mean Reward: 0.6458437086092715. Std of Reward: 0.7418327270421207.\n",
      "Step: 380000. Mean Reward: 0.6718861271676301. Std of Reward: 0.7276922679277033.\n",
      "Step: 382000. Mean Reward: 0.4794522988505747. Std of Reward: 0.8760341798890016.\n",
      "Step: 384000. Mean Reward: 0.6287505. Std of Reward: 0.770481883693413.\n",
      "Saved Model\n",
      "Step: 386000. Mean Reward: 0.5853900826446282. Std of Reward: 0.8381069420711936.\n",
      "Step: 388000. Mean Reward: 0.4716424418604651. Std of Reward: 0.8745332476034314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 390000. Mean Reward: 0.5862835294117648. Std of Reward: 0.8057472552703971.\n",
      "Step: 392000. Mean Reward: 0.5580459893048129. Std of Reward: 0.820918548411245.\n",
      "Saved Model\n",
      "Step: 394000. Mean Reward: 0.518455625. Std of Reward: 0.8549887265153614.\n",
      "Step: 396000. Mean Reward: 0.6095760204081634. Std of Reward: 0.786347379931401.\n",
      "Step: 398000. Mean Reward: 0.5566646706586827. Std of Reward: 0.827553363011762.\n",
      "Step: 400000. Mean Reward: 0.40802534246575345. Std of Reward: 0.9364461000668229.\n",
      "Saved Model\n",
      "Step: 402000. Mean Reward: 0.6518142857142858. Std of Reward: 0.7624401829999919.\n",
      "Step: 404000. Mean Reward: 0.5305086956521738. Std of Reward: 0.8459664984183031.\n",
      "Step: 406000. Mean Reward: 0.5102162337662339. Std of Reward: 0.8829478446062974.\n",
      "Step: 408000. Mean Reward: 0.6069365714285714. Std of Reward: 0.7759323321230783.\n",
      "Saved Model\n",
      "Step: 410000. Mean Reward: 0.507277397260274. Std of Reward: 0.8894049576743382.\n",
      "Step: 412000. Mean Reward: 0.591156157635468. Std of Reward: 0.7863105108084822.\n",
      "Step: 414000. Mean Reward: 0.5474301204819277. Std of Reward: 0.8464619184190866.\n",
      "Step: 416000. Mean Reward: 0.6383978378378379. Std of Reward: 0.7576510184993899.\n",
      "Saved Model\n",
      "Step: 418000. Mean Reward: 0.6651086956521739. Std of Reward: 0.7679472157222564.\n",
      "Step: 420000. Mean Reward: 0.5412019417475729. Std of Reward: 0.8270485879858772.\n",
      "Step: 422000. Mean Reward: 0.5532327044025159. Std of Reward: 0.8254257848080708.\n",
      "Step: 424000. Mean Reward: 0.513123717948718. Std of Reward: 0.8429286474891998.\n",
      "Saved Model\n",
      "Step: 426000. Mean Reward: 0.5571271186440679. Std of Reward: 0.8701350785124108.\n",
      "Step: 428000. Mean Reward: 0.7021581818181818. Std of Reward: 0.6911352625646666.\n",
      "Step: 430000. Mean Reward: 0.6162978417266187. Std of Reward: 0.8005357876207215.\n",
      "Step: 432000. Mean Reward: 0.56888322147651. Std of Reward: 0.8521203475832865.\n",
      "Saved Model\n",
      "Step: 434000. Mean Reward: 0.5765715277777778. Std of Reward: 0.8196529501583102.\n",
      "Step: 436000. Mean Reward: 0.7107556756756757. Std of Reward: 0.675467105792967.\n",
      "Step: 438000. Mean Reward: 0.5512486301369863. Std of Reward: 0.8290163508836059.\n",
      "Step: 440000. Mean Reward: 0.5768602649006622. Std of Reward: 0.7943964370613855.\n",
      "Saved Model\n",
      "Step: 442000. Mean Reward: 0.5818798941798942. Std of Reward: 0.799011181966764.\n",
      "Step: 444000. Mean Reward: 0.6342934426229508. Std of Reward: 0.7484159958925387.\n",
      "Step: 446000. Mean Reward: 0.6166628571428572. Std of Reward: 0.7752425103653379.\n",
      "Step: 448000. Mean Reward: 0.5837656804733727. Std of Reward: 0.7924586019872522.\n",
      "Saved Model\n",
      "Step: 450000. Mean Reward: 0.6181935672514621. Std of Reward: 0.7757719420673648.\n",
      "Step: 452000. Mean Reward: 0.6744190243902439. Std of Reward: 0.7144275780456819.\n",
      "Step: 454000. Mean Reward: 0.6362635838150289. Std of Reward: 0.7623643809672007.\n",
      "Step: 456000. Mean Reward: 0.5682139072847683. Std of Reward: 0.83462498506852.\n",
      "Saved Model\n",
      "Step: 458000. Mean Reward: 0.650909090909091. Std of Reward: 0.7390082175670586.\n",
      "Step: 460000. Mean Reward: 0.5807771276595745. Std of Reward: 0.8027145301891926.\n",
      "Step: 462000. Mean Reward: 0.5714644295302014. Std of Reward: 0.8261832181440116.\n",
      "Step: 464000. Mean Reward: 0.6679280612244899. Std of Reward: 0.7185797553325534.\n",
      "Saved Model\n",
      "Step: 466000. Mean Reward: 0.60623. Std of Reward: 0.7878835581268762.\n",
      "Step: 468000. Mean Reward: 0.5712534482758621. Std of Reward: 0.8258951956587756.\n",
      "Step: 470000. Mean Reward: 0.6186379679144386. Std of Reward: 0.7765473800601116.\n",
      "Step: 472000. Mean Reward: 0.5314659793814432. Std of Reward: 0.8457086324541957.\n",
      "Saved Model\n",
      "Step: 474000. Mean Reward: 0.7417695876288661. Std of Reward: 0.6394481838650287.\n",
      "Step: 476000. Mean Reward: 0.6404740331491713. Std of Reward: 0.7558370337519298.\n",
      "Step: 478000. Mean Reward: 0.6825491017964073. Std of Reward: 0.7266415701576874.\n",
      "Step: 480000. Mean Reward: 0.5972545454545454. Std of Reward: 0.7979300396407927.\n",
      "Saved Model\n",
      "Step: 482000. Mean Reward: 0.5101143750000001. Std of Reward: 0.8579713756768692.\n",
      "Step: 484000. Mean Reward: 0.5989459627329193. Std of Reward: 0.834455157237477.\n",
      "Step: 486000. Mean Reward: 0.6321911764705883. Std of Reward: 0.7424168919096154.\n",
      "Step: 488000. Mean Reward: 0.6789719576719577. Std of Reward: 0.7136917223175796.\n",
      "Saved Model\n",
      "Step: 490000. Mean Reward: 0.6284974489795918. Std of Reward: 0.7679959973745228.\n",
      "Step: 492000. Mean Reward: 0.590475590551181. Std of Reward: 0.8702481634221423.\n",
      "Step: 494000. Mean Reward: 0.5387685185185186. Std of Reward: 0.8365640576354179.\n",
      "Step: 496000. Mean Reward: 0.6715672727272728. Std of Reward: 0.7251363101253853.\n",
      "Saved Model\n",
      "Step: 498000. Mean Reward: 0.6492901734104046. Std of Reward: 0.7761359311003818.\n",
      "Step: 500000. Mean Reward: 0.5402454054054054. Std of Reward: 0.8276992930213719.\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/ml-test-1/model-500001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ml-test-1/model-500001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ml-test-1/model-500001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ml-test-1/model-500001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 4 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
