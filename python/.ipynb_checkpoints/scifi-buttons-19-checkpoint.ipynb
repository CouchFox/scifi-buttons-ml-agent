{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esmu/miniconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 3e6 # Set maximum number of steps to run environment.\n",
    "run_path = \"scifibuttons19\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 5000 # Frequency at which to save training statistics.\n",
    "save_freq = 20000 # Frequency at which to save model.\n",
    "env_name = \"scifibuttons19\" # Name of the training environment file.\n",
    "curriculum_file = 'curricula/lessons19.json'\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 5 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 3 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 2048 #2048 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 3e-4 # Model learning rate.\n",
    "hidden_units = 256 # Number of units in hidden layer.\n",
    "batch_size = 64 #64 # How many experiences per gradient descent update step.\n",
    "normalize = True\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Academy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\tlessonNr -> 4.0\n",
      "Unity brain name: Brain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 30\n",
      "        Action space type: discrete\n",
      "        Action space size (per agent): 5\n",
      "        Memory space size (per agent): 9\n",
      "        Action descriptions: , , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Mean Reward: -1.7119907269777779. Std of Reward: 3.184285648999686.\n",
      "Step: 10000. Mean Reward: -1.1453448163530169. Std of Reward: 2.9964881377593047.\n",
      "Step: 15000. Mean Reward: -1.1018799894212001. Std of Reward: 2.834287490419528.\n",
      "Step: 20000. Mean Reward: -0.9187999885265455. Std of Reward: 2.887724161522148.\n",
      "Saved Model\n",
      "Step: 25000. Mean Reward: -0.7704545355129371. Std of Reward: 2.5572239779180155.\n",
      "Step: 30000. Mean Reward: -0.4904154240643917. Std of Reward: 2.3176146078214694.\n",
      "Step: 35000. Mean Reward: -0.35847024835524083. Std of Reward: 2.1327097489322866.\n",
      "Step: 40000. Mean Reward: -0.08179999458311112. Std of Reward: 1.952355969105436.\n",
      "Saved Model\n",
      "Step: 45000. Mean Reward: 0.019482762532327565. Std of Reward: 1.5803516555176294.\n",
      "Step: 50000. Mean Reward: 0.0875155327652174. Std of Reward: 1.7721420311965952.\n",
      "Step: 55000. Mean Reward: 0.25727586608413794. Std of Reward: 1.472134787792539.\n",
      "Step: 60000. Mean Reward: 0.3312924103365105. Std of Reward: 1.4773384349009298.\n",
      "Saved Model\n",
      "Step: 65000. Mean Reward: 0.40735598467016243. Std of Reward: 1.3412244244235445.\n",
      "Step: 70000. Mean Reward: 0.5357590985095357. Std of Reward: 1.103550482218061.\n",
      "Step: 75000. Mean Reward: 0.5982830645690255. Std of Reward: 1.0362961968399824.\n",
      "Step: 80000. Mean Reward: 0.6283371058323529. Std of Reward: 0.9242736251468234.\n",
      "Saved Model\n",
      "Step: 85000. Mean Reward: 0.71464110283648. Std of Reward: 0.7911811685788276.\n",
      "Step: 90000. Mean Reward: 0.7578330387842807. Std of Reward: 0.7342364181576503.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 1 : \tlessonNr -> 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 95000. Mean Reward: 0.37249258402002966. Std of Reward: 0.8883931827898076.\n",
      "Step: 100000. Mean Reward: 0.15792910688880593. Std of Reward: 0.928703355262352.\n",
      "Saved Model\n",
      "Step: 105000. Mean Reward: 0.18136054690051018. Std of Reward: 0.9143996477624657.\n",
      "Step: 110000. Mean Reward: 0.19920454758538955. Std of Reward: 0.9119688525707262.\n",
      "Step: 115000. Mean Reward: 0.2851395743990148. Std of Reward: 0.8858089471462623.\n",
      "Step: 120000. Mean Reward: 0.27361640026310396. Std of Reward: 0.8931248419619585.\n",
      "Saved Model\n",
      "Step: 125000. Mean Reward: 0.29375706367245763. Std of Reward: 0.8753154431360483.\n",
      "Step: 130000. Mean Reward: 0.3494903592601928. Std of Reward: 0.8405762728851219.\n",
      "Step: 135000. Mean Reward: 0.4041888310010638. Std of Reward: 0.8074123887952944.\n",
      "Step: 140000. Mean Reward: 0.4091979956893484. Std of Reward: 0.813079086833523.\n",
      "Saved Model\n",
      "Step: 145000. Mean Reward: 0.40493567365614036. Std of Reward: 0.8182597209049765.\n",
      "Step: 150000. Mean Reward: 0.42903846236199095. Std of Reward: 0.8212714192852766.\n",
      "Step: 155000. Mean Reward: 0.48102564185373464. Std of Reward: 0.7835957877520335.\n",
      "Step: 160000. Mean Reward: 0.4845404822935449. Std of Reward: 0.7805135324829735.\n",
      "Saved Model\n",
      "Step: 165000. Mean Reward: 0.48015053829677423. Std of Reward: 0.7974522458199982.\n",
      "Step: 170000. Mean Reward: 0.4791877644004219. Std of Reward: 0.8027437611746295.\n",
      "Step: 175000. Mean Reward: 0.5257037820790966. Std of Reward: 0.7639415206206546.\n",
      "Step: 180000. Mean Reward: 0.5882035312673937. Std of Reward: 0.7068202029299612.\n",
      "Saved Model\n",
      "Step: 185000. Mean Reward: 0.5799095483531658. Std of Reward: 0.7245475395005788.\n",
      "Step: 190000. Mean Reward: 0.6131082429124131. Std of Reward: 0.6962958960777326.\n",
      "Step: 195000. Mean Reward: 0.6048752405768715. Std of Reward: 0.7039630300491809.\n",
      "Step: 200000. Mean Reward: 0.6677648207755101. Std of Reward: 0.6444916929071621.\n",
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 2 : \tlessonNr -> 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 205000. Mean Reward: -0.05764380414004428. Std of Reward: 0.8683703854166495.\n",
      "Step: 210000. Mean Reward: -0.28465062892646054. Std of Reward: 0.8066025646722198.\n",
      "Step: 215000. Mean Reward: -0.1933295443189773. Std of Reward: 0.8191282406506576.\n",
      "Step: 220000. Mean Reward: -0.19588369331641964. Std of Reward: 0.8173908215807185.\n",
      "Saved Model\n",
      "Step: 225000. Mean Reward: -0.029377728047270776. Std of Reward: 0.8329717059524988.\n",
      "Step: 230000. Mean Reward: 0.04130385579217684. Std of Reward: 0.7982854368284339.\n",
      "Step: 235000. Mean Reward: 0.10430070044533796. Std of Reward: 0.7862394867575956.\n",
      "Step: 240000. Mean Reward: 0.11476190579535143. Std of Reward: 0.7826398730733614.\n",
      "Saved Model\n",
      "Step: 245000. Mean Reward: 0.1684848497638608. Std of Reward: 0.7704120600370243.\n",
      "Step: 250000. Mean Reward: 0.22283018980166477. Std of Reward: 0.754860156116326.\n",
      "Step: 255000. Mean Reward: 0.2690769245056044. Std of Reward: 0.7440371205247145.\n",
      "Step: 260000. Mean Reward: 0.30308279004455335. Std of Reward: 0.7262377881924105.\n",
      "Saved Model\n",
      "Step: 265000. Mean Reward: 0.33667391462836954. Std of Reward: 0.7061606410408149.\n",
      "Step: 270000. Mean Reward: 0.4020935191073326. Std of Reward: 0.6861997617244554.\n",
      "Step: 275000. Mean Reward: 0.38095948995277185. Std of Reward: 0.6923250720351675.\n",
      "Step: 280000. Mean Reward: 0.3511558871886532. Std of Reward: 0.7135025854205344.\n",
      "Saved Model\n",
      "Step: 285000. Mean Reward: 0.4005109509270073. Std of Reward: 0.687226256318879.\n",
      "Step: 290000. Mean Reward: 0.4330145550976091. Std of Reward: 0.6656801020835978.\n",
      "Step: 295000. Mean Reward: 0.4304712069956021. Std of Reward: 0.6543693569618657.\n",
      "Step: 300000. Mean Reward: 0.48589690988546397. Std of Reward: 0.6228402438202717.\n",
      "Saved Model\n",
      "Step: 305000. Mean Reward: 0.4574024671852156. Std of Reward: 0.6424081435584071.\n",
      "Step: 310000. Mean Reward: 0.47395025130716417. Std of Reward: 0.6512061661807084.\n",
      "Step: 315000. Mean Reward: 0.44827902477637477. Std of Reward: 0.6555807676008136.\n",
      "Step: 320000. Mean Reward: 0.4717531141891079. Std of Reward: 0.6250765411663292.\n",
      "Saved Model\n",
      "Step: 325000. Mean Reward: 0.4416221784967146. Std of Reward: 0.6631567199863375.\n",
      "Step: 330000. Mean Reward: 0.4202291686922917. Std of Reward: 0.6614006211460095.\n",
      "Step: 335000. Mean Reward: 0.434554867547412. Std of Reward: 0.6492943767947079.\n",
      "Step: 340000. Mean Reward: 0.47305031680482185. Std of Reward: 0.6134738219496492.\n",
      "Saved Model\n",
      "Step: 345000. Mean Reward: 0.4635187082520728. Std of Reward: 0.6356317015027129.\n",
      "Step: 350000. Mean Reward: 0.42642928051632045. Std of Reward: 0.6707058232932502.\n",
      "Step: 355000. Mean Reward: 0.4770392184565686. Std of Reward: 0.6395645418592149.\n",
      "Step: 360000. Mean Reward: 0.48158940697464525. Std of Reward: 0.6504695420395599.\n",
      "Saved Model\n",
      "Step: 365000. Mean Reward: 0.4879332745589435. Std of Reward: 0.6512543625735993.\n",
      "Step: 370000. Mean Reward: 0.49073614086309747. Std of Reward: 0.6365788505978927.\n",
      "Step: 375000. Mean Reward: 0.5279436651838497. Std of Reward: 0.6112794227834275.\n",
      "Step: 380000. Mean Reward: 0.47583572375568284. Std of Reward: 0.6469391931322278.\n",
      "Saved Model\n",
      "Step: 385000. Mean Reward: 0.48230921926263626. Std of Reward: 0.6292994202839592.\n",
      "Step: 390000. Mean Reward: 0.4418555685686058. Std of Reward: 0.654043873214433.\n",
      "Step: 395000. Mean Reward: 0.4782071733488048. Std of Reward: 0.6277195601379442.\n",
      "Step: 400000. Mean Reward: 0.46483773035294124. Std of Reward: 0.6263970388725654.\n",
      "Saved Model\n",
      "Step: 405000. Mean Reward: 0.4509028483440628. Std of Reward: 0.6548834970716524.\n",
      "Step: 410000. Mean Reward: 0.4394960498639328. Std of Reward: 0.6574676264456348.\n",
      "Step: 415000. Mean Reward: 0.4450435645818006. Std of Reward: 0.6644525685061584.\n",
      "Step: 420000. Mean Reward: 0.5034423099992308. Std of Reward: 0.6223187697142376.\n",
      "Saved Model\n",
      "Step: 425000. Mean Reward: 0.49911428822466675. Std of Reward: 0.6285009202058814.\n",
      "Step: 430000. Mean Reward: 0.4618415862520792. Std of Reward: 0.6391068763641133.\n",
      "Step: 435000. Mean Reward: 0.45775428210634445. Std of Reward: 0.635895141730312.\n",
      "Step: 440000. Mean Reward: 0.5094060390093477. Std of Reward: 0.613022376838806.\n",
      "Saved Model\n",
      "Step: 445000. Mean Reward: 0.4809018058642284. Std of Reward: 0.6188590810891037.\n",
      "Step: 450000. Mean Reward: 0.4540058789872674. Std of Reward: 0.65239265568151.\n",
      "Step: 455000. Mean Reward: 0.3481039486099134. Std of Reward: 0.7253138886539535.\n",
      "Step: 460000. Mean Reward: 0.2864161872265896. Std of Reward: 0.756694636241923.\n",
      "Saved Model\n",
      "Step: 465000. Mean Reward: 0.31603128265229713. Std of Reward: 0.7378863955623947.\n",
      "Step: 470000. Mean Reward: 0.4266078453901961. Std of Reward: 0.6705064402445197.\n",
      "Step: 475000. Mean Reward: 0.4862293511613217. Std of Reward: 0.628710382443346.\n",
      "Step: 480000. Mean Reward: 0.49945544802039604. Std of Reward: 0.6081841185199447.\n",
      "Saved Model\n",
      "Step: 485000. Mean Reward: 0.5132281374825969. Std of Reward: 0.6339528079608474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "Lesson changed. Now in Lesson 3 : \tlessonNr -> 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 490000. Mean Reward: -2.2741503582701834. Std of Reward: 33.52636706728141.\n",
      "Step: 495000. Mean Reward: -153.69090559589893. Std of Reward: 186.79480120621886.\n",
      "Step: 500000. Mean Reward: -137.0990897478373. Std of Reward: 144.81437979882932.\n",
      "Saved Model\n",
      "Step: 505000. Mean Reward: -202.58874810037526. Std of Reward: 156.86227885770288.\n",
      "Step: 510000. Mean Reward: -100.57666535117997. Std of Reward: 140.9126115504323.\n",
      "Step: 515000. Mean Reward: -217.7644402805558. Std of Reward: 175.54790870309108.\n",
      "Step: 520000. Mean Reward: -261.6314198831414. Std of Reward: 221.53221853984948.\n",
      "Saved Model\n",
      "Step: 525000. Mean Reward: -61.2592853003852. Std of Reward: 78.4562978862028.\n",
      "Step: 530000. Mean Reward: -142.23416438191705. Std of Reward: 155.33786281924048.\n",
      "Step: 535000. Mean Reward: -130.56499605686344. Std of Reward: 172.4664831900118.\n",
      "Step: 540000. Mean Reward: -81.42130271246523. Std of Reward: 128.73588091862496.\n",
      "Saved Model\n",
      "Step: 545000. Mean Reward: -104.64249722610568. Std of Reward: 156.7768308625092.\n",
      "Step: 550000. Mean Reward: -171.34817823410728. Std of Reward: 228.81920882907886.\n",
      "Step: 555000. Mean Reward: -62.17538461133893. Std of Reward: 87.73749979983573.\n",
      "Step: 560000. Mean Reward: -149.3399980650002. Std of Reward: 139.4729572794072.\n",
      "Saved Model\n",
      "Step: 565000. Mean Reward: -103.35499754277942. Std of Reward: 136.3702336675259.\n",
      "Step: 570000. Mean Reward: -155.6354529086899. Std of Reward: 215.78845030520802.\n",
      "Step: 575000. Mean Reward: -259.7699967842337. Std of Reward: 218.90218277392165.\n",
      "Step: 580000. Mean Reward: -158.7642869919992. Std of Reward: 127.69104470313438.\n",
      "Saved Model\n",
      "Step: 585000. Mean Reward: -185.41857048805826. Std of Reward: 126.15726930631018.\n",
      "Step: 590000. Mean Reward: -203.44874498343634. Std of Reward: 173.43554808901342.\n",
      "Step: 595000. Mean Reward: -121.65874928527509. Std of Reward: 209.33020771283486.\n",
      "Step: 600000. Mean Reward: -164.34285438714096. Std of Reward: 149.70469398949612.\n",
      "Saved Model\n",
      "Step: 605000. Mean Reward: -212.9399941538418. Std of Reward: 204.4393806950448.\n",
      "Step: 610000. Mean Reward: -213.44999940876. Std of Reward: 109.08490579799451.\n",
      "Step: 615000. Mean Reward: -153.97833332883368. Std of Reward: 149.6617041256758.\n",
      "Step: 620000. Mean Reward: -186.95624483149845. Std of Reward: 194.0483822641929.\n",
      "Saved Model\n",
      "Step: 625000. Mean Reward: -321.91399625459906. Std of Reward: 117.45890865400459.\n",
      "Step: 630000. Mean Reward: -266.52799182019703. Std of Reward: 186.49201835367828.\n",
      "Step: 635000. Mean Reward: -173.71428278542743. Std of Reward: 155.95618834813277.\n",
      "Step: 640000. Mean Reward: -133.09222170600088. Std of Reward: 195.35418351052692.\n",
      "Saved Model\n",
      "Step: 645000. Mean Reward: -189.07142698967186. Std of Reward: 133.63703955350294.\n",
      "Step: 650000. Mean Reward: -294.3542818530573. Std of Reward: 150.1955407253602.\n",
      "Step: 655000. Mean Reward: -274.3099958485985. Std of Reward: 134.11880697054127.\n",
      "Step: 660000. Mean Reward: -287.60166657766723. Std of Reward: 277.9884341477809.\n",
      "Saved Model\n",
      "Step: 665000. Mean Reward: -269.7366632004994. Std of Reward: 245.5800345753541.\n",
      "Step: 670000. Mean Reward: -466.24599973000596. Std of Reward: 298.64272226864585.\n",
      "Step: 675000. Mean Reward: -230.99999999519915. Std of Reward: 50.753548845357926.\n",
      "Step: 680000. Mean Reward: -283.89199999680017. Std of Reward: 78.22473046680967.\n",
      "Saved Model\n",
      "Step: 685000. Mean Reward: -405.1579996692054. Std of Reward: 340.2481836706682.\n",
      "Step: 690000. Mean Reward: -461.2333323360058. Std of Reward: 318.874228885877.\n",
      "Step: 695000. Mean Reward: -191.51599961876002. Std of Reward: 236.3017657987949.\n",
      "Step: 700000. Mean Reward: -144.57199973779896. Std of Reward: 96.63684197118178.\n",
      "Saved Model\n",
      "Step: 705000. Mean Reward: -217.63399720667775. Std of Reward: 150.40020297932801.\n",
      "Step: 710000. Mean Reward: -318.01499720650133. Std of Reward: 263.70522743127344.\n",
      "Step: 715000. Mean Reward: -295.48399996640023. Std of Reward: 205.7313280937859.\n",
      "Step: 720000. Mean Reward: -250.6699964909987. Std of Reward: 156.33259979895806.\n",
      "Saved Model\n",
      "Step: 725000. Mean Reward: -300.415999972401. Std of Reward: 219.7399446544688.\n",
      "Step: 730000. Mean Reward: -388.6900000000028. Std of Reward: 273.7670223018148.\n",
      "Step: 735000. Mean Reward: -278.1240000000005. Std of Reward: 74.77847326604288.\n",
      "Step: 740000. Mean Reward: -231.50399515839837. Std of Reward: 178.77598798819858.\n",
      "Saved Model\n",
      "Step: 745000. Mean Reward: -477.4999997102065. Std of Reward: 286.17036555104426.\n",
      "Step: 750000. Mean Reward: -467.9859954760016. Std of Reward: 272.51313074336343.\n",
      "Step: 755000. Mean Reward: -309.12199502799785. Std of Reward: 114.50961642334407.\n",
      "Step: 760000. Mean Reward: -138.1999996237999. Std of Reward: 105.18606144882033.\n",
      "Saved Model\n",
      "Step: 765000. Mean Reward: -175.23799977319928. Std of Reward: 99.05615031605005.\n",
      "Step: 770000. Mean Reward: -535.3440000000088. Std of Reward: 351.2519611105471.\n",
      "Step: 775000. Mean Reward: -141.52599997599987. Std of Reward: 95.73319960938107.\n",
      "Step: 780000. Mean Reward: -137.13666234883158. Std of Reward: 165.76109925851858.\n",
      "Saved Model\n",
      "Step: 785000. Mean Reward: -515.9819947256045. Std of Reward: 284.0302758064308.\n",
      "Step: 790000. Mean Reward: -297.52799512319785. Std of Reward: 239.6744813824444.\n",
      "Step: 795000. Mean Reward: -270.71400000000034. Std of Reward: 66.99866165827648.\n",
      "Step: 800000. Mean Reward: -402.0139999406004. Std of Reward: 224.53686854557253.\n",
      "Saved Model\n",
      "Step: 805000. Mean Reward: -716.7100000000123. Std of Reward: 204.8600000000053.\n",
      "Step: 810000. Mean Reward: -349.77599998560277. Std of Reward: 254.91512287833442.\n",
      "Step: 815000. Mean Reward: -567.3820000000084. Std of Reward: 308.3352371299867.\n",
      "Step: 820000. Mean Reward: -393.2459946913991. Std of Reward: 178.5157231546808.\n",
      "Saved Model\n",
      "Step: 825000. Mean Reward: -387.7319999262039. Std of Reward: 304.56980811291953.\n",
      "Step: 830000. Mean Reward: -414.69599997960086. Std of Reward: 299.834136078757.\n",
      "Step: 835000. Mean Reward: -263.9259943855976. Std of Reward: 152.14015000124112.\n",
      "Step: 840000. Mean Reward: -326.44999449399836. Std of Reward: 233.55077462057878.\n",
      "Saved Model\n",
      "Step: 845000. Mean Reward: -217.79799441119854. Std of Reward: 170.14833328437078.\n",
      "Step: 850000. Mean Reward: -169.28332864833203. Std of Reward: 168.527963552232.\n",
      "Step: 855000. Mean Reward: -289.36998889399536. Std of Reward: 176.60422372543772.\n",
      "Step: 860000. Mean Reward: -271.8059999441989. Std of Reward: 195.0816589015232.\n",
      "Saved Model\n",
      "Step: 865000. Mean Reward: -342.2539894747958. Std of Reward: 137.5776189711307.\n",
      "Step: 870000. Mean Reward: -279.7379999376019. Std of Reward: 273.3421000791002.\n",
      "Step: 875000. Mean Reward: -571.6500000000065. Std of Reward: 251.511106951568.\n",
      "Step: 880000. Mean Reward: -375.9819999736008. Std of Reward: 242.8154284888141.\n",
      "Saved Model\n",
      "Step: 885000. Mean Reward: -146.29399994359966. Std of Reward: 105.01816315619853.\n",
      "Step: 890000. Mean Reward: -447.215995036001. Std of Reward: 281.00732147926703.\n",
      "Step: 895000. Mean Reward: -500.0999901119997. Std of Reward: 264.85173499995045.\n",
      "Step: 900000. Mean Reward: -395.9499999035838. Std of Reward: 294.7242034394668.\n",
      "Saved Model\n",
      "Step: 905000. Mean Reward: -183.16199502387747. Std of Reward: 166.66464437746444.\n",
      "Step: 910000. Mean Reward: -444.15199998560456. Std of Reward: 265.707898841554.\n",
      "Step: 915000. Mean Reward: -539.6439948740042. Std of Reward: 248.4303297222675.\n",
      "Step: 920000. Mean Reward: -301.79833327933636. Std of Reward: 258.76212538232875.\n",
      "Saved Model\n",
      "Step: 925000. Mean Reward: -300.17199661399866. Std of Reward: 151.5570446937719.\n",
      "Step: 930000. Mean Reward: -370.6499983038359. Std of Reward: 283.47862048162267.\n",
      "Step: 935000. Mean Reward: -357.0199976950012. Std of Reward: 256.9421866618994.\n",
      "Step: 940000. Mean Reward: -180.21166656783282. Std of Reward: 90.17599360924663.\n",
      "Saved Model\n",
      "Step: 945000. Mean Reward: -242.95999632639723. Std of Reward: 127.6979359977258.\n",
      "Step: 950000. Mean Reward: -268.4866666636672. Std of Reward: 200.77061684938153.\n",
      "Step: 955000. Mean Reward: -215.7999999061996. Std of Reward: 228.19720773016965.\n",
      "Step: 960000. Mean Reward: -188.40399989440084. Std of Reward: 124.33365303370262.\n",
      "Saved Model\n",
      "Step: 965000. Mean Reward: -378.07599653799906. Std of Reward: 208.30401789937397.\n",
      "Step: 970000. Mean Reward: -287.59999665039766. Std of Reward: 126.41928124963825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 975000. Mean Reward: -351.94800000000237. Std of Reward: 243.95625332424487.\n",
      "Step: 980000. Mean Reward: -186.0759999999991. Std of Reward: 95.56050807734337.\n",
      "Saved Model\n",
      "Step: 985000. Mean Reward: -286.4183304968323. Std of Reward: 135.00169263909288.\n",
      "Step: 990000. Mean Reward: -235.57999986350214. Std of Reward: 282.33187681213764.\n",
      "Step: 995000. Mean Reward: -375.205997132. Std of Reward: 67.044263558036.\n",
      "Step: 1000000. Mean Reward: -288.67666657333507. Std of Reward: 250.19417729380595.\n",
      "Saved Model\n",
      "Step: 1005000. Mean Reward: -346.0866655006653. Std of Reward: 270.5968883731614.\n",
      "Step: 1010000. Mean Reward: -228.4749999265019. Std of Reward: 271.7727831782415.\n",
      "Step: 1015000. Mean Reward: -295.4980000000011. Std of Reward: 134.28624671201487.\n",
      "Step: 1020000. Mean Reward: -252.74399942883846. Std of Reward: 191.18432451222225.\n",
      "Saved Model\n",
      "Step: 1025000. Mean Reward: -234.53333382983166. Std of Reward: 179.28199275869815.\n",
      "Step: 1030000. Mean Reward: -436.8579991479959. Std of Reward: 161.10680316473773.\n",
      "Step: 1035000. Mean Reward: -406.66600101200027. Std of Reward: 236.2217881107361.\n",
      "Step: 1040000. Mean Reward: -241.21199993548035. Std of Reward: 233.77660702640452.\n",
      "Saved Model\n",
      "Step: 1045000. Mean Reward: -402.618000000001. Std of Reward: 237.34323056704224.\n",
      "Step: 1050000. Mean Reward: -315.49199872477936. Std of Reward: 140.1215759071464.\n",
      "Step: 1055000. Mean Reward: -533.7699981599992. Std of Reward: 208.6951346925283.\n",
      "Step: 1060000. Mean Reward: -412.7279999460041. Std of Reward: 223.59098274860122.\n",
      "Saved Model\n",
      "Step: 1065000. Mean Reward: -315.6440000000016. Std of Reward: 251.74800000000673.\n",
      "Step: 1070000. Mean Reward: -120.42666662216641. Std of Reward: 111.83991680714516.\n",
      "Step: 1075000. Mean Reward: -175.67666664016642. Std of Reward: 104.5728261170456.\n",
      "Step: 1080000. Mean Reward: -447.92600005120073. Std of Reward: 281.0734959288224.\n",
      "Saved Model\n",
      "Step: 1085000. Mean Reward: -169.94599999879978. Std of Reward: 119.43436115390033.\n",
      "Step: 1090000. Mean Reward: -294.62799999844026. Std of Reward: 212.3276090766846.\n",
      "Step: 1095000. Mean Reward: -364.02833335101894. Std of Reward: 270.8752607843525.\n",
      "Step: 1100000. Mean Reward: -445.77800000000605. Std of Reward: 323.556273151989.\n",
      "Saved Model\n",
      "Step: 1105000. Mean Reward: -440.33999999760624. Std of Reward: 318.01579099438277.\n",
      "Step: 1110000. Mean Reward: -297.4920000002812. Std of Reward: 207.17533650473456.\n",
      "Step: 1115000. Mean Reward: -630.0260089119995. Std of Reward: 154.39100005700237.\n",
      "Step: 1120000. Mean Reward: -400.30800000028376. Std of Reward: 294.527633297446.\n",
      "Saved Model\n",
      "Step: 1125000. Mean Reward: -191.00000000000006. Std of Reward: 145.21925381987077.\n",
      "Step: 1130000. Mean Reward: -190.91999999819996. Std of Reward: 127.18160024325792.\n",
      "Step: 1135000. Mean Reward: -317.75799969760055. Std of Reward: 83.6267786797725.\n",
      "Step: 1140000. Mean Reward: -277.3040018421971. Std of Reward: 139.2709155246442.\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-583a438fd989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Decide and take an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mnew_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_horizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Jupyter/ScifiButtonsTrial/python/ppo/trainer.py\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, info, env, brain_name, steps, normalize)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_variance\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mrun_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_variance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m           \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m           \u001b[0mfeed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons18/model-2000001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/scifibuttons18/model-2000001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 8 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 8 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
